###### Abstract

Generative models offer a scalable and flexible paradigm for simulating complex environments, yet current approaches fall short in addressing the domain-specific requirements of autonomous drivingâ€”such as multi-agent interactions, fine-grained control, and multi-camera consistency. We introduce GAIA-2, Generative AI for Autonomy, a latent diffusion world model that unifies these capabilities within a single generative framework. GAIA-2 supports controllable video generation conditioned on a rich set of structured inputs: ego-vehicle dynamics, agent configurations, environmental factors, and road semantics. It generates high-resolution, spatiotemporally consistent multi-camera videos across geographically diverse driving environments (UK, US, Germany). The model integrates both structured conditioning and external latent embeddings (e.g., from a proprietary driving model) to facilitate flexible and semantically grounded scene synthesis. Through this integration, GAIA-2 enables scalable simulation of both common and rare driving scenarios, advancing the use of generative world models as a core tool in the development of autonomous systems. Videos are available at [https://wayve.ai/thinking/gaia-2](https://wayve.ai/thinking/gaia-2).

ìƒì„± ëª¨ë¸ì€ ë³µì¡í•œ í™˜ê²½ì„ ì‹œë®¬ë ˆì´ì…˜í•˜ëŠ” í™•ì¥ ê°€ëŠ¥í•˜ê³  ìœ ì—°í•œ íŒ¨ëŸ¬ë‹¤ì„ì„ ì œê³µí•©ë‹ˆë‹¤. í•˜ì§€ë§Œ í˜„ì¬ ì ‘ê·¼ ë°©ì‹ì€ ììœ¨ ì£¼í–‰ê³¼ ê°™ì´ íŠ¹ì • ì˜ì—­ì˜ ìš”êµ¬ ì‚¬í•­ì„ ì¶©ë¶„íˆ ì¶©ì¡±í•˜ì§€ ëª»í•˜ê³  ìˆìŠµë‹ˆë‹¤ â€“ ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ìƒí˜¸ ì‘ìš©, ì •ë°€í•œ ì œì–´, ë‹¤ì¤‘ ì¹´ë©”ë¼ ì¼ê´€ì„± ë“±ì´ ê·¸ ì˜ˆì…ë‹ˆë‹¤. ì €í¬ëŠ” GAIA-2, ììœ¨ ì£¼í–‰ì„ ìœ„í•œ ìƒì„±í˜• AIë¥¼ ì†Œê°œí•©ë‹ˆë‹¤. GAIA-2ëŠ” ì´ëŸ¬í•œ ê¸°ëŠ¥ì„ ë‹¨ì¼ ìƒì„±í˜• í”„ë ˆì„ì›Œí¬ ë‚´ì—ì„œ í†µí•©í•˜ëŠ” ì ì¬ í™•ì‚° ì„¸ê³„ ëª¨ë¸ì…ë‹ˆë‹¤. GAIA-2ëŠ” ììœ¨ ì£¼í–‰ ì°¨ëŸ‰ ë™ì—­í•™, ì—ì´ì „íŠ¸ êµ¬ì„±, í™˜ê²½ ìš”ì¸, ë„ë¡œ ì˜ë¯¸ë¡ ê³¼ ê°™ì€ í’ë¶€í•œ êµ¬ì¡°í™”ëœ ì…ë ¥ì— ë”°ë¼ ì œì–´ ê°€ëŠ¥í•œ ë¹„ë””ì˜¤ ìƒì„±ì„ ì§€ì›í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì˜êµ­, ë¯¸êµ­, ë…ì¼ ë“± ë‹¤ì–‘í•œ ì§€ë¦¬ì  í™˜ê²½ì—ì„œ ê³ í•´ìƒë„, ê³µê°„-ì‹œê°„ì ìœ¼ë¡œ ì¼ê´€ëœ ë‹¤ì¤‘ ì¹´ë©”ë¼ ë¹„ë””ì˜¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ë˜í•œ, ìì²´ ê°œë°œëœ ì£¼í–‰ ëª¨ë¸ì˜ ì ì¬ í™•ì‚° ì„ë² ë”©ê³¼ í•¨ê»˜ êµ¬ì¡°í™”ëœ ì¡°ê±´ë¶€ ì œì–´ë¥¼ í†µí•©í•˜ì—¬ ìœ ì—°í•˜ê³  ì˜ë¯¸ë¡ ì ìœ¼ë¡œ ê¸°ë°˜ëœ ì¥ë©´ í•©ì„± ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤. ì´ëŸ¬í•œ í†µí•©ì„ í†µí•´ GAIA-2ëŠ” ì¼ë°˜ì ì¸ ì‹œë‚˜ë¦¬ì˜¤ì™€ í¬ê·€í•œ ì‹œë‚˜ë¦¬ì˜¤ ëª¨ë‘ë¥¼ í™•ì¥ ê°€ëŠ¥í•˜ê²Œ ì‹œë®¬ë ˆì´ì…˜í•  ìˆ˜ ìˆë„ë¡ ì§€ì›í•˜ë©°, ììœ¨ ì‹œìŠ¤í…œ ê°œë°œì˜ í•µì‹¬ ë„êµ¬ë¡œì„œ ìƒì„±í˜• ì„¸ê³„ ëª¨ë¸ì˜ í™œìš©ì„ ë°œì „ì‹œí‚µë‹ˆë‹¤. ê´€ë ¨ ë¹„ë””ì˜¤ëŠ” ë‹¤ìŒ ë§í¬ì—ì„œ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤: [https://wayve.ai/thinking/gaia-2](https://wayve.ai/thinking/gaia-2).

## 1 Introduction

Realistic simulation of driving scenarios is a foundational requirement for the development, training, and evaluation of autonomous driving systems. Generative world models enable scalable and diverse synthetic data creation, reducing reliance on expensive real-world data collection and facilitating robust evaluation in safe and repeatable environments. Unlike general-purpose text-to-video or image-to-video models, which primarily focus on visual realism and temporal coherence, autonomous driving applications demand fine-grained control over domain-specific aspects of the scene.

ììœ¨ ì£¼í–‰ ì‹œìŠ¤í…œ ê°œë°œ, êµìœ¡, í‰ê°€ì— ìˆì–´ì„œ í˜„ì‹¤ì ì¸ ì£¼í–‰ ì‹œë®¬ë ˆì´ì…˜ì€ í•„ìˆ˜ì ì¸ ìš”ì†Œì…ë‹ˆë‹¤. ìƒì„±í˜• ì›”ë“œ ëª¨ë¸ì€ í™•ì¥ ê°€ëŠ¥í•˜ê³  ë‹¤ì–‘í•œ í•©ì„± ë°ì´í„° ìƒì„±ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ì—¬, ê³ ê°€ì˜ ì‹¤ì œ ë°ì´í„° ìˆ˜ì§‘ì— ëŒ€í•œ ì˜ì¡´ë„ë¥¼ ì¤„ì´ê³ , ì•ˆì „í•˜ê³  ë°˜ë³µì ì¸ í™˜ê²½ì—ì„œ ê°•ë ¥í•œ í‰ê°€ë¥¼ ìš©ì´í•˜ê²Œ í•©ë‹ˆë‹¤. ì¼ë°˜ì ì¸ í…ìŠ¤íŠ¸-íˆ¬-ë¹„ë””ì˜¤ ë˜ëŠ” ì´ë¯¸ì§€-íˆ¬-ë¹„ë””ì˜¤ ëª¨ë¸ê³¼ ë‹¬ë¦¬, ììœ¨ ì£¼í–‰ ì• í”Œë¦¬ì¼€ì´ì…˜ì€ ì¥ë©´ì˜ íŠ¹ì • ì˜ì—­ì— ëŒ€í•œ ë¯¸ì„¸í•œ ì œì–´ ê¸°ëŠ¥ì„ ìš”êµ¬í•©ë‹ˆë‹¤.

Generative models for autonomous driving must accurately simulate factors such as the actions of the ego-vehicle, the locations and movements of other agents (e.g., vehicles, pedestrians, cyclists), and their interactions. Moreover, these models must allow conditional generation based on contextual attributes, such as geographic location, weather, time of day, road configuration (e.g., speed limits, number of lanes, pedestrian crossings, traffic lights, intersections), and rare but critical edge-case scenarios. The ability to generate consistent multi-camera video streams is also essential, as autonomous vehicles rely on spatially and temporally coherent input from multiple perspectives for perception and decision-making. Meeting these domain-specific constraints presents unique challenges that are not adequately addressed by existing video generative models.

ììœ¨ ì£¼í–‰ì„ ìœ„í•œ ìƒì„± ëª¨ë¸ì€ ë‹¤ìŒê³¼ ê°™ì€ ìš”ì†Œë“¤ì„ ì •í™•í•˜ê²Œ ì‹œë®¬ë ˆì´ì…˜í•´ì•¼ í•©ë‹ˆë‹¤. ì¦‰, ë³¸ ì°¨ëŸ‰ì˜ í–‰ë™, ë‹¤ë¥¸ ëŒ€ìƒì„(ì˜ˆ: ì°¨ëŸ‰, ë³´í–‰ì, ìì „ê±°)ì˜ ìœ„ì¹˜ì™€ ì›€ì§ì„, ê·¸ë¦¬ê³  ì´ë“¤ì˜ ìƒí˜¸ì‘ìš©ì„ ì •í™•í•˜ê²Œ ì¬í˜„í•´ì•¼ í•©ë‹ˆë‹¤. ë˜í•œ, ì´ëŸ¬í•œ ëª¨ë¸ì€ ì§€ë¦¬ì  ìœ„ì¹˜, ë‚ ì”¨, ì‹œê°„ëŒ€, ë„ë¡œ êµ¬ì„±(ì˜ˆ: ì†ë„ ì œí•œ, ì°¨ì„  ìˆ˜, íš¡ë‹¨ë³´ë„, ì‹ í˜¸ë“±, êµì°¨ë¡œ)ì™€ ê°™ì´ ë§¥ë½ì ì¸ ì†ì„±ì„ ê¸°ë°˜ìœ¼ë¡œ ì¡°ê±´ë¶€ ìƒì„± ê¸°ëŠ¥ì„ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤. ë“œë¬¼ì§€ë§Œ ë§¤ìš° ì¤‘ìš”í•œ ì˜ˆì™¸ì ì¸ ìƒí™©ê¹Œì§€ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤. ë˜í•œ, ììœ¨ ì£¼í–‰ ì°¨ëŸ‰ì€ ì¸ì§€ ë° ì˜ì‚¬ ê²°ì •ì— í•„ìš”í•œ ê³µê°„ì , ì‹œê°„ì ìœ¼ë¡œ ì¼ê´€ëœ ë‹¤ì¤‘ ì¹´ë©”ë¼ ì˜ìƒ ìŠ¤íŠ¸ë¦¼ì„ ìƒì„±í•  ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ íŠ¹ì • ì˜ì—­ì˜ ì œì•½ ì‚¬í•­ì„ ì¶©ì¡±í•˜ëŠ” ê²ƒì€ ê¸°ì¡´ ë¹„ë””ì˜¤ ìƒì„± ëª¨ë¸ì´ ì¶©ë¶„íˆ ë‹¤ë£¨ì§€ ëª»í•˜ëŠ” ë…íŠ¹í•œ ê³¼ì œë¥¼ ì œì‹œí•©ë‹ˆë‹¤.

While recent advancements in latent video generation, particularly those leveraging continuous [^1] or quantized [^2] latent spaces, have led to improvements in efficiency and visual quality, current approaches remain limited in scope. In the context of autonomous driving, several models have introduced task-specific conditioning mechanisms [^5], enabling some control over scenario elements. However, existing models typically support only a subset of the required capabilities, such as limited conditioning types, lack of multi-camera generation, or poor spatial-temporal coherence. As such, a comprehensive and unified generative framework remains an open challenge in the field.

ìµœê·¼ ì ì¬ ì˜ìƒ ìƒì„± ê¸°ìˆ ì˜ ë°œì „, íŠ¹íˆ ì—°ì†ì ì¸ [^1] ë˜ëŠ” ì–‘ìí™”ëœ [^2] ì ì¬ ê³µê°„ì„ í™œìš©í•œ ê¸°ìˆ ë“¤ì€ íš¨ìœ¨ì„±ê³¼ ì‹œê°ì  í’ˆì§ˆ í–¥ìƒì— ê¸°ì—¬í–ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ í˜„ì¬ì˜ ì ‘ê·¼ ë°©ì‹ì€ ì—¬ì „íˆ ì ìš© ë²”ìœ„ì— ì œí•œì´ ìˆìŠµë‹ˆë‹¤. ììœ¨ ì£¼í–‰ ë¶„ì•¼ì—ì„œëŠ” ì—¬ëŸ¬ ëª¨ë¸ë“¤ì´ ì‘ì—…ë³„ ì¡°ê±´í™” ë©”ì»¤ë‹ˆì¦˜ [^5]ì„ ë„ì…í•˜ì—¬ ì¼ë¶€ ì‹œë‚˜ë¦¬ì˜¤ ìš”ì†Œë¥¼ ì œì–´í•  ìˆ˜ ìˆë„ë¡ í–ˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ê¸°ì¡´ ëª¨ë¸ë“¤ì€ ì œí•œì ì¸ ì¡°ê±´í™” ìœ í˜•, ë‹¤ì¤‘ ì¹´ë©”ë¼ ìƒì„± ë¶€ì¬, ë˜ëŠ” ê³µê°„-ì‹œê°„ì  ì¼ê´€ì„± ë¶€ì¡± ë“± í•„ìš”í•œ ê¸°ëŠ¥ì˜ ì¼ë¶€ë§Œì„ ì§€ì›í•˜ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤. ë”°ë¼ì„œ, í•´ë‹¹ ë¶„ì•¼ì—ì„œ í¬ê´„ì ì´ê³  í†µí•©ì ì¸ ìƒì„± í”„ë ˆì„ì›Œí¬ë¥¼ êµ¬ì¶•í•˜ëŠ” ê²ƒì€ ì—¬ì „íˆ í•´ê²°í•´ì•¼ í•  ì¤‘ìš”í•œ ê³¼ì œì…ë‹ˆë‹¤.

To address this gap, we introduce GAIA-2, a domain-specialized latent diffusion model that represents a significant advancement in video-generative world modelling for autonomous driving. GAIA-2 advances prior work by supporting a conditional generation of high-resolution, multi-camera driving scenes, with fine-grained control over ego-vehicle actions, agent behavior, scene geometry, and environmental factors. The model can generate up to five temporally and spatially consistent camera streams at a resolution of $448\times 960$ , accommodating various multi-camera rig configurations used in real-world autonomous vehicle systems.

ì´ í‹ˆì„ ë©”ìš°ê¸° ìœ„í•´, ì €í¬ëŠ” GAIA-2ë¼ëŠ” ì˜ì—­ë³„ ì „ë¬¸í™”ëœ ì ì¬ í™•ì‚° ëª¨ë¸ì„ ì†Œê°œí•˜ê²Œ ë©ë‹ˆë‹¤. ì´ëŠ” ììœ¨ ì£¼í–‰ì„ ìœ„í•œ ë¹„ë””ì˜¤ ìƒì„± ì„¸ê³„ ëª¨ë¸ë§ ë¶„ì•¼ì—ì„œ íšê¸°ì ì¸ ë°œì „ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. GAIA-2ëŠ” ì´ì „ ì—°êµ¬ë¥¼ ë°œì „ì‹œì¼œ ê³ í•´ìƒë„, ë‹¤ì¹´ë©”ë¼ ì£¼í–‰ ì¥ë©´ì˜ ì¡°ê±´ë¶€ ìƒì„± ê¸°ëŠ¥ì„ ì§€ì›í•˜ë©°, ììœ¨ ì°¨ëŸ‰ì˜ í–‰ë™, ì—ì´ì „íŠ¸ í–‰ë™, ì¥ë©´ ê¸°í•˜í•™, í™˜ê²½ ìš”ì¸ì— ëŒ€í•œ ì •ë°€í•œ ì œì–´ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤. ì´ ëª¨ë¸ì€ 448x960 í•´ìƒë„ì˜ ì‹œê°„ì , ê³µê°„ì ìœ¼ë¡œ ì¼ê´€ëœ 5ê°œ ì¹´ë©”ë¼ ìŠ¤íŠ¸ë¦¼ì„ ìƒì„±í•  ìˆ˜ ìˆìœ¼ë©°, ì‹¤ì œ ììœ¨ ì£¼í–‰ ì‹œìŠ¤í…œì—ì„œ ì‚¬ìš©ë˜ëŠ” ë‹¤ì–‘í•œ ë‹¤ì¹´ë©”ë¼  Rig êµ¬ì„±ì— ì í•©í•©ë‹ˆë‹¤.

![Refer to caption](./figure/1.png)

Figure 1: A selection of â€˜ from scratch â€™ generations demonstrating the diversity of synthetic scenarios possible with GAIA-2.

GAIA-2 supports conditioning on a wide range of scene attributes, including ego-vehicle kinematics (e.g., speed, curvature), geographic regions (UK, US, Germany), time of day, weather, and a rich taxonomy of road layout featuresâ€”such as number and type of lanes (e.g., drivable, bus, cycle), presence of pedestrian crossings, traffic lights, and intersection topology (e.g., one-way roads, roundabouts). It also allows direct control over the locations, orientations, and dimensions of dynamic agents within the scene. This extensive set of conditioning parameters allows for precise control over the generated scenarios, enabling simulation of both typical driving conditions and edge cases critical for robust system evaluation.

GAIA-2ëŠ” ë‹¤ì–‘í•œ ì¥ë©´ ì†ì„±ì—ì„œ í›ˆë ¨ì„ ì§€ì›í•˜ë©°, ì—¬ê¸°ì—ëŠ” ììœ¨ ì£¼í–‰ ì°¨ëŸ‰ì˜ ìš´ë™í•™ì  ì •ë³´(ì˜ˆ: ì†ë„, ê³¡ë¥ ), ì§€ë¦¬ì  ì§€ì—­(ì˜êµ­, ë¯¸êµ­, ë…ì¼), ì‹œê°„ëŒ€, ë‚ ì”¨, ê·¸ë¦¬ê³  ë„ë¡œ êµ¬ì¡° íŠ¹ì§•ì— ëŒ€í•œ í’ë¶€í•œ ë¶„ë¥˜ ì²´ê³„(ì˜ˆ: ì°¨ì„  ìˆ˜ì™€ ì¢…ë¥˜, ë³´í–‰ì íš¡ë‹¨ë¡œë‚˜, ì‹ í˜¸ë“±, êµì°¨ë¡œ êµ¬ì¡°, ë‹¨ë°©í–¥ ë„ë¡œ, ì›í˜• êµì°¨ë¡œ ë“±)ê°€ í¬í•¨ë©ë‹ˆë‹¤. ë˜í•œ, ì¥ë©´ ë‚´ ë™ì  ì—ì´ì „íŠ¸ì˜ ìœ„ì¹˜, ë°©í–¥, í¬ê¸°ë¥¼ ì§ì ‘ ì œì–´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ì²˜ëŸ¼ ê´‘ë²”ìœ„í•œ í›ˆë ¨ ë§¤ê°œë³€ìˆ˜ë¥¼ í†µí•´ ìƒì„±ë˜ëŠ” ì‹œë‚˜ë¦¬ì˜¤ì— ëŒ€í•œ ì •ë°€í•œ ì œì–´ê°€ ê°€ëŠ¥í•˜ë©°, ì¼ë°˜ì ì¸ ì£¼í–‰ ì¡°ê±´ë¿ë§Œ ì•„ë‹ˆë¼ ì‹œìŠ¤í…œì˜ ê²¬ê³ ì„±ì„ í‰ê°€í•˜ëŠ” ë° ì¤‘ìš”í•œ íŠ¹ìˆ˜ ìƒí™© ì‹œë®¬ë ˆì´ì…˜ë„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

To ensure flexibility and interoperability, GAIA-2 enables conditioning on external latent spaces, including CLIP embeddings and a proprietary model trained to produce driving-specific latent representations. This capability allows semantic control over scene content and facilitates integration with downstream planning or perception modules. GAIA-2 further supports multiple video generation modes, including generation from scratch, prediction from past context, and selective content editing through inpainting.

GAIA-2ë¥¼ í†µí•´ ìœ ì—°ì„±ê³¼ ìƒí˜¸ ìš´ìš©ì„±ì„ í™•ë³´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì™¸ë¶€ ì ì¬ ê³µê°„, ì¦‰ CLIP ì„ë² ë”©ê³¼ ìš´ì „ íŠ¹í™” ì ì¬ í‘œí˜„ì„ ìƒì„±í•˜ë„ë¡ í›ˆë ¨ëœ ìì²´ ëª¨ë¸ì„ í™œìš©í•˜ì—¬ ì¥ë©´ ì½˜í…ì¸ ì— ëŒ€í•œ ì˜ë¯¸ë¡ ì  ì œì–´ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•˜ê³ , ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ê³„íš ë˜ëŠ” ì¸ì§€ ëª¨ë“ˆê³¼ì˜ í†µí•©ì„ ìš©ì´í•˜ê²Œ í•©ë‹ˆë‹¤. ë˜í•œ GAIA-2ëŠ” ì²˜ìŒë¶€í„° ìƒì„±, ê³¼ê±° ë§¥ë½ ì˜ˆì¸¡, ì¸í˜ì¸íŒ…ì„ í†µí•œ ì„ íƒì  ì½˜í…ì¸  í¸ì§‘ ë“± ë‹¤ì–‘í•œ ë¹„ë””ì˜¤ ìƒì„± ëª¨ë“œë¥¼ ì§€ì›í•©ë‹ˆë‹¤.

By integrating these capabilities into a single framework, GAIA-2 sets a new benchmark in video-generative world models for autonomous driving. It enables detailed, controllable, and realistic simulation across diverse conditions, providing a powerful tool for scalable training and robust evaluation of autonomous driving systems.

ì €í¬ GAIA-2ëŠ” ì´ëŸ¬í•œ ê¸°ëŠ¥ë“¤ì„ í•˜ë‚˜ì˜ í”„ë ˆì„ì›Œí¬ë¡œ í†µí•©í•¨ìœ¼ë¡œì¨ ììœ¨ ì£¼í–‰ì„ ìœ„í•œ ë¹„ë””ì˜¤ ìƒì„±í˜• ì›”ë“œ ëª¨ë¸ ë¶„ì•¼ì—ì„œ ìƒˆë¡œìš´ ê¸°ì¤€ì„ ì œì‹œí•©ë‹ˆë‹¤. ë‹¤ì–‘í•œ í™˜ê²½ì—ì„œ ìƒì„¸í•˜ê³ , ì œì–´ ê°€ëŠ¥í•˜ë©°, í˜„ì‹¤ì ì¸ ì‹œë®¬ë ˆì´ì…˜ì„ ì œê³µí•˜ì—¬ ììœ¨ ì£¼í–‰ ì‹œìŠ¤í…œì˜ í™•ì¥ ê°€ëŠ¥í•œ í•™ìŠµ ë° ê²¬ê³ í•œ í‰ê°€ë¥¼ ìœ„í•œ ê°•ë ¥í•œ ë„êµ¬ë¡œ í™œìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

![Refer to caption](./figure/2.png)

Figure 2:GAIA-2 world model. The architecture schematic shows full surround camera views independently encoded by a video tokenizer. The combined multi-camera latents are used as past context and linearly interpolated with sampled noise. We add camera parameters along with positional encodings before passing the latents through a space-time factorized transformer. Conditioning is implemented via cross-attention and adaptive layer norm. The latents are denoised into future latents, conditioned on various inputs, including actions, 3D bounding boxes, metadata, and scenario embeddings. The denoised latents are then decoded back to pixel space by the video tokenizer. Below, we depict the various tasks GAIA-2 is trained on, including from scratch generations, context rollouts, and spatial inpainting. At inference, we can generate beyond the window duration GAIA-2 was trained on by taking overlapping strides to generate new frames conditioned on previously generated frames.

## 2 Model

GAIA-2 is a surround-view video generative world model with structured conditioning, multi-camera coherence, and high spatial-temporal resolution. The architecture ([Figure 2](https://arxiv.org/html/2503.20523v1#S1.F2 "In 1 Introduction â€£ GAIA-2: A Controllable Multi-View Generative World Model for Autonomous Driving")) is composed of two primary components: a video tokenizer and a latent world model. Together, these modules enable GAIA-2 to generate realistic and semantically coherent video across multiple viewpoints with rich conditional control.

GAIA-2ëŠ” êµ¬ì¡°í™”ëœ ì¡°ê±´ë¶€, ë‹¤ì¤‘ ì¹´ë©”ë¼ ì¼ê´€ì„±, ê·¸ë¦¬ê³  ë†’ì€ ê³µê°„-ì‹œê°„ í•´ìƒë„ë¥¼ ê°–ì¶˜ ì„œë¼ìš´ë“œ ë·° ë¹„ë””ì˜¤ ìƒì„± ì„¸ê³„ ëª¨ë¸ì…ë‹ˆë‹¤. ì´ ëª¨ë¸ì˜ ì•„í‚¤í…ì²˜([Figure 2](https://arxiv.org/html/2503.20523v1#S1.F2 "In 1 Introduction â€£ GAIA-2: A Controllable Multi-View Generative World Model for Autonomous Driving"))ëŠ” ë‘ ê°€ì§€ ì£¼ìš” êµ¬ì„± ìš”ì†Œë¡œ ì´ë£¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤: ë¹„ë””ì˜¤ í† í¬ë‚˜ì´ì €ì™€ ì ì¬ ì„¸ê³„ ëª¨ë¸ì…ë‹ˆë‹¤. ì´ ëª¨ë“ˆë“¤ì´ í•¨ê»˜ ì‘ë™í•˜ì—¬ GAIA-2ê°€ ë‹¤ì¤‘ ê´€ì ì—ì„œ í’ë¶€í•œ ì¡°ê±´ë¶€ ì œì–´ì™€ í•¨ê»˜ í˜„ì‹¤ì ì´ê³  ì˜ë¯¸ì ìœ¼ë¡œ ì¼ê´€ëœ ë¹„ë””ì˜¤ë¥¼ ìƒì„±í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.

The video tokenizer compresses the raw high-resolution video into a compact, continuous latent space, preserving semantic and temporal structure. This compact representation enables efficient learning and generation at scale. The world model then learns to predict future latent states, conditioned on past latent states, actions, and a diverse set of domain-specific control signals. It can also be used to generate novel states completely from scratch and modify video content through inpainting. The predicted latent states are subsequently decoded back into pixel space using the video tokenizer decoder.

ì €í¬ ì˜ìƒ í† í¬ë‚˜ì´ì €ëŠ” ê³ í•´ìƒë„ ì›ë³¸ ì˜ìƒì„ ì••ì¶•ëœ ì—°ì†ì ì¸ ì ì¬ ê³µê°„ìœ¼ë¡œ ë³€í™˜í•˜ì—¬, ì˜ë¯¸ì™€ ì‹œê°„ì  êµ¬ì¡°ë¥¼ ë³´ì¡´í•©ë‹ˆë‹¤. ì´ ì••ì¶•ëœ í‘œí˜„ì€ ëŒ€ê·œëª¨ í•™ìŠµ ë° ìƒì„±ì— íš¨ìœ¨ì„±ì„ ì œê³µí•©ë‹ˆë‹¤. ì„¸ê³„ ëª¨ë¸ì€ ê³¼ê±°ì˜ ì ì¬ ìƒíƒœ, í–‰ë™, ê·¸ë¦¬ê³  ë‹¤ì–‘í•œ ë„ë©”ì¸ë³„ ì œì–´ ì‹ í˜¸ì— ê¸°ë°˜í•˜ì—¬ ë¯¸ë˜ì˜ ì ì¬ ìƒíƒœë¥¼ ì˜ˆì¸¡í•˜ë„ë¡ í•™ìŠµí•©ë‹ˆë‹¤. ë˜í•œ, ì™„ì „íˆ ì²˜ìŒë¶€í„° ìƒˆë¡œìš´ ìƒíƒœë¥¼ ìƒì„±í•˜ê±°ë‚˜, ì¸í˜ì¸íŒ…ì„ í†µí•´ ì˜ìƒ ì½˜í…ì¸ ë¥¼ ìˆ˜ì •í•˜ëŠ” ë°ì—ë„ í™œìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆì¸¡ëœ ì ì¬ ìƒíƒœëŠ” ì˜ìƒ í† í¬ë‚˜ì´ì € ë””ì½”ë”ë¥¼ ì‚¬ìš©í•˜ì—¬ í”½ì…€ ê³µê°„ìœ¼ë¡œ ë‹¤ì‹œ ë””ì½”ë”©ë©ë‹ˆë‹¤.

In contrast to most latent diffusion models, GAIA-2 employs a much higher spatial compression rate (e.g., $32\times$ vs. the more typical $8\times$ ), which is offset by increasing the channel dimension of the latent space (e.g., 64 channels instead of 16). This yields fewer but semantically richer latent tokens. The resulting advantages are twofold: (1) shorter latent sequences enable faster inference and better memory efficiency, and (2) the model demonstrates improved ability to capture video content and temporal dynamics. This parameterization strategy is inspired by prior work on compact latent representations [^10].

ëŒ€ë¶€ë¶„ì˜ ì ì¬ í™•ì‚° ëª¨ë¸ê³¼ ë‹¬ë¦¬, GAIA-2ëŠ” í›¨ì”¬ ë†’ì€ ê³µê°„ ì••ì¶•ë¥ (ì˜ˆ: 32ë°° ëŒ€ 8ë°°)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ëŠ” ì ì¬ ê³µê°„ ì±„ë„ ì°¨ì›ì„ ëŠ˜ë¦¼ìœ¼ë¡œì¨ ìƒì‡„ë©ë‹ˆë‹¤(ì˜ˆ: 64ê°œì˜ ì±„ë„ ëŒ€ì‹  16ê°œì˜ ì±„ë„). ê·¸ ê²°ê³¼, ë” ì ì§€ë§Œ ì˜ë¯¸ì ìœ¼ë¡œ í’ë¶€í•œ ì ì¬ í† í°ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ì¥ì ì€ ë‘ ê°€ì§€ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì²«ì§¸, ì§§ì€ ì ì¬ ì‹œí€€ìŠ¤ëŠ” ë” ë¹ ë¥¸ ì¶”ë¡ ê³¼ í–¥ìƒëœ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ë‘˜ì§¸, ëª¨ë¸ì€ ë¹„ë””ì˜¤ ì½˜í…ì¸ ì™€ ì‹œê°„ì  ë™ì—­í•™ì„ í¬ì°©í•˜ëŠ” ëŠ¥ë ¥ì´ í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ íŒŒë¼ë¯¸í„°í™” ì „ëµì€ ì••ì¶•ëœ ì ì¬ í‘œí˜„ì— ëŒ€í•œ ì´ì „ ì—°êµ¬ì—ì„œ ì˜ê°ì„ ë°›ì•˜ìŠµë‹ˆë‹¤.

Unlike its predecessor GAIA-1 [^5], which relied on discrete latent variables, GAIA-2 employs a continuous latent space, improving temporal smoothness and reconstruction fidelity. Furthermore, GAIA-2 introduces a flexible conditioning interface that supports ego-vehicle actions, dynamic agent states (e.g., 3D bounding boxes), structured metadata, CLIP and scenario embeddings, and camera geometry. This design enables robust control over scene semantics and agent behavior during generation, while ensuring cross-view consistency and temporal coherence.

GAIA-2ëŠ” ì´ì „ ëª¨ë¸ì¸ GAIA-1[^5]ì™€ ë‹¬ë¦¬, ì—°ì†ì ì¸ ì ì¬ ê³µê°„ì„ í™œìš©í•˜ì—¬ ì‹œê°„ì  íë¦„ì„ ë”ìš± ë¶€ë“œëŸ½ê²Œ í•˜ê³  ì¬êµ¬ì„± ì •í™•ë„ë¥¼ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤. ë˜í•œ, GAIA-2ëŠ” ììœ¨ ì£¼í–‰ ì°¨ëŸ‰ì˜ í–‰ë™, ë™ì  ì—ì´ì „íŠ¸ ìƒíƒœ(ì˜ˆ: 3D ë°”ìš´ë”© ë°•ìŠ¤), êµ¬ì¡°í™”ëœ ë©”íƒ€ë°ì´í„°, CLIP ë° ì‹œë‚˜ë¦¬ì˜¤ ì„ë² ë”©, ì¹´ë©”ë¼ ê¸°í•˜í•™ ë“±ì„ ì§€ì›í•˜ëŠ” ìœ ì—°í•œ ì¡°ê±´í™” ì¸í„°í˜ì´ìŠ¤ë¥¼ ë„ì…í–ˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ì„¤ê³„ëŠ” ìƒì„± ê³¼ì •ì—ì„œ ì¥ë©´ ì˜ë¯¸ì™€ ì—ì´ì „íŠ¸ í–‰ë™ì— ëŒ€í•œ ê°•ë ¥í•œ ì œì–´ ê¸°ëŠ¥ì„ ì œê³µí•˜ë©°, ë™ì‹œì— ë‹¤ì¤‘ ë·° ì¼ê´€ì„±ê³¼ ì‹œê°„ì  ì‘ì§‘ì„±ì„ ë³´ì¥í•©ë‹ˆë‹¤.

### 2.1 Video Tokenizer

The video tokenizer compresses pixel-space video into a compact latent space that is continuous and semantically structured. It is composed of a space-time factorized transformer with an asymmetric encoder-decoder architecture (with 85M and 200M parameters, respectively). The encoder extracts spatiotemporally downsampled latents that are temporally independent; the decoder reconstructs full-frame video from these latents using temporal context for temporal consistency.

ì €í¬ ë¹„ë””ì˜¤ í† í¬ë‚˜ì´ì € ê¸°ìˆ ì€ í”½ì…€ ê³µê°„ì˜ ë¹„ë””ì˜¤ë¥¼ ì—°ì†ì ì´ê³  ì˜ë¯¸ì ìœ¼ë¡œ êµ¬ì¡°í™”ëœ, ì½¤íŒ©íŠ¸í•œ ì ì¬ ê³µê°„ìœ¼ë¡œ ì••ì¶•í•©ë‹ˆë‹¤. ì´ ê¸°ìˆ ì€ ê°ê° 85ë°±ë§Œ, 200ë°±ë§Œ íŒŒë¼ë¯¸í„°ë¥¼ ê°€ì§„ ë¹„ëŒ€ì¹­ ì¸ì½”ë”-ë””ì½”ë” ì•„í‚¤í…ì²˜ë¥¼ íŠ¹ì§•ìœ¼ë¡œ í•˜ëŠ” ê³µê°„-ì‹œê°„ ë¶„í•´ëœ íŠ¸ëœìŠ¤í¬ë¨¸ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì¸ì½”ë”ëŠ” ê³µê°„-ì‹œê°„ì ìœ¼ë¡œ ë‹¤ìš´ìƒ˜í”Œë§ëœ ì ì¬ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ë©°, ë””ì½”ë”ëŠ” ì‹œê°„ì  ì¼ê´€ì„±ì„ ìœ„í•´ ì´ëŸ¬í•œ ì ì¬ ë°ì´í„°ë¥¼ í™œìš©í•˜ì—¬ ì „ì²´ í”„ë ˆì„ ë¹„ë””ì˜¤ë¥¼ ì¬êµ¬ì„±í•©ë‹ˆë‹¤.

![Refer to caption](./figure/3.png)

Figure 3:GAIA-2 video tokenizer. The architecture schematic depicts input frames spatiotemporally downsampled into temporally independent latents. A high spatial compression rate is compensated for by an increased latent channel dimension. Latents are decoded back to video frames, leveraging full spatiotemporal attention to ensure temporal consistency. To autoencode long sequences, we employ a rolling inference mechanism where current latents are decoded using past and future context in a sliding window fashion. The examples show input frames, decoded frames, and a visualization of the latent space. The first 3 principal components of the latent features are mapped to RGB values. Notably, the latent space is semantically stable across frames and across samples.

#### 2.1.1 Encoder

Given an input video $({\mathbf{i}}_{1},...,{\mathbf{i}}_{T_{v}})$ , the encoder $e_{\phi}$ computes latent tokens $({\mathbf{z}}_{1},...,{\mathbf{z}}_{T_{L}})=e_{\phi}({\mathbf{i}}_{1},...,{%
\mathbf{i}}_{T_{v}})$ , where $T_{v}$ corresponds to the number of video frames and $T_{L}$ the number of latents. Let us denote by $H_{v}\times W_{v}$ the spatial resolution of the video frames and $H\times W$ the spatial resolution of the latent tokens. The encoder downsamples spatially by a factor $\frac{H_{v}}{H}=32$ and temporally by a factor $\frac{T_{v}}{T_{L}}=8$ . The latent dimension is $L=64$ , which results in a total compression of ( $\frac{T_{v}\times H_{v}\times W_{v}\times 3}{T_{L}\times H\times W\times L}=384)$ with $(T_{v},H_{v},W_{v})=(24,448,960)$ and $(T_{L},H,W)=(3,14,30)$ .

ì…ë ¥ ë¹„ë””ì˜¤ $({\mathbf{i}}_{1},...,{\mathbf{i}}_{T_{v}})$ ë¥¼ ì£¼ì‹œë©´, ì¸ì½”ë” $e_{\phi}$ëŠ” ì ì¬ í† í° $({\mathbf{z}}_{1},...,{\mathbf{z}}_{T_{L}})=e_{\phi}({\mathbf{i}}_{1},...,{\mathbf{i}}_{T_{v}})$ ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œ $T_{v}$ëŠ” ë¹„ë””ì˜¤ í”„ë ˆì„ì˜ ê°œìˆ˜ë¥¼ ë‚˜íƒ€ë‚´ë©°, $T_{L}$ì€ ì ì¬ í† í°ì˜ ê°œìˆ˜ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ë¹„ë””ì˜¤ í”„ë ˆì„ì˜ ê³µê°„ í•´ìƒë„ëŠ” $H_{v}\times W_{v}$ë¡œ, ì ì¬ í† í°ì˜ ê³µê°„ í•´ìƒë„ëŠ” $H\times W$ë¡œ í‘œí˜„ë©ë‹ˆë‹¤. ì¸ì½”ë”ëŠ” ê³µê°„ì ìœ¼ë¡œ 32ë°°, ì‹œê°„ì ìœ¼ë¡œ 8ë°°ë¡œ ë‹¤ìš´ìƒ˜í”Œë§í•©ë‹ˆë‹¤. ì ì¬ ì°¨ì›ì€ $L=64$ì´ë©°, ì´ëŠ” ì´ ì••ì¶•ë¥ ì´ ($\frac{T_{v}\times H_{v}\times W_{v}\times 3}{T_{L}\times H\times W\times L}=384$)ë¡œ ë‚˜íƒ€ë‚©ë‹ˆë‹¤.  ($T_{v},H_{v},W_{v}$)ëŠ” (24,448,960)ì´ê³ , ($T_{L},H,W$)ëŠ” (3,14,30)ì…ë‹ˆë‹¤.

This downsampling is achieved by temporally striding the input frames by $2\times$ and the following modules:

1. A downsampling convolutional block with stride $2\times 8\times 8$ (time, height, width) followed by another downsampling convolutional block with stride $2\times 2\times 2$ (both operating at embedding dimension $512$ ).
2. A series of $24$ spatial transformer blocks with dimension $512$ and $16$ heads.
3. A final convolution with stride $1\times 2\times 2$ , followed by a linear projection to $2L$ channels to model a Gaussian distribution over latents. Note that the latent dimension is doubled as the encoder predicts the mean and standard deviation of a Gaussian distribution. During both training and inference, the latents are sampled from this resulting distribution.

ì €í¬ëŠ” ì…ë ¥ í”„ë ˆì„ì„ ì‹œê°„ ë°©í–¥ìœ¼ë¡œ 2ë°°ì”© ìŠ¤íŠ¸ë¼ì´ë“œí•˜ì—¬, ê·¸ë¦¬ê³  ë‹¤ìŒ ëª¨ë“ˆë“¤ì„ í†µí•´ ë‹¤ìš´ìƒ˜í”Œë§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

1. ì‹œê°„(time), ë†’ì´(height), ë„ˆë¹„(width) ê°ê° 2x8x8 ìŠ¤íŠ¸ë¼ì´ë“œë¥¼ ê°–ëŠ” ë‹¤ìš´ìƒ˜í”Œë§ ì»¨ë³¼ë£¨ì…˜ ë¸”ë¡ê³¼, ì„ë² ë”© ì°¨ì› 512ì—ì„œ ì‘ë™í•˜ëŠ” ë˜ ë‹¤ë¥¸ ë‹¤ìš´ìƒ˜í”Œë§ ì»¨ë³¼ë£¨ì…˜ ë¸”ë¡ì´ ìˆìŠµë‹ˆë‹¤.
2. ì„ë² ë”© ì°¨ì› 512ì˜ 24ê°œì˜ ìŠ¤í˜ì…œ íŠ¸ëœìŠ¤í¬ë¨¸ ë¸”ë¡ê³¼ 16ê°œì˜ í—¤ë“œê°€ í¬í•¨ë©ë‹ˆë‹¤.
3. ë§ˆì§€ë§‰ìœ¼ë¡œ 1x2x2 ìŠ¤íŠ¸ë¼ì´ë“œë¥¼ ê°–ëŠ” ì»¨ë³¼ë£¨ì…˜ ì—°ì‚°ì„ ìˆ˜í–‰í•˜ê³ , ê·¸ë¦¬ê³  2L ì±„ë„ë¡œ ì„ í˜• íˆ¬ì˜ì„ í†µí•´ ì ì¬ ë¶„í¬(latent distribution)ì— ëŒ€í•œ ê°€ìš°ì‹œì•ˆ ë¶„í¬ë¥¼ ëª¨ë¸ë§í•©ë‹ˆë‹¤. ì ì¬ ì°¨ì›ì€ ì¸ì½”ë”ê°€ ê°€ìš°ì‹œì•ˆ ë¶„í¬ì˜ í‰ê· ê³¼ í‘œì¤€ í¸ì°¨ë¥¼ ì˜ˆì¸¡í•˜ê¸° ë•Œë¬¸ì— 2ë°°ë¡œ í™•ì¥ë©ë‹ˆë‹¤. í•™ìŠµ ê³¼ì •ê³¼ ì¶”ë¡  ê³¼ì • ëª¨ë‘ì—ì„œ ì ì¬ ê°’ì€ ì´ ê²°ê³¼ë¡œ ìƒì„±ëœ ë¶„í¬ì—ì„œ ìƒ˜í”Œë§ë©ë‹ˆë‹¤.

#### 2.1.2 Decoder

The decoder architecture is:

1. A linear projection from the latent dimension to the embedding dimension followed by a first upsampling convolutional block with stride $1\times 2\times 2$ (upsampling is achieved with a depth-to-space module [^12]).
2. A series of 16 space-time factorized transformer blocks with dimension $512$ and $16$ heads.
3. An upsampling convolutional block with stride $2\times 2\times 2$ followed by $8$ space-time factorized transformer blocks with dimension $512$ and $16$ heads.
4. A final upsampling convolutional block with stride $2\times 8\times 8$ and dimension $3$ corresponding to the pixel RGB channels.

ë””ì½”ë” ì•„í‚¤í…ì²˜ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

1. ì ì¬ ì°¨ì›(latent dimension)ì—ì„œ ì„ë² ë”© ì°¨ì›(embedding dimension)ë¡œ ì„ í˜• íˆ¬ì˜(linear projection)ì„ ìˆ˜í–‰í•œ í›„, ìŠ¤íŠ¸ë¼ì´ë“œ(stride)ê°€ $1\times 2\times 2$ì¸ ì²« ë²ˆì§¸ ì—…ìƒ˜í”Œë§(upsampling) ì»¨ë³¼ë£¨ì…˜ ë¸”ë¡(convolutional block)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì—…ìƒ˜í”Œë§ì€ ê¹Šì´-ê³µê°„ ëª¨ë“ˆ(depth-to-space module)ì„ í†µí•´ êµ¬í˜„ë©ë‹ˆë‹¤.
2. 512ì°¨ì›ê³¼ 16ê°œì˜ í—¤ë“œ(heads)ë¥¼ ê°€ì§„ 16ê°œì˜ ê³µê°„-ì‹œê°„ ë¶„í•´ëœ íŠ¸ëœìŠ¤í¬ë¨¸ ë¸”ë¡(space-time factorized transformer blocks)ìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.
3. ìŠ¤íŠ¸ë¼ì´ë“œ(stride)ê°€ $2\times 2\times 2$ì¸ ì—…ìƒ˜í”Œë§(upsampling) ì»¨ë³¼ë£¨ì…˜ ë¸”ë¡ ë‹¤ìŒì— ìŠ¤íŠ¸ë¼ì´ë“œ(stride)ê°€ $2\times 2\times 2$ì¸ ì—…ìƒ˜í”Œë§(upsampling) ì»¨ë³¼ë£¨ì…˜ ë¸”ë¡ ë‹¤ìŒì— 512ì°¨ì›ê³¼ 16ê°œì˜ í—¤ë“œ(heads)ë¥¼ ê°€ì§„ 8ê°œì˜ ê³µê°„-ì‹œê°„ ë¶„í•´ëœ íŠ¸ëœìŠ¤í¬ë¨¸ ë¸”ë¡(space-time factorized transformer blocks)ì´ ì´ì–´ì§‘ë‹ˆë‹¤.
4. ë§ˆì§€ë§‰ìœ¼ë¡œ ìŠ¤íŠ¸ë¼ì´ë“œ(stride)ê°€ $2\times 8\times 8$ì´ê³  3ì°¨ì›ìœ¼ë¡œ, RGB ì±„ë„ì— í•´ë‹¹í•˜ëŠ” ì°¨ì›ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.

A key difference between the encoder and decoder is that the encoder independently maps $8$ consecutive video frames to a single temporal latent, while the decoder jointly decodes the $T_{L}=3$ temporal latents to $T_{v}=24$ video frames for temporal consistency. During inference, the video frames are decoded with a sliding window. The logic is shown in diagrams â€˜ Training â€™ and â€˜ Inference â€™ of [Figure 3](https://arxiv.org/html/2503.20523v1#S2.F3 "In 2.1 Video Tokenizer â€£ 2 Model â€£ GAIA-2: A Controllable Multi-View Generative World Model for Autonomous Driving").

ì—”ì½”ë”ì™€ ë””ì½”ë”ì˜ í•µì‹¬ì ì¸ ì°¨ì´ì ì€, ì—”ì½”ë”ê°€ 8ê°œì˜ ì—°ì†ëœ ë¹„ë””ì˜¤ í”„ë ˆì„ì„ í•˜ë‚˜ì˜ í…œí¬ëŸ´ ì ëŸ°íŠ¸(temporal latent)ë¡œ ë…ë¦½ì ìœ¼ë¡œ ë§¤í•‘í•œë‹¤ëŠ” ì ì…ë‹ˆë‹¤. ë°˜ë©´, ë””ì½”ë”ëŠ” 3ê°œì˜ í…œí¬ëŸ´ ì ëŸ°íŠ¸(temporal latent)ë¥¼ í•¨ê»˜ ë””ì½”ë”©í•˜ì—¬ 24ê°œì˜ ë¹„ë””ì˜¤ í”„ë ˆì„ì„ ìƒì„±í•¨ìœ¼ë¡œì¨ ì‹œê°„ì  ì¼ê´€ì„±ì„ ìœ ì§€í•©ë‹ˆë‹¤. ì¶”ë¡  ê³¼ì •ì—ì„œëŠ” ìŠ¬ë¼ì´ë”© ìœˆë„ìš°(sliding window) ë°©ì‹ìœ¼ë¡œ ë¹„ë””ì˜¤ í”„ë ˆì„ì„ ë””ì½”ë”©í•©ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ [Figure 3]ì˜ â€˜Trainingâ€™ ë° â€˜Inferenceâ€™ ë‹¤ì´ì–´ê·¸ë¨ì„ ì°¸ê³ í•´ ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤.

#### 2.1.3 Training Losses

The video tokenizer is trained with a combination of pixel reconstruction and latent space losses:

- Image reconstruction with $L_{1}$ , $L_{2}$ , and perceptual losses [^13].
- DINO [^14] distillation on the latent features through a cosine similarity loss, encouraging semantic alignment with pre-trained representations. The video frames are encoded with the DINO model and temporally downsampled with linear interpolation to match the dimensionality of the latent features.
- Kullback-Leibler divergence loss [^1] with respect to a standard Gaussian distribution to regularize the latent space.

ì €í¬ ë¹„ë””ì˜¤ í† í¬ë‚˜ì´ì €(Video Tokenizer)ëŠ” í”½ì…€ ì¬êµ¬ì„±ê³¼ ì ì¬ ê³µê°„ ì†ì‹¤ì˜ ì¡°í•©ì„ í†µí•´ í•™ìŠµë˜ì—ˆìŠµë‹ˆë‹¤.

- $L_1$, $L_2$, ê·¸ë¦¬ê³  ì¸ì§€ ì†ì‹¤[^13]ì„ í™œìš©í•œ ì´ë¯¸ì§€ ì¬êµ¬ì„±ì„ í†µí•´ í•™ìŠµë˜ì—ˆìŠµë‹ˆë‹¤.
- DINO[^14] ì¦ë¥˜ë¥¼ í†µí•´ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì†ì‹¤ì„ ì‚¬ìš©í•˜ì—¬ ì ì¬ íŠ¹ì§•ê³¼ì˜ ì˜ë¯¸ì  ì •ë ¬ì„ ì¥ë ¤í•©ë‹ˆë‹¤. ì´ëŠ” ì‚¬ì „ í›ˆë ¨ëœ í‘œí˜„ê³¼ì˜ ì¼ê´€ì„±ì„ ë†’ì´ëŠ” ë° ëª©ì ì´ ìˆìŠµë‹ˆë‹¤. ë¹„ë””ì˜¤ í”„ë ˆì„ì€ DINO ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì¸ì½”ë”©ë˜ê³ , ì„ í˜• ë³´ê°„ë²•ì„ í†µí•´ ì ì¬ íŠ¹ì§•ì˜ ì°¨ì›ê³¼ ì¼ì¹˜í•˜ë„ë¡ ì‹œê°„ì ìœ¼ë¡œ ë‹¤ìš´ìƒ˜í”Œë§ë©ë‹ˆë‹¤.
- í‘œì¤€ ê°€ìš°ì‹œì•ˆ ë¶„í¬ì— ëŒ€í•œ ì¿¨ë°±-ë¼ì´ë¸”ëŸ¬ ë‹¤ì´ë²„ì „ìŠ¤ ì†ì‹¤[^1]ì„ ì‚¬ìš©í•˜ì—¬ ì ì¬ ê³µê°„ì„ ì •ê·œí™”í•©ë‹ˆë‹¤.

To improve visual quality, we further fine-tune the decoder with a GAN loss [^4] using a 3D convolutional discriminator and the image reconstruction losses, while keeping the encoder frozen. The discriminator is a series of residual 3D convolutional blocks with base channel 64, stride 2 in time and space with 3D blur pooling [^15], channel multipliers \[2, 4, 8, 8\], 3D instance normalization, and LeakyReLU with slope $0.2$ . It uses spectral normalization [^16] and the original GAN loss implemented with a softplus activation function.

ì‹œê°ì  í’ˆì§ˆì„ ë”ìš± í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´, ì €í¬ëŠ” 3D ì»¨ë³¼ë£¨ì…˜ ë””ìŠ¤í¬ë¦¬ë¯¸ë„¤ì´í„°ì™€ ì´ë¯¸ì§€ ì¬êµ¬ì„± ì†ì‹¤ì„ í™œìš©í•˜ì—¬ GAN ì†ì‹¤ë¡œ ë””ì½”ë”ë¥¼ ì¶”ê°€ì ìœ¼ë¡œ ë¯¸ì„¸ ì¡°ì •í–ˆìœ¼ë©°, ì¸ì½”ë”ëŠ” ë™ê²°í–ˆìŠµë‹ˆë‹¤. ë””ìŠ¤í¬ë¦¬ë¯¸ë„¤ì´í„°ëŠ” ê¸°ë³¸ ì±„ë„ 64, ì‹œê°„ ë° ê³µê°„ì—ì„œ ìŠ¤íŠ¸ë¼ì´ë“œ 2ë¥¼ ê°€ì§€ëŠ” ì”ë¥˜ 3D ì»¨ë³¼ë£¨ì…˜ ë¸”ë¡ì˜ ì‹œë¦¬ì¦ˆë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©°, 3D ë¸”ëŸ¬ í’€ë§[^15], ì±„ë„ ê³±ì…ˆ[2, 4, 8, 8], 3D ì¸ìŠ¤í„´ìŠ¤ ì •ê·œí™”, ê·¸ë¦¬ê³  ê¸°ìš¸ê¸° $0.2$ì˜ LeakyReLUë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ë˜í•œ, ìŠ¤í™íŠ¸ëŸ´ ì •ê·œí™”[^16]ë¥¼ ì ìš©í•˜ê³ , ì†Œí”„íŠ¸í”ŒëŸ¬ìŠ¤ í™œì„±í™” í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì›ë˜ GAN ì†ì‹¤ì„ êµ¬í˜„í•©ë‹ˆë‹¤.

### 2.2 World Model

The latent world model predicts future latent states conditioned on past latents, actions, and a rich set of conditioning inputs. It is implemented as a space-time factorized transformer with 8.4B parameters and is trained using flow matching [^17] for stability and sample efficiency.

ì ì¬ ì„¸ê³„ ëª¨ë¸ì€ ê³¼ê±°ì˜ ì ì¬ ìƒíƒœ, í–‰ë™, ê·¸ë¦¬ê³  í’ë¶€í•œ ì¡°ê±´ ì…ë ¥ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ë¯¸ë˜ì˜ ì ì¬ ìƒíƒœë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤. 8.4B íŒŒë¼ë¯¸í„°ë¡œ êµ¬ì„±ëœ ê³µê°„-ì‹œê°„ ë¶„í•´ëœ íŠ¸ëœìŠ¤í¬ë¨¸ í˜•íƒœë¡œ êµ¬í˜„ë˜ì—ˆìœ¼ë©°, ì•ˆì •ì„±ê³¼ ìƒ˜í”Œ íš¨ìœ¨ì„±ì„ ìœ„í•´ í”Œë¡œìš° ë§¤ì¹­ [^17] ë°©ì‹ìœ¼ë¡œ í›ˆë ¨ë˜ì—ˆìŠµë‹ˆë‹¤.

Let ${\mathbf{x}}_{1:T}\in\mathbb{R}^{T\times N\times H\times W\times L}$ be the input latents, where $T$ is the temporal window and $N$ is the number of cameras. The input latents are obtained by independently encoding each camera view with the encoder $e_{\phi}$ . At each timestep $t$ , we also provide an action vector ${\mathbf{a}}_{t}$ and a conditioning vector ${\mathbf{c}}_{t}$ .

ì…ë ¥ ì ì¬ê°’ ${\mathbf{x}}_{1:T}$ì€ $\mathbb{R}^{T\times N\times H\times W\times L}$ í˜•íƒœë¥¼ ê°€ì§€ë©°, ì—¬ê¸°ì„œ $T$ëŠ” ì‹œê°„ ì°½ í¬ê¸°, $N$ì€ ì¹´ë©”ë¼ ìˆ˜ì…ë‹ˆë‹¤. ì…ë ¥ ì ì¬ê°’ì€ ê° ì¹´ë©”ë¼ ë·°ë¥¼ ë…ë¦½ì ìœ¼ë¡œ ì¸ì½”ë” $e_{\phi}$ë¥¼ ì‚¬ìš©í•˜ì—¬ íšë“í•©ë‹ˆë‹¤. ê° ì‹œê°„ ë‹¨ê³„ $t$ì—ì„œ, ì•¡ì…˜ ë²¡í„° ${\mathbf{a}}_{t}$ì™€ ì¡°ê±´ ë²¡í„° ${\mathbf{c}}_{t}$ë„ í•¨ê»˜ ì œê³µí•©ë‹ˆë‹¤.

#### 2.2.1 Architecture

The world model is a space-time factorized transformer with hidden dimension $C$ . Each action ${\mathbf{a}}_{t}$ is embedded to $\mathbb{R}^{C}$ , and the conditioning vector ${\mathbf{c}}_{t}$ embedded to $\mathbb{R}^{K\times C}$ , where $K$ corresponds to the number of conditioning variables. The flow matching time $\tau\in[0,1]$ is also mapped to $\mathbb{R}^{C}$ using a sinusoidal encoding [^18].

ì„¸ê³„ ëª¨ë¸ì€ $C$ ì°¨ì›ì˜ ì€íëœ ê³µê°„-ì‹œê°„ ë¶„í•´ëœ íŠ¸ëœìŠ¤í¬ë¨¸ì…ë‹ˆë‹¤. ê° í–‰ë™ ${\mathbf{a}}_{t}$ì€ $\mathbb{R}^{C}$ ê³µê°„ìœ¼ë¡œ ì„ë² ë”©ë˜ë©°, ì¡°ê±´ ë²¡í„° ${\mathbf{c}}_{t}$ì€ $\mathbb{R}^{K\times C}$ ê³µê°„ìœ¼ë¡œ ì„ë² ë”©ë©ë‹ˆë‹¤. ì—¬ê¸°ì„œ $K$ëŠ” ì¡°ê±´ ë³€ìˆ˜ì˜ ê°œìˆ˜ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ë˜í•œ, íë¦„ ì¼ì¹˜ ì‹œê°„ $\tau[0,1]$ì€ ì‹œë…¸ìŠ¤ìš”idal ì¸ì½”ë”©[^18]ì„ ì‚¬ìš©í•˜ì—¬ $\mathbb{R}^{C}$ ê³µê°„ìœ¼ë¡œ ë§¤í•‘ë©ë‹ˆë‹¤.

Flow matching time $\tau$ and action ${\mathbf{a}}_{t}$ are injected into each transformer block through the adaptive layer norm [^19], while we use cross-attention for the other conditioning variables ${\mathbf{c}}_{t}$ . We have found that action conditioning was more accurate when using adaptive layer norm over cross-attention. As the action affects every spatial token, the adaptive layer norm provides an explicit information gateway instead of having to rely on a learnable attention mechanism.

ê° íŠ¸ëœìŠ¤í¬ë¨¸ ë¸”ë¡ì—ëŠ” ì ì‘í˜• ë ˆì´ì–´ ì •ê·œí™”[^19]ë¥¼ í†µí•´ íë¦„ ì¼ì¹˜ ì‹œê°„ $\tau$ì™€ ì•¡ì…˜ ${\mathbf{a}}_{t}$ê°€ ì£¼ì…ë˜ë©°, ë‹¤ë¥¸ ì¡°ê±´ ë³€ìˆ˜ ${\mathbf{c}}_{t}$ì—ëŠ” êµì°¨ ì–´í…ì…˜ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì•¡ì…˜ ì¡°ê±´í™”ê°€ êµì°¨ ì–´í…ì…˜ë³´ë‹¤ ì ì‘í˜• ë ˆì´ì–´ ì •ê·œí™”ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš° ë” ì •í™•í•œ ê²°ê³¼ë¥¼ ì–»ì€ ê²ƒì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤. ì•¡ì…˜ì€ ëª¨ë“  ê³µê°„ í† í°ì— ì˜í–¥ì„ ë¯¸ì¹˜ë¯€ë¡œ, ì ì‘í˜• ë ˆì´ì–´ ì •ê·œí™”ëŠ” í•™ìŠµ ê°€ëŠ¥í•œ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì— ì˜ì¡´í•˜ëŠ” ëŒ€ì‹  ëª…ì‹œì ì¸ ì •ë³´ í†µë¡œë¥¼ ì œê³µí•©ë‹ˆë‹¤.

Regarding positional encoding, we independently encode: (i) spatial token position with a sinusoidal embedding, (ii) camera timestamp with a sinusoidal embedding followed by a small MLP, and (iii) camera geometry (distortion, intrinsic, and extrinsics) with learnable linear layers. All these positional encodings are added to the input latents at the beginning of each transformer block, similar to [^20].

ìœ„ì¹˜ ì¸ì½”ë”©ì— ëŒ€í•´, ì €í¬ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ë…ë¦½ì ìœ¼ë¡œ ì¸ì½”ë”©í•©ë‹ˆë‹¤. ì²«ì§¸, ì‹œê³µê°„ í† í°ì˜ ìœ„ì¹˜ë¥¼ ì‚¬ì´Ğ½ÑƒÑ í•¨ìˆ˜ ê¸°ë°˜ ì„ë² ë”©ìœ¼ë¡œ, ë‘˜ì§¸, ì¹´ë©”ë¼ íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ì‚¬ì´Ğ½ÑƒÑ í•¨ìˆ˜ ê¸°ë°˜ ì„ë² ë”© í›„ ì‘ì€ MLPë¡œ, ì…‹ì§¸, ì¹´ë©”ë¼ ê¸°í•˜í•™(ì™œê³¡, ë‚´ë¶€, ì™¸ë¶€)ì„ í•™ìŠµ ê°€ëŠ¥í•œ ì„ í˜• ë ˆì´ì–´ë¡œ ì¸ì½”ë”©í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ëª¨ë“  ìœ„ì¹˜ ì¸ì½”ë”©ì€ ê° íŠ¸ëœìŠ¤í¬ë¨¸ ë¸”ë¡ì˜ ì‹œì‘ ì‹œì ì— ì…ë ¥ ì ì¬ ë²¡í„°ì— ë”í•´ì§‘ë‹ˆë‹¤. [^20]ê³¼ ìœ ì‚¬í•˜ê²Œ ë§ì´ì£ .

The world model contains $22$ space-time factorized transformer blocks with hidden dimension $C=4096$ and $32$ heads. Each transformer block contains a spatial attention (over space and cameras), a temporal attention, a cross-attention, and an MLP layer with an adaptive layer norm. For increased training stability, we use query-key normalization [^21] before each attention layer.

ì„¸ê³„ ëª¨ë¸ì€ 4096 ì°¨ì›ì˜ ì€ë‹‰ ë³€ìˆ˜(C)ë¥¼ ê°€ì§„ 22ê°œì˜ ê³µê°„-ì‹œê°„ ë¶„í•´ëœ íŠ¸ëœìŠ¤í¬ë¨¸ ë¸”ë¡ì„ í¬í•¨í•˜ê³  ìˆìœ¼ë©°, ê° ë¸”ë¡ì€ 32ê°œì˜ í—¤ë“œë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ê° íŠ¸ëœìŠ¤í¬ë¨¸ ë¸”ë¡ì€ ê³µê°„ ë° ì¹´ë©”ë¼ì— ëŒ€í•œ ê³µê°„ ì–´í…ì…˜, ì‹œê°„ ì–´í…ì…˜, êµì°¨ ì–´í…ì…˜, ê·¸ë¦¬ê³  ì ì‘í˜• ë ˆì´ì–´ ì •ê·œí™”ë¥¼ ê°–ëŠ” MLP ë ˆì´ì–´ë¥¼ í¬í•¨í•©ë‹ˆë‹¤. í›ˆë ¨ ì•ˆì •ì„±ì„ ë†’ì´ê¸° ìœ„í•´, ê° ì–´í…ì…˜ ë ˆì´ì–´ ì „ì— ì¿¼ë¦¬-í‚¤ ì •ê·œí™”ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

#### 2.2.2 Losses

At training time, we randomly sample the number of context frames $t\in\{0,...,T-1\}$ where $t=0$ corresponds to from scratch generation. We also sample a flow matching time $\tau\in[0,1]$ according to a pre-defined distribution (see [Section 2.2.4](https://arxiv.org/html/2503.20523v1#S2.SS2.SSS4 "2.2.4 Flow matching Time Distribution â€£ 2.2 World Model â€£ 2 Model â€£ GAIA-2: A Controllable Multi-View Generative World Model for Autonomous Driving") for more details). The context latents ${\mathbf{x}}_{1:t}$ are unchanged, while the future latents ${\mathbf{x}}_{t+1:T}$ are linearly interpolated with random Gaussian noise ${\bm{\epsilon}}_{t+1:T}\sim\mathcal{N}({\bm{0}},{\bm{I}})$ .

í›ˆë ¨ ê³¼ì •ì—ì„œ, ì»¨í…ìŠ¤íŠ¸ í”„ë ˆì„ ê°œìˆ˜ $t$ëŠ” $\{0, ..., T-1\}$ ë²”ìœ„ì—ì„œ ë¬´ì‘ìœ„ë¡œ ìƒ˜í”Œë§ë©ë‹ˆë‹¤. ì—¬ê¸°ì„œ $t=0$ì€ ì²˜ìŒë¶€í„° ìƒì„±í•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ë˜í•œ, ë¯¸ë¦¬ ì •ì˜ëœ ë¶„í¬ì— ë”°ë¼ íë¦„ ì¼ì¹˜ ì‹œê°„ $\tau$ë¥¼ ë¬´ì‘ìœ„ë¡œ ìƒ˜í”Œë§í•©ë‹ˆë‹¤. (ìì„¸í•œ ë‚´ìš©ì€ [Section 2.2.4](https://arxiv.org/html/2503.20523v1#S2.SS2.SSS4 "2.2.4 Flow matching Time Distribution â€£ 2.2 World Model â€£ 2 Model â€£ GAIA-2: A Controllable Multi-View Generative World Model for Autonomous Driving")ë¥¼ ì°¸ì¡°í•´ ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤). ì»¨í…ìŠ¤íŠ¸ ì ì¬ ë²¡í„° ${\mathbf{x}}_{1:t}$ëŠ” ë³€ê²½ë˜ì§€ ì•Šìœ¼ë©°, ë¯¸ë˜ ì ì¬ ë²¡í„° ${\mathbf{x}}_{t+1:T}$ëŠ” ëœë¤ ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆ ${\bm{\epsilon}}_{t+1:T}\sim\mathcal{N}({\bm{0}},{\bm{I}})$ë¡œ ì„ í˜• ë³´ê°„ë©ë‹ˆë‹¤.

<table><tbody><tr><td></td><td><math><semantics><mrow><msubsup><mi>ğ±</mi> <mrow><mrow><mi>t</mi> <mo>+</mo> <mn>1</mn></mrow><mo>:</mo><mi>T</mi></mrow> <mi>Ï„</mi></msubsup> <mo>=</mo> <mrow><mrow><mi>Ï„</mi> <mo>â¢</mo> <msub><mi>ğ±</mi> <mrow><mrow><mi>t</mi> <mo>+</mo> <mn>1</mn></mrow><mo>:</mo><mi>T</mi></mrow></msub></mrow> <mo>+</mo> <mrow><mrow><mo>(</mo><mrow><mn>1</mn> <mo>âˆ’</mo> <mi>Ï„</mi></mrow><mo>)</mo></mrow> <mo>â¢</mo> <msub><mi>Ïµ</mi> <mrow><mrow><mi>t</mi> <mo>+</mo> <mn>1</mn></mrow><mo>:</mo><mi>T</mi></mrow></msub></mrow></mrow></mrow> <annotation-xml><apply><apply><csymbol>superscript</csymbol> <apply><csymbol>subscript</csymbol> <ci>ğ±</ci><apply><ci>:</ci><apply><ci>ğ‘¡</ci> <cn>1</cn></apply> <ci>ğ‘‡</ci></apply></apply> <ci>ğœ</ci></apply> <apply><apply><ci>ğœ</ci> <apply><csymbol>subscript</csymbol> <ci>ğ±</ci><apply><ci>:</ci><apply><ci>ğ‘¡</ci> <cn>1</cn></apply> <ci>ğ‘‡</ci></apply></apply></apply> <apply><apply><cn>1</cn> <ci>ğœ</ci></apply> <apply><csymbol>subscript</csymbol> <ci>bold-italic-Ïµ</ci><apply><ci>:</ci><apply><ci>ğ‘¡</ci> <cn>1</cn></apply> <ci>ğ‘‡</ci></apply></apply></apply></apply></apply></annotation-xml> <annotation>{\mathbf{x}}_{t+1:T}^{\tau}=\tau{\mathbf{x}}_{t+1:T}+(1-\tau){\bm{\epsilon}}_{% t+1:T}</annotation> <annotation>bold_x start_POSTSUBSCRIPT italic_t + 1: italic_T end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_Ï„ end_POSTSUPERSCRIPT = italic_Ï„ bold_x start_POSTSUBSCRIPT italic_t + 1: italic_T end_POSTSUBSCRIPT + ( 1 - italic_Ï„ ) bold_italic_Ïµ start_POSTSUBSCRIPT italic_t + 1: italic_T end_POSTSUBSCRIPT</annotation></semantics></math></td><td></td><td rowspan="1"><span>(1)</span></td></tr></tbody></table>

The velocity target vector ${\mathbf{v}}_{t+1:T}$ is the difference between target latents and random noise:

<table><tbody><tr><td></td><td><math><semantics><mrow><msub><mi>ğ¯</mi> <mrow><mrow><mi>t</mi> <mo>+</mo> <mn>1</mn></mrow><mo>:</mo><mi>T</mi></mrow></msub> <mo>=</mo> <mrow><msub><mi>ğ±</mi> <mrow><mrow><mi>t</mi> <mo>+</mo> <mn>1</mn></mrow><mo>:</mo><mi>T</mi></mrow></msub> <mo>âˆ’</mo> <msub><mi>Ïµ</mi> <mrow><mrow><mi>t</mi> <mo>+</mo> <mn>1</mn></mrow><mo>:</mo><mi>T</mi></mrow></msub></mrow></mrow> <annotation-xml><apply><apply><csymbol>subscript</csymbol> <ci>ğ¯</ci><apply><ci>:</ci><apply><ci>ğ‘¡</ci> <cn>1</cn></apply> <ci>ğ‘‡</ci></apply></apply> <apply><apply><csymbol>subscript</csymbol> <ci>ğ±</ci><apply><ci>:</ci><apply><ci>ğ‘¡</ci> <cn>1</cn></apply> <ci>ğ‘‡</ci></apply></apply> <apply><csymbol>subscript</csymbol> <ci>bold-italic-Ïµ</ci><apply><ci>:</ci><apply><ci>ğ‘¡</ci> <cn>1</cn></apply> <ci>ğ‘‡</ci></apply></apply></apply></apply></annotation-xml> <annotation>{\mathbf{v}}_{t+1:T}={\mathbf{x}}_{t+1:T}-{\bm{\epsilon}}_{t+1:T}</annotation> <annotation>bold_v start_POSTSUBSCRIPT italic_t + 1: italic_T end_POSTSUBSCRIPT = bold_x start_POSTSUBSCRIPT italic_t + 1: italic_T end_POSTSUBSCRIPT - bold_italic_Ïµ start_POSTSUBSCRIPT italic_t + 1: italic_T end_POSTSUBSCRIPT</annotation></semantics></math></td><td></td><td rowspan="1"><span>(2)</span></td></tr></tbody></table>

The world model $f_{\theta}$ predicts the target velocity ${\mathbf{v}}_{t+1:T}$ conditioned on context latents ${\mathbf{x}}_{1:t}$ , actions ${\mathbf{a}}_{1:T}$ and conditioning variables ${\mathbf{c}}_{1:T}$ .

<table><tbody><tr><td></td><td><math><semantics><mrow><msub><mover><mi>ğ¯</mi> <mo>^</mo></mover> <mrow><mrow><mi>t</mi> <mo>+</mo> <mn>1</mn></mrow><mo>:</mo><mi>T</mi></mrow></msub> <mo>=</mo> <mrow><msub><mi>f</mi> <mi>Î¸</mi></msub> <mo>â¢</mo> <mrow><mo>(</mo><mrow><msubsup><mi>ğ±</mi> <mrow><mrow><mi>t</mi> <mo>+</mo> <mn>1</mn></mrow><mo>:</mo><mi>T</mi></mrow> <mi>Ï„</mi></msubsup> <mo>|</mo> <mrow><msub><mi>ğ±</mi> <mrow><mn>1</mn><mo>:</mo><mi>t</mi></mrow></msub><mo>,</mo><msub><mi>ğš</mi> <mrow><mn>1</mn><mo>:</mo><mi>T</mi></mrow></msub><mo>,</mo><msub><mi>ğœ</mi> <mrow><mn>1</mn><mo>:</mo><mi>T</mi></mrow></msub><mo>,</mo><mi>Ï„</mi></mrow></mrow><mo>)</mo></mrow></mrow></mrow> <annotation-xml><apply><apply><csymbol>subscript</csymbol> <apply><ci>^</ci> <ci>ğ¯</ci></apply><apply><ci>:</ci><apply><ci>ğ‘¡</ci> <cn>1</cn></apply> <ci>ğ‘‡</ci></apply></apply> <apply><apply><csymbol>subscript</csymbol> <ci>ğ‘“</ci> <ci>ğœƒ</ci></apply> <apply><csymbol>conditional</csymbol> <apply><csymbol>superscript</csymbol> <apply><csymbol>subscript</csymbol> <ci>ğ±</ci><apply><ci>:</ci><apply><ci>ğ‘¡</ci> <cn>1</cn></apply> <ci>ğ‘‡</ci></apply></apply> <ci>ğœ</ci></apply> <list><apply><csymbol>subscript</csymbol> <ci>ğ±</ci><apply><ci>:</ci><cn>1</cn> <ci>ğ‘¡</ci></apply></apply> <apply><csymbol>subscript</csymbol> <ci>ğš</ci><apply><ci>:</ci><cn>1</cn> <ci>ğ‘‡</ci></apply></apply> <apply><csymbol>subscript</csymbol> <ci>ğœ</ci><apply><ci>:</ci><cn>1</cn> <ci>ğ‘‡</ci></apply></apply> <ci>ğœ</ci></list></apply></apply></apply></annotation-xml> <annotation>\hat{{\mathbf{v}}}_{t+1:T}=f_{\theta}({\mathbf{x}}_{t+1:T}^{\tau}|{\mathbf{x}}% _{1:t},{\mathbf{a}}_{1:T},{\mathbf{c}}_{1:T},\tau)</annotation> <annotation>over^ start_ARG bold_v end_ARG start_POSTSUBSCRIPT italic_t + 1: italic_T end_POSTSUBSCRIPT = italic_f start_POSTSUBSCRIPT italic_Î¸ end_POSTSUBSCRIPT ( bold_x start_POSTSUBSCRIPT italic_t + 1: italic_T end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_Ï„ end_POSTSUPERSCRIPT | bold_x start_POSTSUBSCRIPT 1: italic_t end_POSTSUBSCRIPT, bold_a start_POSTSUBSCRIPT 1: italic_T end_POSTSUBSCRIPT, bold_c start_POSTSUBSCRIPT 1: italic_T end_POSTSUBSCRIPT, italic_Ï„ )</annotation></semantics></math></td><td></td><td rowspan="1"><span>(3)</span></td></tr></tbody></table>

The model is trained with an $L_{2}$ loss between the predicted and the target velocity.

<table><tbody><tr><td></td><td><math><semantics><mrow><msub><mi>â„’</mi> <mtext>world-model</mtext></msub> <mo>=</mo> <mrow><msub><mi>ğ”¼</mi> <mrow><mrow><mi>t</mi> <mo>âˆ¼</mo> <mrow><mi>P</mi> <mo>â¢</mo> <mrow><mo>(</mo><mi>t</mi><mo>)</mo></mrow></mrow></mrow><mo>,</mo><mrow><mi>Ï„</mi> <mo>âˆ¼</mo> <mrow><mi>p</mi> <mo>â¢</mo> <mrow><mo>(</mo><mi>Ï„</mi><mo>)</mo></mrow></mrow></mrow></mrow></msub> <mo>â¢</mo> <mrow><mo>[</mo><msup><mrow><mo>â€–</mo> <mrow><msub><mi>ğ¯</mi> <mrow><mrow><mi>t</mi> <mo>+</mo> <mn>1</mn></mrow><mo>:</mo><mi>T</mi></mrow></msub> <mo>âˆ’</mo> <msub><mover><mi>ğ¯</mi> <mo>^</mo></mover> <mrow><mrow><mi>t</mi> <mo>+</mo> <mn>1</mn></mrow><mo>:</mo><mi>T</mi></mrow></msub></mrow> <mo>â€–</mo></mrow> <mn>2</mn></msup><mo>]</mo></mrow></mrow></mrow> <annotation-xml><apply><apply><csymbol>subscript</csymbol> <ci>â„’</ci> <ci><mtext>world-model</mtext></ci></apply> <apply><apply><csymbol>subscript</csymbol> <ci>ğ”¼</ci> <apply><csymbol>formulae-sequence</csymbol> <apply><csymbol>similar-to</csymbol> <ci>ğ‘¡</ci> <apply><ci>ğ‘ƒ</ci> <ci>ğ‘¡</ci></apply></apply> <apply><csymbol>similar-to</csymbol> <ci>ğœ</ci> <apply><ci>ğ‘</ci> <ci>ğœ</ci></apply></apply></apply></apply> <apply><csymbol>delimited-[]</csymbol> <apply><csymbol>superscript</csymbol> <apply><csymbol>norm</csymbol> <apply><apply><csymbol>subscript</csymbol> <ci>ğ¯</ci><apply><ci>:</ci><apply><ci>ğ‘¡</ci> <cn>1</cn></apply> <ci>ğ‘‡</ci></apply></apply> <apply><csymbol>subscript</csymbol> <apply><ci>^</ci> <ci>ğ¯</ci></apply><apply><ci>:</ci><apply><ci>ğ‘¡</ci> <cn>1</cn></apply> <ci>ğ‘‡</ci></apply></apply></apply></apply> <cn>2</cn></apply></apply></apply></apply></annotation-xml> <annotation>\mathcal{L}_{\text{world-model}}=\mathbb{E}_{t\sim P(t),\tau\sim p(\tau)}[||{% \mathbf{v}}_{t+1:T}-\hat{{\mathbf{v}}}_{t+1:T}||^{2}]</annotation> <annotation>caligraphic_L start_POSTSUBSCRIPT world-model end_POSTSUBSCRIPT = blackboard_E start_POSTSUBSCRIPT italic_t âˆ¼ italic_P ( italic_t ), italic_Ï„ âˆ¼ italic_p ( italic_Ï„ ) end_POSTSUBSCRIPT [ | | bold_v start_POSTSUBSCRIPT italic_t + 1: italic_T end_POSTSUBSCRIPT - over^ start_ARG bold_v end_ARG start_POSTSUBSCRIPT italic_t + 1: italic_T end_POSTSUBSCRIPT | | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ]</annotation></semantics></math></td><td></td><td rowspan="1"><span>(4)</span></td></tr></tbody></table>

where $t\sim P(t)$ denotes the distribution of context frames and $\tau\sim p(\tau)$ the distribution of flow matching time.

#### 2.2.3 Conditioning

GAIA-2 supports rich and structured conditioning inputs that enable fine-grained control over the generated scenes. These inputs include ego-vehicle actions, dynamic agent properties, scene-level metadata, camera configurations, timestamp embedding, and external latent representations such as CLIP or proprietary scenario embeddings. The conditioning mechanisms are integrated into the world model through a combination of adaptive layer normalization (for action), additive modules (for camera geometry and timestamp), and cross-attention (for all other variables).

ì €í¬ GAIA-2ëŠ” ìƒì„±ë˜ëŠ” ì¥ë©´ì„ ì •ë°€í•˜ê²Œ ì œì–´í•  ìˆ˜ ìˆëŠ” í’ë¶€í•˜ê³  êµ¬ì¡°í™”ëœ ì¡°ê±´ë¶€ ì…ë ¥ë“¤ì„ ì§€ì›í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ì…ë ¥ì—ëŠ” ììœ¨ ì£¼í–‰ ì°¨ëŸ‰ì˜ ë™ì‘, ë™ì  ì—ì´ì „íŠ¸ ì†ì„±, ì¥ë©´ ìˆ˜ì¤€ ë©”íƒ€ë°ì´í„°, ì¹´ë©”ë¼ ì„¤ì •, íƒ€ì„ìŠ¤íƒ¬í”„ ì„ë² ë”©, CLIP ë˜ëŠ” ìì²´ì ì¸ ì‹œë‚˜ë¦¬ì˜¤ ì„ë² ë”©ê³¼ ê°™ì€ ì™¸ë¶€ ì ì¬ í‘œí˜„ ë“±ì´ í¬í•¨ë©ë‹ˆë‹¤. ì´ëŸ¬í•œ ì¡°ê±´ë¶€ ë©”ì»¤ë‹ˆì¦˜ì€ ì ì‘í˜• ë ˆì´ì–´ ì •ê·œí™”(í–‰ë™ì„ ìœ„í•œ), ì²¨ê°€ ëª¨ë“ˆ(ì¹´ë©”ë¼ ê¸°í•˜í•™ ë° íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ìœ„í•œ), ê·¸ë¦¬ê³  êµì°¨ ì–´í…ì…˜(ëª¨ë“  ë‹¤ë¥¸ ë³€ìˆ˜ë¥¼ ìœ„í•œ)ì„ í†µí•´ ì„¸ê³„ ëª¨ë¸ì— í†µí•©ë©ë‹ˆë‹¤.

##### Camera Parameters.

We compute separate embeddings for intrinsics, extrinsics, and distortion, which are then summed to form a unified camera encoding. For intrinsics, we extract focal lengths and principal point coordinates from the intrinsic matrix, normalize them, and project them into a shared latent space. Extrinsics and distortion coefficients are similarly processed through their respective encoders to yield compact representations. This configuration allows the model to effectively incorporate real-world camera variations. [Figure 5](https://arxiv.org/html/2503.20523v1#S5.F5 "In Classifier-Free Guidance. â€£ 5 Inference â€£ GAIA-2: A Controllable Multi-View Generative World Model for Autonomous Driving") illustrates the top three most common camera rig configurations in the training dataset.

ì €í¬ëŠ” ë‚´ë¶€(intrinsics), ì™¸ë¶€(extrinsics), ì™œê³¡(distortion)ì— ëŒ€í•´ ê°ê° ë³„ë„ì˜ ì„ë² ë”©ì„ ê³„ì‚°í•©ë‹ˆë‹¤. ì´ ì„ë² ë”©ë“¤ì„ í•©ì³ì„œ ì¼ê´€ëœ ì¹´ë©”ë¼ ì¸ì½”ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤. ë‚´ë¶€ ì •ë³´ì˜ ê²½ìš°, ë‚´ë¶€ í–‰ë ¬ì—ì„œ ì´ˆì  ê±°ë¦¬ì™€ ì£¼ì  ì¢Œí‘œë¥¼ ì¶”ì¶œí•˜ì—¬ ì •ê·œí™”í•˜ê³ , ê³µìœ ëœ ì ì¬ ê³µê°„ìœ¼ë¡œ íˆ¬ì˜í•©ë‹ˆë‹¤. ì™¸ë¶€ ì •ë³´ì™€ ì™œê³¡ ê³„ìˆ˜ëŠ” ê°ê°ì˜ ì¸ì½”ë”ë¥¼ í†µí•´ ì²˜ë¦¬í•˜ì—¬ ì••ì¶•ëœ í‘œí˜„ì„ ì–»ìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ êµ¬ì„±ì€ ëª¨ë¸ì´ ì‹¤ì œ ì¹´ë©”ë¼ì˜ ë³€ë™ì„±ì„ íš¨ê³¼ì ìœ¼ë¡œ í†µí•©í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤. [ê·¸ë¦¼ 5](https://arxiv.org/html/2503.20523v1#S5.F5 "In Classifier-Free Guidance. â€£ 5 Inference â€£ GAIA-2: A Controllable Multi-View Generative World Model for Autonomous Driving")ëŠ” í•™ìŠµ ë°ì´í„°ì…‹ì—ì„œ ê°€ì¥ í”í•˜ê²Œ ì‚¬ìš©ë˜ëŠ” ì„¸ ê°€ì§€ ì¹´ë©”ë¼  Rig êµ¬ì„±ì˜ ì˜ˆì‹œë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.

##### Video Frequency.

To account for variable video frame rates, GAIA-2 uses timestamp conditioning. Each timestamp is: (i) Normalized relative to the present time and scaled to the range $[-1,1]$ , (ii) Transformed using sinusoidal functions (Fourier feature encoding), and (iii) Passed through an MLP to produce a vector in the shared latent space. This encoding captures both low- and high-frequency temporal variation and enables the model to reason effectively over videos recorded at different rates.

ë‹¤ì–‘í•œ ë¹„ë””ì˜¤ í”„ë ˆì„ ì†ë„ë¥¼ ê³ ë ¤í•˜ê¸° ìœ„í•´ GAIA-2ëŠ” íƒ€ì„ìŠ¤íƒ¬í”„ ì¡°ê±´í™”ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ê° íƒ€ì„ìŠ¤íƒ¬í”„ëŠ” (i) í˜„ì¬ ì‹œê°„ ëŒ€ë¹„ ì •ê·œí™”ë˜ì–´ [-1, 1] ë²”ìœ„ë¡œ ì¡°ì •ë˜ê³ , (ii) ì •í˜„íŒŒ í•¨ìˆ˜(í‘¸ë¦¬ì— íŠ¹ì§• ì¸ì½”ë”©)ë¥¼ ì‚¬ìš©í•˜ì—¬ ë³€í™˜ë˜ë©°, (iii) ê³µìœ ëœ ì ì¬ ê³µê°„ì— ë²¡í„°ë¥¼ ìƒì„±í•˜ê¸° ìœ„í•´ MLPë¥¼ í†µê³¼ë©ë‹ˆë‹¤. ì´ ì¸ì½”ë”©ì€ ì €ì£¼íŒŒ ë° ê³ ì£¼íŒŒ ì‹œê°„ì  ë³€ë™ì„ ëª¨ë‘ í¬ì°©í•˜ì—¬ ì„œë¡œ ë‹¤ë¥¸ ì†ë„ë¡œ ì´¬ì˜ëœ ë¹„ë””ì˜¤ì— ëŒ€í•´ ëª¨ë¸ì´ íš¨ê³¼ì ìœ¼ë¡œ ì¶”ë¡ í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.

##### Action.

The ego-vehicle action is parameterized by speed and curvature. Since these quantities span multiple orders of magnitude, we use a symmetric logarithmic transformation symlog [^22] for normalization:

ì†ë„ì™€ ê³¡ë¥ ì— ë”°ë¼ ì„¤ì •ë˜ëŠ” â€˜ì´ê³  ì°¨ëŸ‰ ì•¡ì…˜â€™ì€ ì—¬ëŸ¬ ì°¨ë¡€ì˜ í¬ê¸° ë³€í™”ë¥¼ ê³ ë ¤í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ìš”ì†Œë“¤ì€ ìƒë‹¹íˆ í° ë²”ìœ„ë¥¼ ê°€ì§€ê³  ìˆê¸° ë•Œë¬¸ì—, ì •ê·œí™”ë¥¼ ìœ„í•´ ëŒ€ì¹­ ë¡œê·¸ ë³€í™˜(symlog)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

<table><tbody><tr><td></td><td><math><semantics><mrow><mrow><mtext>symlog</mtext> <mo>â¢</mo> <mrow><mo>(</mo><mi>y</mi><mo>)</mo></mrow></mrow> <mo>=</mo> <mrow><mtext>sign</mtext> <mo>â¢</mo> <mrow><mo>(</mo><mi>y</mi><mo>)</mo></mrow> <mo>â¢</mo> <mfrac><mrow><mi>log</mi> <mo>â¡</mo> <mrow><mo>(</mo><mrow><mn>1</mn> <mo>+</mo> <mrow><mi>s</mi> <mo>â¢</mo> <mrow><mo>|</mo> <mi>y</mi> <mo>|</mo></mrow></mrow></mrow><mo>)</mo></mrow></mrow> <mrow><mi>log</mi> <mo>â¡</mo> <mrow><mo>(</mo><mrow><mn>1</mn> <mo>+</mo> <mrow><mi>s</mi> <mo>â¢</mo> <mrow><mo>|</mo> <msub><mi>y</mi> <mtext>max</mtext></msub> <mo>|</mo></mrow></mrow></mrow><mo>)</mo></mrow></mrow></mfrac></mrow></mrow> <annotation-xml><apply><apply><ci><mtext>symlog</mtext></ci> <ci>ğ‘¦</ci></apply> <apply><ci><mtext>sign</mtext></ci> <ci>ğ‘¦</ci> <apply><apply><apply><cn>1</cn> <apply><ci>ğ‘ </ci> <apply><ci>ğ‘¦</ci></apply></apply></apply></apply> <apply><apply><cn>1</cn> <apply><ci>ğ‘ </ci> <apply><apply><csymbol>subscript</csymbol> <ci>ğ‘¦</ci> <ci><mtext>max</mtext></ci></apply></apply></apply></apply></apply></apply></apply></apply></annotation-xml> <annotation>\text{symlog}(y)=\text{sign}(y)\frac{\log(1+s|y|)}{\log(1+s|y_{\text{max}}|)}</annotation> <annotation>symlog ( italic_y ) = sign ( italic_y ) divide start_ARG roman_log ( 1 + italic_s | italic_y | ) end_ARG start_ARG roman_log ( 1 + italic_s | italic_y start_POSTSUBSCRIPT max end_POSTSUBSCRIPT | ) end_ARG</annotation></semantics></math></td><td></td><td rowspan="1"><span>(5)</span></td></tr></tbody></table>

Here, $y$ represents speed or curvature, and $s$ is a scale factor: For curvature (units: $m^{-1}$ , range: $0.0001$ to $0.1$ ), we use $s=1000$ to amplify low values. For speed (units: m/s, range: $00$ to $75$ ), we use $s=3.6$ to express values in km/h. The result is a compact representation scaled to $[-1,1]$ , improving training stability.

ì—¬ê¸°ì—ì„œ $y$ëŠ” ì†ë„ ë˜ëŠ” ê³¡ë¥ ì„ ë‚˜íƒ€ë‚´ë©°, $s$ëŠ” ì²™ë„ ì¸ìì…ë‹ˆë‹¤. ê³¡ë¥ (ë‹¨ìœ„: $m^{-1}$, ë²”ìœ„: 0.0001 ~ 0.1)ì˜ ê²½ìš°, ë‚®ì€ ê°’ì„ ì¦í­í•˜ê¸° ìœ„í•´ $s=1000$ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì†ë„(ë‹¨ìœ„: m/s, ë²”ìœ„: 0 ~ 75)ì˜ ê²½ìš°, $s=3.6$ì„ ì‚¬ìš©í•˜ì—¬ ê°’ì„ km/h ë‹¨ìœ„ë¡œ í‘œí˜„í•©ë‹ˆë‹¤. ê²°ê³¼ëŠ” [-1, 1] ë²”ìœ„ë¡œ ì¶•ì†Œëœ í‘œí˜„ìœ¼ë¡œ ë‚˜íƒ€ë‚´ì–´ í›ˆë ¨ ì•ˆì •ì„±ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.

##### Dynamic Agents.

To represent surrounding agents, we use 3D bounding boxes predicted by a 3D object detector [^23] re-trained on our dataset. Each box encodes the 3D location, orientation, dimensions, and category of an agent. The 3D boxes are projected into the 2D image plane and normalized, yielding $f_{i}\in\mathbb{R}^{T\times N\times B\times 13}$ conditioning features where $T$ denotes the number of temporal latents, $N$ the number of cameras, and $B$ the maximum number of 3D bounding boxes (zero-padding as needed). Each feature dimension is embedded independently and aggregated via a single-layer MLP.

ì£¼ë³€ ê°ì²´ë¥¼ í‘œí˜„í•˜ê¸° ìœ„í•´, ì €í¬ëŠ” ë‹¹ì‚¬ì˜ ë°ì´í„°ì…‹ìœ¼ë¡œ ì¬í•™ìŠµëœ 3D ê°ì²´ íƒì§€ê¸°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜ˆì¸¡ëœ 3D ë°”ìš´ë”© ë°•ìŠ¤ë¥¼ í™œìš©í•©ë‹ˆë‹¤. ê° ë°•ìŠ¤ëŠ” ê°ì²´ì˜ 3ì°¨ì› ìœ„ì¹˜, ë°©í–¥, í¬ê¸°, ê·¸ë¦¬ê³  ê°ì²´ ì¢…ë¥˜ë¥¼ ë‹´ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ 3ì°¨ì› ë°•ìŠ¤ëŠ” 2ì°¨ì› ì´ë¯¸ì§€ í‰ë©´ìœ¼ë¡œ íˆ¬ì˜ë˜ê³  ì •ê·œí™”ë˜ì–´, $f_{i}\in\mathbb{R}^{T\times N\times B\times 13}$ í˜•íƒœì˜ íŠ¹ì§• ë²¡í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œ $T$ëŠ” ì‹œê°„ì  ì ì¬ ë³€ìˆ˜ì˜ ê°œìˆ˜ë¥¼, $N$ì€ ì¹´ë©”ë¼ì˜ ê°œìˆ˜ë¥¼, ê·¸ë¦¬ê³  $B$ëŠ” ìµœëŒ€ 3D ë°”ìš´ë”© ë°•ìŠ¤ ê°œìˆ˜(í•„ìš”ì— ë”°ë¼ 0ìœ¼ë¡œ ì±„ì›€)ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ê° íŠ¹ì§• ì°¨ì›ì€ ë…ë¦½ì ìœ¼ë¡œ ì„ë² ë”©ë˜ê³  ë‹¨ì¼ ë ˆì´ì–´ MLPë¥¼ í†µí•´ ì§‘ê³„ë©ë‹ˆë‹¤.

To enhance model robustness and generalizability, we implement dropout at both the feature dimension and instance levels during training. Specifically, feature dimensions are dropped out with a probability of $p=0.3$ , allowing the model to operate under incomplete information at inference. This setup allows, for example, conditioning on 2D projected boxes without specifying the 3D locations of instances, or omitting orientations to let the model predict the most plausible orientation based on other conditions.

ëª¨ë¸ì˜ ê°•ê±´ì„±ê³¼ ì¼ë°˜í™” ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´, í•™ìŠµ ê³¼ì •ì—ì„œ íŠ¹ì§• ì°¨ì› ë° ì¸ìŠ¤í„´ìŠ¤ ìˆ˜ì¤€ ëª¨ë‘ì—ì„œ ë“œë¡­ì•„ì›ƒì„ ì ìš©í•©ë‹ˆë‹¤. êµ¬ì²´ì ìœ¼ë¡œ, íŠ¹ì§• ì°¨ì›ì—ì„œëŠ” $p=0.3$ì˜ í™•ë¥ ë¡œ ë“œë¡­ì•„ì›ƒì„ ì ìš©í•˜ì—¬, ì¶”ë¡  ì‹œ ë¶ˆì™„ì „í•œ ì •ë³´ í•˜ì—ì„œ ëª¨ë¸ì´ ì‘ë™í•˜ë„ë¡ í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì¸ìŠ¤í„´ìŠ¤ì˜ 3D ìœ„ì¹˜ë¥¼ ëª…ì‹œí•˜ì§€ ì•Šê³  2D íˆ¬ì˜ ìƒìë¥¼ ì¡°ê±´ìœ¼ë¡œ í™œìš©í•˜ê±°ë‚˜, ë°©í–¥ ì •ë³´ë¥¼ ì œê±°í•˜ì—¬ ëª¨ë¸ì´ ë‹¤ë¥¸ ì¡°ê±´ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ê°€ì¥ ê°€ëŠ¥ì„± ë†’ì€ ë°©í–¥ì„ ì˜ˆì¸¡í•˜ë„ë¡ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

At the instance level, for each camera, we sample a frame $t\in\{1,...,T\}$ and calculate the number of detected instances $N_{\text{instances}}$ . We then sample the number of instances to condition on $n\in\{0,...,\min(B,N_{\text{instances}})\}$ , and apply dropout to the instances exceeding this sample size. This allows the model to adapt to a variable number of dynamic agents during inference. Note that while keeping $n$ constant across time, we do not use instance tracking, allowing the model to independently determine whether the conditioning features across frames belong to the same or different instances.

ê° ì¹´ë©”ë¼ë³„ë¡œ, íŠ¹ì • ì‹œì  $t$ (1ë¶€í„° $T$ê¹Œì§€)ë¥¼ ìƒ˜í”Œë§í•˜ì—¬ ê²€ì¶œëœ ì¸ìŠ¤í„´ìŠ¤ ìˆ˜ $N_{\text{instances}}$ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. ì´í›„, $n$ (0ë¶€í„° $\min(B, N_{\text{instances}})$ê¹Œì§€)ì„ ìƒ˜í”Œë§í•˜ì—¬ ì¡°ê±´ë¶€ í•™ìŠµì„ ìˆ˜í–‰í•˜ê³ , ì´ ìƒ˜í”Œ í¬ê¸°ë¥¼ ì´ˆê³¼í•˜ëŠ” ì¸ìŠ¤í„´ìŠ¤ì— ë“œë¡­ì•„ì›ƒì„ ì ìš©í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì¶”ë¡  ê³¼ì •ì—ì„œ ë™ì  ì—ì´ì „íŠ¸ì˜ ê°œìˆ˜ê°€ ë³€í•¨ì— ë”°ë¼ ëª¨ë¸ì´ ì ì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì‹œê°„ ê²½ê³¼ì— ë”°ë¼ $n$ ê°’ì„ ì¼ì •í•˜ê²Œ ìœ ì§€í•˜ë©´ì„œë„ ì¸ìŠ¤í„´ìŠ¤ ì¶”ì ì„ ì‚¬ìš©í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì—, ëª¨ë¸ì´ ê° í”„ë ˆì„ì˜ ì¡°ê±´ë¶€ íŠ¹ì§•ì´ ë™ì¼í•œ ì¸ìŠ¤í„´ìŠ¤ì— ì†í•˜ëŠ”ì§€ ë˜ëŠ” ë‹¤ë¥¸ ì¸ìŠ¤í„´ìŠ¤ì— ì†í•˜ëŠ”ì§€ë¥¼ ë…ë¦½ì ìœ¼ë¡œ íŒë‹¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

##### Metadata.

Metadata features are categorical and embedded using dedicated learnable embedding layers. These include: Country, weather, time of day; Speed limits; Number and types of lanes (e.g., bus, cycle); Pedestrian crossings, traffic lights and their states; One-way road indicators and intersection types. These embeddings allow GAIA-2 to learn nuanced relationships between scene-level features and their effects on behavior, enabling simulation of both typical and rare scenarios.

ë°ì´í„° íŠ¹ì§•ì€ íŠ¹í™”ëœ í•™ìŠµ ê°€ëŠ¥í•œ ì„ë² ë”© ë ˆì´ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ ë²”ì£¼í˜•ìœ¼ë¡œ í‘œí˜„ë˜ë©°, ë‹¤ìŒê³¼ ê°™ì€ ë‚´ìš©ì„ í¬í•¨í•©ë‹ˆë‹¤. êµ­ê°€, ë‚ ì”¨, ì‹œê°„ëŒ€; ì†ë„ ì œí•œ; ì°¨ì„  ìˆ˜ì™€ ì¢…ë¥˜(ì˜ˆ: ë²„ìŠ¤, ìì „ê±° ì°¨ì„ ); ë³´í–‰ì íš¡ë‹¨ë¡œë‚˜, ì‹ í˜¸ë“± ë° ìƒíƒœ; ë‹¨ë°©í–¥ ë„ë¡œ í‘œì‹œ ë° êµì°¨ë¡œ ìœ í˜•ì…ë‹ˆë‹¤. ì´ëŸ¬í•œ ì„ë² ë”©ì„ í†µí•´ GAIA-2ê°€ ì¥ë©´ ìˆ˜ì¤€ì˜ íŠ¹ì§•ê³¼ í–‰ë™ì— ë¯¸ì¹˜ëŠ” ë¯¸ë¬˜í•œ ê´€ê³„ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ í•˜ì—¬, ì¼ë°˜ì ì¸ ì‹œë‚˜ë¦¬ì˜¤ì™€ ë“œë¬¸ ì‹œë‚˜ë¦¬ì˜¤ ëª¨ë‘ë¥¼ ì‹œë®¬ë ˆì´ì…˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

##### CLIP Embedding.

To enable semantic scene conditioning, GAIA-2 supports conditioning on CLIP embeddings [^24]. During training, we extract CLIP features using the image encoder on video frames. At inference, these can be replaced with CLIP text encoder outputs from natural language prompts. All CLIP embeddings are projected into the modelâ€™s latent space using a learnable linear projection. This enables zero-shot control over scene semantics through natural language or visual similarity.

ì‹œë§¨í‹± ì¥ë©´ ì¡°ê±´í™”ë¥¼ í™œì„±í™”í•˜ê¸° ìœ„í•´ GAIA-2ëŠ” CLIP ì„ë² ë”©ì— ëŒ€í•œ ì¡°ê±´í™”ë¥¼ ì§€ì›í•©ë‹ˆë‹¤ [^24]. í•™ìŠµ ê³¼ì • ì¤‘, ì €í¬ëŠ” ë¹„ë””ì˜¤ í”„ë ˆì„ì— ëŒ€í•´ ì´ë¯¸ì§€ ì¸ì½”ë”ë¥¼ ì‚¬ìš©í•˜ì—¬ CLIP íŠ¹ì§•ì„ ì¶”ì¶œí•©ë‹ˆë‹¤. ì¶”ë¡  ë‹¨ê³„ì—ì„œëŠ” ì´ëŸ¬í•œ íŠ¹ì§•ì„ ìì—°ì–´ í”„ë¡¬í”„íŠ¸ì—ì„œ ìƒì„±ëœ CLIP í…ìŠ¤íŠ¸ ì¸ì½”ë” ì¶œë ¥ìœ¼ë¡œ ëŒ€ì²´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëª¨ë“  CLIP ì„ë² ë”©ì€ í•™ìŠµ ê°€ëŠ¥í•œ ì„ í˜• íˆ¬ì˜ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì˜ ì ì¬ ê³µê°„ìœ¼ë¡œ íˆ¬ì˜ë©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ìì—°ì–´ ë˜ëŠ” ì‹œê°ì  ìœ ì‚¬ì„±ì„ í†µí•´ ì¥ë©´ì˜ ì˜ë¯¸ì— ëŒ€í•œ ì œì–´ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.

##### Scenario Embedding.

GAIA-2 can also be conditioned on scenario embeddings obtained from an internal proprietary model trained to encode driving-specific information. These embeddings compactly capture ego-action and scene context, such as road layout and agent configurations. The scenario vectors are projected via a learnable linear layer into the latent space before integration into the transformer. This allows high-level scenario generation from a compact abstract representation.

GAIA-2ëŠ” ë‚´ë¶€ì ìœ¼ë¡œ ê°œë°œëœ íŠ¹í—ˆ ëª¨ë¸ë¡œë¶€í„° ì–»ì€, ìš´ì „ ê´€ë ¨ ì •ë³´ë¥¼ ì••ì¶•ì ìœ¼ë¡œ ë‹´ê³  ìˆëŠ” ì‹œë‚˜ë¦¬ì˜¤ ì„ë² ë”©ì„ í†µí•´ ì¡°ê±´ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ì„ë² ë”©ì€ ìš´ì „ìì˜ í–‰ë™ê³¼ ì£¼ë³€ í™˜ê²½ ë§¥ë½, ì˜ˆë¥¼ ë“¤ì–´ ë„ë¡œ êµ¬ì¡° ë° ì°¨ëŸ‰ êµ¬ì„±ê³¼ ê°™ì€ ì •ë³´ë¥¼ ê°„ê²°í•˜ê²Œ í¬ì°©í•©ë‹ˆë‹¤. ì‹œë‚˜ë¦¬ì˜¤ ë²¡í„°ëŠ” íŠ¸ëœìŠ¤í¬ë¨¸ì— í†µí•©ë˜ê¸° ì „ì— í•™ìŠµ ê°€ëŠ¥í•œ ì„ í˜• ë ˆì´ì–´ë¥¼ í†µí•´ ì ì¬ ê³µê°„ìœ¼ë¡œ íˆ¬ì˜ë©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ê°„ê²°í•œ ì¶”ìƒ í‘œí˜„ìœ¼ë¡œë¶€í„° ê³ ìˆ˜ì¤€ì˜ ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.

#### 2.2.4 Flow matching Time Distribution

A critical factor for training the world model under the flow matching framework is the distribution of the flow matching time $\tau$ . This distribution determines how frequently the model sees latent inputs that are close to real latents versus heavily perturbed.

ì„¸ê³„ ëª¨ë¸ì„ íë¦„ ì¼ì¹˜ í”„ë ˆì„ì›Œí¬ í•˜ì—ì„œ í›ˆë ¨í•˜ëŠ” ë° ìˆì–´ ì¤‘ìš”í•œ ìš”ì†ŒëŠ” íë¦„ ì¼ì¹˜ ì‹œê°„ $\tau$ì˜ ë¶„í¬ì…ë‹ˆë‹¤. ì´ ë¶„í¬ëŠ” ëª¨ë¸ì´ ì‹¤ì œ ì ì¬ ë°ì´í„°ì™€ ì–¼ë§ˆë‚˜ ìì£¼ ìœ ì‚¬í•˜ê±°ë‚˜ ì‹¬í•˜ê²Œ ì™œê³¡ëœ ì ì¬ ë°ì´í„°ì— ë…¸ì¶œë˜ëŠ”ì§€ë¥¼ ê²°ì •í•©ë‹ˆë‹¤.

We use a bimodal logit-normal distribution with two modes:

- A primary mode centered at $\mu=0.5$ , $\sigma=1.4$ , sampled with probability $p=0.8$ . This biases the model towards learning with low-to-moderate noise levels. Empirically, this encourages learning useful gradients, as even small amounts of noise can significantly perturb high-capacity latents.
- A secondary mode centered at $\mu=-3.0$ , $\sigma=1.0$ , sampled with probability $p=0.2$ . This concentrates training on nearly pure noise regions around $\tau=0$ , helping the model learn spatial structures and low-level dynamics, such as ego-motion or object trajectories.

ì €í¬ëŠ” ì´ì¤‘ ëª¨ë“œ ë¡œì§“-ì •ê·œ ë¶„í¬ë¥¼ ì‚¬ìš©í•˜ë©°, ë‘ ê°œì˜ ì¤‘ì‹¬ ëª¨ë“œë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.

- ì£¼ ëª¨ë“œëŠ” í‰ê· (Î¼)ì´ 0.5, í‘œì¤€í¸ì°¨(Ïƒ)ê°€ 1.4ë¡œ ì„¤ì •ë˜ì–´ ìˆìœ¼ë©°, í™•ë¥ (p) 0.8ë¡œ ìƒ˜í”Œë§ë©ë‹ˆë‹¤. ì´ëŠ” ëª¨ë¸ì´ ë‚®ì€ ìˆ˜ì¤€ì—ì„œ ì¤‘ê°„ ì •ë„ì˜ ë…¸ì´ì¦ˆ ìˆ˜ì¤€ìœ¼ë¡œ í•™ìŠµí•˜ë„ë¡ í¸í–¥ì‹œí‚¤ëŠ” íš¨ê³¼ê°€ ìˆìŠµë‹ˆë‹¤. ì‹¤ì œë¡œ, ì‹¬ì§€ì–´ ì•„ì£¼ ì‘ì€ ì–‘ì˜ ë…¸ì´ì¦ˆë„ ê³ ìš©ëŸ‰ì˜ ì ì¬ ë³€ìˆ˜(latent)ë¥¼ ìƒë‹¹ ë¶€ë¶„ ë³€í™”ì‹œì¼œ ìœ ìš©í•œ ê¸°ìš¸ê¸° í•™ìŠµì„ ìœ ë„í•©ë‹ˆë‹¤.

- ë³´ì¡° ëª¨ë“œëŠ” í‰ê· (Î¼)ì´ -3.0, í‘œì¤€í¸ì°¨(Ïƒ)ê°€ 1.0ìœ¼ë¡œ ì„¤ì •ë˜ì–´ ìˆìœ¼ë©°, í™•ë¥ (p) 0.2ë¡œ ìƒ˜í”Œë§ë©ë‹ˆë‹¤. ì´ëŠ” Ï„=0 ì£¼ë³€ì˜ ìˆœìˆ˜í•œ ë…¸ì´ì¦ˆ ì˜ì—­ì— ëŒ€í•œ í•™ìŠµì„ ì§‘ì¤‘ì‹œì¼œ, ëª¨ë¸ì´ ê³µê°„ì  êµ¬ì¡°ì™€ ì €ìˆ˜ì¤€ ë™ì—­í•™, ì˜ˆë¥¼ ë“¤ì–´ ìê¸° ë™ì„  ë˜ëŠ” ê°ì²´ ì¶”ì ê³¼ ê°™ì€ ê²ƒì„ í•™ìŠµí•˜ë„ë¡ ë•ìŠµë‹ˆë‹¤.

This bimodal strategy ensures that training is effective across both low and high-noise regimes, improving generalization and sample quality.

ì €í¬ì˜ ì´ ì´ì› ì „ëµì€ ì €ì¡ìŒê³¼ ê³ ì¡ìŒ í™˜ê²½ ëª¨ë‘ì—ì„œ í›ˆë ¨ íš¨ê³¼ë¥¼ ë³´ì¥í•˜ì—¬ ì¼ë°˜í™” ëŠ¥ë ¥ê³¼ ìƒ˜í”Œ í’ˆì§ˆì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.

In addition, the input latents ${\mathbf{x}}_{t}$ are normalized by their mean $\mu_{x}$ and standard deviation $\sigma_{x}$ , following [^25], to ensure their magnitude matches that of the added Gaussian noise. This avoids scale mismatch between the signal and the perturbation, which can otherwise degrade training dynamics.

ë˜í•œ, ì…ë ¥ ì ì¬ ë³€ìˆ˜ ${\mathbf{x}}_{t}$ëŠ” í‰ê·  $\mu_{x}$ì™€ í‘œì¤€ í¸ì°¨ $\sigma_{x}$ë¥¼ ì‚¬ìš©í•˜ì—¬ ì •ê·œí™”í•˜ë©°, [^25]ì— ë”°ë¼ ì§„í–‰ë©ë‹ˆë‹¤. ì´ëŠ” ì¶”ê°€ëœ ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆì˜ í¬ê¸°ì™€ ì¼ì¹˜í•˜ë„ë¡ ë³´ì¥í•˜ê¸° ìœ„í•¨ì…ë‹ˆë‹¤. ì´ë ‡ê²Œ í•¨ìœ¼ë¡œì¨ ì‹ í˜¸ì™€ ë³€ë™ ê°„ì˜ í¬ê¸° ë¶ˆì¼ì¹˜ ë¬¸ì œë¥¼ ë°©ì§€í•˜ê³ , í•™ìŠµ ë™ì—­í•™ì„ ì €í•´í•  ìˆ˜ ìˆëŠ” ìƒí™©ì„ ë§‰ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## 3 Data

GAIA-2 is trained on a large-scale internal dataset specifically curated to support the diverse demands of video generation for autonomous driving. The dataset comprises approximately 25 million video sequences, each spanning 2 seconds, collected between 2019 and 2024. Recordings were obtained across three countriesâ€”the United Kingdom, the United States, and Germanyâ€”to ensure coverage of geographically and environmentally diverse driving conditions.

GAIA-2ëŠ” ììœ¨ì£¼í–‰ì„ ìœ„í•œ ë‹¤ì–‘í•œ ë¹„ë””ì˜¤ ìƒì„± ìš”êµ¬ì‚¬í•­ì„ ì§€ì›í•˜ê¸° ìœ„í•´ íŠ¹ë³„íˆ êµ¬ì„±ëœ ëŒ€ê·œëª¨ ë‚´ë¶€ ë°ì´í„°ì…‹ìœ¼ë¡œ í›ˆë ¨ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ ë°ì´í„°ì…‹ì€ 2019ë…„ë¶€í„° 2024ë…„ê¹Œì§€ ìˆ˜ì§‘ëœ ì•½ 2,500ë§Œ ê°œì˜ ë¹„ë””ì˜¤ ì‹œí€€ìŠ¤ë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©°, ê° ì‹œí€€ìŠ¤ëŠ” 2ì´ˆì˜ ê¸¸ì´ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ì˜êµ­, ë¯¸êµ­, ë…ì¼ ë“± ì„¸ ë‚˜ë¼ì—ì„œ ì´¬ì˜ëœ ì˜ìƒë“¤ì„ í†µí•´ ì§€ë¦¬ì  ë° í™˜ê²½ì ìœ¼ë¡œ ë‹¤ì–‘í•œ ì£¼í–‰ í™˜ê²½ì„ í¬ê´„ì ìœ¼ë¡œ í™•ë³´í–ˆìŠµë‹ˆë‹¤.

To capture the complexity of real-world autonomous driving, data collection involved multiple vehicle platforms, including three different car models and two van types. Each vehicle was outfitted with either five or six cameras, configured to provide comprehensive 360-degree surround-view coverage. The camera systems varied in their capture frequenciesâ€”20 Hz, 25 Hz, and 30 Hzâ€”introducing a range of temporal resolutions. This variability reflects the heterogeneous nature of sensor setups deployed in actual autonomous vehicles and supports GAIA-2â€™s ability to generalize across different input rates and hardware specifications.

ì‹¤ì œ ììœ¨ ì£¼í–‰ì˜ ë³µì¡ì„±ì„ í¬ì°©í•˜ê¸° ìœ„í•´ ë°ì´í„° ìˆ˜ì§‘ì—ëŠ” ì„¸ ê°€ì§€ ì°¨ì¢…ê³¼ ë‘ ì¢…ë¥˜ì˜ íŒ¨ëŸ¬ë²Œ ì°¨ëŸ‰ì„ í¬í•¨í•œ ë‹¤ì–‘í•œ ì°¨ëŸ‰ í”Œë«í¼ì´ ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤. ê° ì°¨ëŸ‰ì€ 5ëŒ€ ë˜ëŠ” 6ëŒ€ì˜ ì¹´ë©”ë¼ë¡œ ì¥ì°©ë˜ì—ˆìœ¼ë©°, 360ë„ ì „ë°©ìœ„ ì‹œì•¼ë¥¼ ì œê³µí•˜ë„ë¡ êµ¬ì„±ë˜ì—ˆìŠµë‹ˆë‹¤. ì¹´ë©”ë¼ ì‹œìŠ¤í…œì˜ íšë“ ì£¼íŒŒìˆ˜ëŠ” 20Hz, 25Hz, 30Hzë¡œ ë‹¤ì–‘í•˜ê²Œ ì„¤ì •ë˜ì–´ ì—¬ëŸ¬ ê°€ì§€ ì‹œê°„ í•´ìƒë„ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì´ëŠ” ì‹¤ì œ ììœ¨ ì£¼í–‰ì°¨ì— ë°°í¬ëœ ì„¼ì„œ ì„¤ì •ì˜ ë‹¤ì–‘ì„±ì„ ë°˜ì˜í•˜ë©°, GAIA-2ê°€ ë‹¤ì–‘í•œ ì…ë ¥ ì†ë„ì™€ í•˜ë“œì›¨ì–´ ì‚¬ì–‘ì— ëŒ€í•´ ì¼ë°˜í™”ë  ìˆ˜ ìˆë„ë¡ ì§€ì›í•©ë‹ˆë‹¤.

An important characteristic of the dataset is the variability in camera placement throughout the data collection period. Over time, the positions and calibrations of the cameras were adjusted across platforms, introducing a broad spectrum of spatial configurations. This diversity provides a strong training signal for generalizing across different camera rigs, a key requirement for scalable synthetic data generation in the autonomous driving domain.

ë°ì´í„°ì…‹ì˜ ì¤‘ìš”í•œ íŠ¹ì§• ì¤‘ í•˜ë‚˜ëŠ” ë°ì´í„° ìˆ˜ì§‘ ê¸°ê°„ ë™ì•ˆ ì¹´ë©”ë¼ ë°°ì¹˜ì— ë‚˜íƒ€ë‚˜ëŠ” ë‹¤ì–‘ì„±ì…ë‹ˆë‹¤. ì‹œê°„ ê²½ê³¼ì— ë”°ë¼ ë‹¤ì–‘í•œ í”Œë«í¼ì—ì„œ ì¹´ë©”ë¼ì˜ ìœ„ì¹˜ì™€ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ì´ ì¡°ì •ë˜ë©´ì„œ, ê´‘ë²”ìœ„í•œ ê³µê°„ êµ¬ì„±ì´ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë‹¤ì–‘ì„±ì€ ë‹¤ì–‘í•œ ì¹´ë©”ë¼ êµ¬ì„±ì„ í™œìš©í•˜ì—¬ ì¼ë°˜í™”í•˜ëŠ” ë° ê°•ë ¥í•œ í•™ìŠµ ì‹ í˜¸ë¥¼ ì œê³µí•˜ë©°, ììœ¨ ì£¼í–‰ ë¶„ì•¼ì˜ í™•ì¥ ê°€ëŠ¥í•œ í•©ì„± ë°ì´í„° ìƒì„±ì— ìˆì–´ í•µì‹¬ì ì¸ ìš”êµ¬ ì‚¬í•­ì„ ì¶©ì¡±í•©ë‹ˆë‹¤.

The dataset also encompasses a wide range of driving scenarios, including diverse weather conditions, times of day, road types, and traffic environments. To ensure coverage across this complexity, we explicitly balance the training data not just on individual features, but on their joint probability distribution. This approach enables a more representative learning signal by modeling realistic co-occurrencesâ€”e.g., specific lighting and weather conditions in certain geographies or behaviors unique to particular road types. To prevent redundant training samples, we enforce a minimum temporal stride between selected sequences, reducing the risk of duplication while maintaining a natural distribution.

ì €í¬ ë°ì´í„°ì…‹ì€ ë‹¤ì–‘í•œ ì£¼í–‰ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ í¬ê´„í•˜ë©°, ë‹¤ì–‘í•œ ê¸°ìƒ ì¡°ê±´, ì‹œê°„ëŒ€, ë„ë¡œ ìœ í˜•, êµí†µ í™˜ê²½ ë“±ì„ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë³µì¡ì„±ì„ ê³ ë ¤í•˜ì—¬, ì €í¬ëŠ” ê°œë³„ íŠ¹ì§•ë¿ë§Œ ì•„ë‹ˆë¼, ì´ë“¤ì˜ ê³µë¶„ì‚° ë¶„í¬ë¥¼ í†µí•´ í•™ìŠµ ë°ì´í„°ë¥¼ ê· í˜• ìˆê²Œ ì¡°ì •í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ë”ìš± ëŒ€í‘œì ì¸ í•™ìŠµ ì‹ í˜¸ë¥¼ ìƒì„±í•˜ë©°, íŠ¹ì • ì§€ë¦¬ì  ìœ„ì¹˜ë‚˜ íŠ¹ì • ë„ë¡œ ìœ í˜•ì— ë”°ë¥¸ ê³ ìœ í•œ í–‰ë™ê³¼ ê°™ì€ í˜„ì‹¤ì ì¸ ë™ì‹œ ë°œìƒì„ ëª¨ë¸ë§í•©ë‹ˆë‹¤. ì¤‘ë³µëœ í•™ìŠµ ìƒ˜í”Œì„ ë°©ì§€í•˜ê¸° ìœ„í•´, ì„ íƒëœ ì‹œí€€ìŠ¤ ê°„ ìµœì†Œ ì‹œê°„ ê°„ê²©ì„ ì ìš©í•˜ì—¬, ì¤‘ë³µ ìœ„í—˜ì„ ì¤„ì´ë©´ì„œë„ ìì—°ìŠ¤ëŸ¬ìš´ ë¶„í¬ë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.

For evaluation, we implement a geographically held-out validation strategy. Specific validation geofences are defined to exclude certain regions from the training set entirely. This ensures that model evaluation is performed on unseen locations, providing a more rigorous assessment of generalization performance across different environments.

í‰ê°€ ëª©ì ìœ¼ë¡œ, ì €í¬ëŠ” ì§€ë¦¬ì ìœ¼ë¡œ ë¶„ë¦¬ëœ ê²€ì¦ ì „ëµì„ ì ìš©í•©ë‹ˆë‹¤. íŠ¹ì • ì§€ì—­ì„ ì™„ì „íˆ í›ˆë ¨ ë°ì´í„° ì„¸íŠ¸ì—ì„œ ì œì™¸í•˜ë„ë¡ êµ¬ì²´ì ì¸ ê²€ì¦ ì˜ì—­(geofence)ì´ ì •ì˜ë©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ëª¨ë¸ í‰ê°€ê°€ ë³´ì§€ ëª»í•œ ì§€ì—­ì—ì„œ ìˆ˜í–‰ë˜ë¯€ë¡œ, ë‹¤ì–‘í•œ í™˜ê²½ì—ì„œì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ ë³´ë‹¤ ì—„ë°€í•˜ê²Œ í‰ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

In summary, this dataset provides a robust foundation for training GAIA-2. Its extensive temporal and spatial coverage, diversity in vehicle and camera configurations, and principled validation setup make it uniquely well-suited for developing generative world models capable of producing realistic and controllable driving video across varied real-world conditions.

ìš”ì•½í•˜ìë©´, ì´ ë°ì´í„°ì…‹ì€ GAIA-2 í›ˆë ¨ì„ ìœ„í•œ ê²¬ê³ í•œ ê¸°ë°˜ì„ ì œê³µí•©ë‹ˆë‹¤. ê´‘ë²”ìœ„í•œ ì‹œê°„ì  ë° ê³µê°„ì  ì»¤ë²„ë¦¬ì§€, ë‹¤ì–‘í•œ ì°¨ëŸ‰ ë° ì¹´ë©”ë¼ êµ¬ì„±, ê·¸ë¦¬ê³  ì²´ê³„ì ì¸ ê²€ì¦ ì„¤ì •ì€ ì‹¤ì§ˆì ì¸ í™˜ê²½ì˜ ë‹¤ì–‘í•œ ì¡°ê±´ì—ì„œ í˜„ì‹¤ì ì´ê³  ì œì–´ ê°€ëŠ¥í•œ ì£¼í–‰ ë¹„ë””ì˜¤ë¥¼ ìƒì„±í•  ìˆ˜ ìˆëŠ” ìƒì„±í˜• ì›”ë“œ ëª¨ë¸ ê°œë°œì— ë§¤ìš° ì í•©í•©ë‹ˆë‹¤.

## 4 Training Procedure

This section describes the training procedures for both components of GAIA-2: the video tokenizer and the world model. Each component is trained independently using large-scale compute infrastructure and tailored loss configurations to optimize their respective objectives.

ë³¸ ì„¹ì…˜ì—ì„œëŠ” GAIA-2ì˜ ë‘ ê°€ì§€ êµ¬ì„± ìš”ì†Œ, ì¦‰ ë¹„ë””ì˜¤ í† í¬ë‚˜ì´ì €ì™€ ì›”ë“œ ëª¨ë¸ì— ëŒ€í•œ í›ˆë ¨ ì ˆì°¨ë¥¼ ì„¤ëª…í•©ë‹ˆë‹¤. ê° êµ¬ì„± ìš”ì†ŒëŠ” ëŒ€ê·œëª¨ ì»´í“¨íŒ… ì¸í”„ë¼ì™€ ë§ì¶¤í˜• ì†ì‹¤ êµ¬ì„± ì„¤ì •ì„ ì‚¬ìš©í•˜ì—¬ ê°ê°ì˜ ëª©í‘œë¥¼ ìµœì í™”í•˜ê¸° ìœ„í•´ ë…ë¦½ì ìœ¼ë¡œ í›ˆë ¨ë©ë‹ˆë‹¤.

##### Video Tokenizer.

The video tokenizer was trained for $300{\scriptstyle,}000$ steps with a batch size of $128$ using 128 H100 GPUs. Input sequences consisted of $T_{v}=24$ video frames sampled at their native capture frequencies (20, 25, or 30 Hz). Random spatial crops of size $448\times 960$ were extracted from the frames. For each training sample, a camera view was randomly selected from the available $N=5$ perspectives. Notably, each camera stream was encoded independently.

ë¹„ë””ì˜¤ í† í¬ë‚˜ì´ì €ëŠ” 128ê°œì˜ H100 GPUë¥¼ ì‚¬ìš©í•˜ì—¬ 30ë§Œ ë‹¨ê³„ ë™ì•ˆ ë°°ì¹˜ í¬ê¸°ë¥¼ 128ë¡œ ì„¤ì •í•˜ì—¬ í›ˆë ¨ë˜ì—ˆìŠµë‹ˆë‹¤. ì…ë ¥ ì‹œí€€ìŠ¤ëŠ” ì›ë³¸ ìº¡ì²˜ ì£¼íŒŒìˆ˜(20Hz, 25Hz, 30Hz)ì—ì„œ ì¶”ì¶œëœ 24ê°œì˜ ë¹„ë””ì˜¤ í”„ë ˆì„ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. í”„ë ˆì„ì—ì„œ 448x960 í¬ê¸°ì˜ ë¬´ì‘ìœ„ ê³µê°„ í¬ë¡­ì„ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤. ê° í›ˆë ¨ ìƒ˜í”Œì— ëŒ€í•´ ì‚¬ìš© ê°€ëŠ¥í•œ N=5ê°œì˜ ë·° ì¤‘ì—ì„œ ë¬´ì‘ìœ„ë¡œ ì¹´ë©”ë¼ ë·°ë¥¼ ì„ íƒí–ˆìŠµë‹ˆë‹¤. íŠ¹íˆ, ê° ì¹´ë©”ë¼ ìŠ¤íŠ¸ë¦¼ì€ ë…ë¦½ì ìœ¼ë¡œ ì¸ì½”ë”©ë˜ì—ˆìŠµë‹ˆë‹¤.

The tokenizer performs $8\times$ temporal and $32\times$ spatial downsampling, yielding a compressed representation with latent dimension $L=64$ , and an effective total compression rate of approximately 400 ( $\frac{24\times 448\times 960\times 3}{3\times 14\times 30\times 64}\simeq 400$ ).

í† í¬ë‚˜ì´ì €(Tokenizer)ëŠ” ì‹œê°„ì (Temporal) ìš”ì¸ìœ¼ë¡œ 8ë°°, ê³µê°„ì (Spatial) ìš”ì¸ìœ¼ë¡œ 32ë°° ë‹¤ìš´ìƒ˜í”Œë§(Downsampling)ì„ ìˆ˜í–‰í•˜ì—¬, ì ì¬ ì°¨ì›(Latent Dimension) L=64ì˜ ì••ì¶•ëœ í‘œí˜„ì„ ìƒì„±í•˜ë©°, ì•½ 400ì˜ íš¨ê³¼ì ì¸ ì´ ì••ì¶•ë¥ (Compression Rate)ì„ ì œê³µí•©ë‹ˆë‹¤.

The tokenizerâ€™s loss function is composed of a combination of image reconstruction, perceptual, and semantic alignment terms: (1) DINO v2 (Large) [^14] distillation in the latent space with a weight of $0.1$ . (2) KL divergence [^1] between the latent distribution and a unit Gaussian, with a low weight of $1\mathrm{e}{-6}$ to encourage smoothness. (3) Pixel-level losses: $L_{1}$ loss (weight $0.2$ ), $L_{2}$ loss (weight $2.0$ ), and LPIPS perceptual loss [^26] (weight $0.1$ ).

ì €í¬ í† í¬ë‚˜ì´ì €ì˜ ì†ì‹¤ í•¨ìˆ˜ëŠ” ì´ë¯¸ì§€ ì¬êµ¬ì„±, ì¸ì§€ì , ì˜ë¯¸ì  ì •ë ¬ í•­ë“¤ì˜ ì¡°í•©ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤. êµ¬ì²´ì ìœ¼ë¡œëŠ” ë‹¤ìŒê³¼ ê°™ì€ ìš”ì†Œë“¤ì„ í¬í•¨í•˜ê³  ìˆìŠµë‹ˆë‹¤.

(1) DINO v2 (Large) [^14]ë¥¼ ì‚¬ìš©í•˜ì—¬ ì ì¬ ê³µê°„ì—ì„œ ì¦ë¥˜í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ, ê°€ì¤‘ì¹˜ëŠ” 0.1ì…ë‹ˆë‹¤.

(2) ì ì¬ ë¶„í¬ì™€ ë‹¨ìœ„ ê°€ìš°ì‹œì•ˆ ì‚¬ì´ì˜ KL ë‹¤ì´ë²„ì „ìŠ¤ [^1]ë¥¼ ì‚¬ìš©í•˜ì—¬, ë¶€ë“œëŸ¬ì›€ì„ ìœ ë„í•˜ê¸° ìœ„í•´ ê°€ì¤‘ì¹˜ê°€ ë§¤ìš° ë‚®ì€ 1e-6ì„ ì ìš©í•©ë‹ˆë‹¤.

(3) í”½ì…€ ë ˆë²¨ ì†ì‹¤: L1 ì†ì‹¤ (ê°€ì¤‘ì¹˜ 0.2), L2 ì†ì‹¤ (ê°€ì¤‘ì¹˜ 2.0), ê·¸ë¦¬ê³  LPIPS ì¸ì§€ì  ì†ì‹¤ [^26] (ê°€ì¤‘ì¹˜ 0.1)ì„ í™œìš©í•©ë‹ˆë‹¤.

An exponential moving average (EMA) of the tokenizer parameters $\phi$ was maintained throughout training, with a decay factor of $0.9999$ and updates at every training step. The EMA weights were used at inference.

í›ˆë ¨ ê³¼ì • ì „ë°˜ì— ê±¸ì³ í† í¬ë‚˜ì´ì € íŒŒë¼ë¯¸í„° $\phi$ì˜ ì§€ìˆ˜ ì´ë™ í‰ê· (EMA)ì´ ìœ ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ê°ì‡  ê³„ìˆ˜ëŠ” 0.9999ì˜€ìœ¼ë©°, í›ˆë ¨ ë‹¨ê³„ë§ˆë‹¤ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤. ì¶”ë¡  ì‹œì—ëŠ” EMA ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.

Training was optimized using AdamW with: $2{\scriptstyle,}500$ warm-up steps to the base learning rate of $1\mathrm{e}{-4}$ , and $5{\scriptstyle,}000$ cooldown steps to a final learning rate of $1\mathrm{e}{-5}$ ; Adam betas $[0.9,0.95]$ , weight decay $0.1$ , and gradient clipping at $1.0$ . After initial training, the tokenizer decoder was fine-tuned for an additional $20{\scriptstyle,}000$ steps using a GAN loss (weight $0.1$ ) in combination with the previous reconstruction losses. The discriminator was optimized with a learning rate of $1\mathrm{e}{-5}$ .

í›ˆë ¨ ê³¼ì •ì€ AdamW ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ìµœì í™”ë˜ì—ˆìœ¼ë©°, ë‹¤ìŒê³¼ ê°™ì€ ì„¤ì •ìœ¼ë¡œ ì§„í–‰ë˜ì—ˆìŠµë‹ˆë‹¤.

ê¸°ë³¸ í•™ìŠµë¥ ì¸ 1e-4ë¡œ 2,500ë‹¨ê³„ì˜ ì›Œë°ì—… ë‹¨ê³„ë¥¼ ê±°ì³¤ê³ , ìµœì¢… í•™ìŠµë¥ ì¸ 1e-5ë¡œ 5,000ë‹¨ê³„ì˜ ëƒ‰ê° ë‹¨ê³„ë¥¼ ì ìš©í–ˆìŠµë‹ˆë‹¤. Adam ë² íƒ€ ê°’ì€ [0.9, 0.95], ê°€ì¤‘ì¹˜ ê°ì‡ ëŠ” 0.1, ê·¸ë¦¬ê³  ê¸°ìš¸ê¸° í´ë¦¬í•‘ì€ 1.0ìœ¼ë¡œ ì„¤ì •í–ˆìŠµë‹ˆë‹¤. ì´ˆê¸° í›ˆë ¨ ì´í›„, GAN ì†ì‹¤(ê°€ì¤‘ì¹˜ 0.1)ì„ í™œìš©í•˜ì—¬ ì¶”ê°€ 20,000ë‹¨ê³„ ë™ì•ˆ í† í¬ë‚˜ì´ì € ë””ì½”ë”ë¥¼ ë¯¸ì„¸ ì¡°ì •í–ˆìŠµë‹ˆë‹¤. ë˜í•œ, ì´ì „ì— ì‚¬ìš©ë˜ì—ˆë˜ ì¬êµ¬ì¶• ì†ì‹¤ê³¼ í•¨ê»˜ ìµœì í™”ë˜ì—ˆìŠµë‹ˆë‹¤. ë””ìŠ¤í† í¬ë‚˜ì´ì €ëŠ” í•™ìŠµë¥  1e-5ë¡œ ìµœì í™”ë˜ì—ˆìŠµë‹ˆë‹¤.

##### World Model.

The latent world model was trained for $460{\scriptstyle,}000$ steps with a batch size of $256$ on 256 H100 GPUs. Inputs consisted of $48$ video frames at native capture frequencies (20, 25, or 30 Hz), spatial resolution $448\times 960$ and across $N=5$ cameras. After encoding these videos to the latent space, this corresponds to $T\times N\times H\times W=6\times 5\times 14\times 30=12{\scriptstyle,}600$ input tokens.

ì €í¬ëŠ” 256ê°œì˜ H100 GPUë¥¼ ì‚¬ìš©í•˜ì—¬ ì ì¬ ì„¸ê³„ ëª¨ë¸ì„ 46ë§Œ ë‹¨ê³„ ë™ì•ˆ í›ˆë ¨í–ˆìŠµë‹ˆë‹¤. ë°°ì¹˜ í¬ê¸°ëŠ” 256ìœ¼ë¡œ ì„¤ì •í–ˆìœ¼ë©°, ê° ì…ë ¥ì€ 20Hz, 25Hz, 30Hzì˜ ì›ë³¸ ìº¡ì²˜ ì£¼íŒŒìˆ˜ì—ì„œ ì´¬ì˜ëœ 48ê°œì˜ ë¹„ë””ì˜¤ í”„ë ˆì„ìœ¼ë¡œ êµ¬ì„±ë˜ì—ˆìŠµë‹ˆë‹¤. ê³µê°„ í•´ìƒë„ëŠ” 448x960ì´ë©°, 5ê°œì˜ ì¹´ë©”ë¼ë¥¼ í†µí•´ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë¹„ë””ì˜¤ë¥¼ ì ì¬ ê³µê°„ìœ¼ë¡œ ì¸ì½”ë”©í•˜ë©´, ì´ ì…ë ¥ í† í° ìˆ˜ëŠ” 6 x 5 x 14 x 30 = 12,600ê°œë¡œ ê³„ì‚°ë©ë‹ˆë‹¤.

To encourage generalization, we sampled different training tasks as follows: 70% from-scratch generation, 20% contextual prediction, and 10% spatial inpainting. To regularize the model and enable classifier-free guidance, conditioning variables were randomly dropped. Each individual conditioning variable was independently dropped with 80% probability, and all of them were simultaneously dropped with 10% probability.

ì €í¬ëŠ” ì¼ë°˜í™” ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ ë‹¤ìŒê³¼ ê°™ì€ ë‹¤ì–‘í•œ í•™ìŠµ ì‘ì—…ì„ ìƒ˜í”Œë§í–ˆìŠµë‹ˆë‹¤. ì²˜ìŒë¶€í„° ìƒì„±í•˜ëŠ” ì‘ì—…ì˜ ë¹„ì¤‘ì€ 70%, ë¬¸ë§¥ ì˜ˆì¸¡ ì‘ì—…ì˜ ë¹„ì¤‘ì€ 20%, ê·¸ë¦¬ê³  ê³µê°„ ë³´ìˆ˜ ì‘ì—…ì˜ ë¹„ì¤‘ì€ 10%ì…ë‹ˆë‹¤. ëª¨ë¸ì„ ì •ê·œí™”í•˜ê³  ë¶„ë¥˜ì-ë¬´ë£Œ ì§€ì¹¨ì„ í™œì„±í™”í•˜ê¸° ìœ„í•´, ì¡°ê±´ ë³€ìˆ˜ë“¤ì„ ë¬´ì‘ìœ„ë¡œ ì œê±°í–ˆìŠµë‹ˆë‹¤. ê° ê°œë³„ ì¡°ê±´ ë³€ìˆ˜ëŠ” ë…ë¦½ì ìœ¼ë¡œ 80% í™•ë¥ ë¡œ ì œê±°ë˜ì—ˆìœ¼ë©°, ëª¨ë“  ì¡°ê±´ ë³€ìˆ˜ë“¤ì€ ë™ì‹œì— 10% í™•ë¥ ë¡œ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤.

Input camera views were randomly dropped with 10% probability to enhance robustness to partial observability. Latent tokens were normalized using a fixed mean of $\mu_{x}=0.0$ and standard deviation $\sigma_{x}=0.32$ , as empirically determined during tokenizer training.

ë¬´ì„  ì¹´ë©”ë¼ ì˜ìƒì´ ë¬´ì‘ìœ„ë¡œ ì¤‘ë‹¨ë˜ëŠ” í˜„ìƒì„ í†µí•´ ë¶€ë¶„ì ì¸ ì‹œì•¼ ì œí•œì— ëŒ€í•œ ì•ˆì •ì„±ì„ ë†’ì´ê³  ìˆìŠµë‹ˆë‹¤. ì ì¬ í† í°ì€ í† í¬ë‚˜ì´ì € í›ˆë ¨ ê³¼ì •ì—ì„œ ê²½í—˜ì ìœ¼ë¡œ ê²°ì •ëœ í‰ê·  ê°’ Î¼x=0.0ê³¼ í‘œì¤€ í¸ì°¨ Ïƒx=0.32ë¥¼ ì‚¬ìš©í•˜ì—¬ ì •ê·œí™”ë©ë‹ˆë‹¤.

As with the tokenizer, we maintained an EMA of the world model parameters $\theta$ with a decay factor of $0.9999$ and updates at every training step. EMA weights were used for inference. The optimizer was AdamW with: $2{\scriptstyle,}500$ warm-up steps to an initial learning rate of $5\mathrm{e}{-5}$ and cosine decay over the full training duration to a final learning rate of $6.5\mathrm{e}{-6}$ ; Adam betas $[0.9,0.99]$ , weight decay $0.1$ , and gradient clipping $1.0$ .

í† í¬ë‚˜ì´ì €ì™€ ë§ˆì°¬ê°€ì§€ë¡œ, ì„¸ê³„ ëª¨ë¸ íŒŒë¼ë¯¸í„° $\theta$ì— ëŒ€í•œ EMA(Exponential Moving Average)ë¥¼ ìœ ì§€í•˜ë©°, í•™ìŠµ ë‹¨ê³„ë§ˆë‹¤ ê°ì‡  ê³„ìˆ˜ 0.9999ë¥¼ ì ìš©í•˜ì—¬ ì—…ë°ì´íŠ¸í–ˆìŠµë‹ˆë‹¤. ì¶”ë¡  ì‹œì—ëŠ” EMA ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. ì˜µí‹°ë§ˆì´ì €ëŠ” AdamWë¥¼ ì‚¬ìš©í–ˆìœ¼ë©°, ë‹¤ìŒê³¼ ê°™ì€ ì„¤ì •ìœ¼ë¡œ êµ¬ì„±ë˜ì—ˆìŠµë‹ˆë‹¤: 2,500ë‹¨ê³„ì˜ ì›Œë°ì—… ë‹¨ê³„ë¥¼ ì´ˆê¸° í•™ìŠµë¥  5e-5ì—ì„œ ì‹œì‘í•˜ì—¬ ì „ì²´ í•™ìŠµ ê¸°ê°„ ë™ì•ˆ ì½”ì‚¬ì¸ ê°ì‡ ë¥¼ ì ìš©, ìµœì¢… í•™ìŠµë¥  6.5e-6ê¹Œì§€, Adam ë² íƒ€ ê°’ [0.9, 0.99], ê°€ì¤‘ì¹˜ ê°ì‡  0.1, ê·¸ë¦¬ê³  ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ 1.0ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.

## 5 Inference

The GAIA-2 model supports a range of inference tasks that showcase its flexibility and controllability across video generation scenarios. These tasks are unified by a shared denoising process operating in the latent space, followed by decoding into pixel space via the video tokenizer.

GAIA-2 ëª¨ë¸ì€ ë‹¤ì–‘í•œ ì¶”ë¡  ì‘ì—…ì„ ì§€ì›í•˜ì—¬ ë¹„ë””ì˜¤ ìƒì„± ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ìœ ì—°ì„±ê³¼ ì œì–´ ê°€ëŠ¥ì„±ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì´ëŸ¬í•œ ì‘ì—…ë“¤ì€ ì ì¬ ê³µê°„ì—ì„œ ì‘ë™í•˜ëŠ” ê³µìœ  ë…¸ì´ì¦ˆ ì œê±° ê³¼ì •ì„ ê±°ì³ ë¹„ë””ì˜¤ í† í¬ë‚˜ì´ì €ë¥¼ í†µí•´ í”½ì…€ ê³µê°„ìœ¼ë¡œ ë””ì½”ë”©ë©ë‹ˆë‹¤.

![Refer to caption](./figure/4.png)

Figure 4: CLIP conditioning. Conditioning GAIA-2 generations with text prompts encoded through the CLIP text encoder provides nuanced control over environmental features.

##### Inference Tasks.

We consider four primary inference modes, each illustrating a distinct capability of the model:

- Generation from scratch involves sampling pure Gaussian noise and denoising them with guidance from conditioning variables defined in SectionÂ  [2.2.3](https://arxiv.org/html/2503.20523v1#S2.SS2.SSS3 "2.2.3 Conditioning â€£ 2.2 World Model â€£ 2 Model â€£ GAIA-2: A Controllable Multi-View Generative World Model for Autonomous Driving"). The resulting latents are then decoded to video frames using the video tokenizer decoder, with temporally consistent outputs produced via the rolling window decoding mechanism illustrated in FigureÂ  [3](https://arxiv.org/html/2503.20523v1#S2.F3 "Figure 3 â€£ 2.1 Video Tokenizer â€£ 2 Model â€£ GAIA-2: A Controllable Multi-View Generative World Model for Autonomous Driving").
- Autoregressive prediction enables forecasting future latents from a sequence of past context latents. Given an initial context window of $k=3$ temporal latents, the model predicts the next set of latents, appends it to the context, and repeats the process using a sliding window. This approach allows for long-horizon rollouts while incorporating conditioning signals such as ego motion. An example is provided in FigureÂ  [9](https://arxiv.org/html/2503.20523v1#S6.F9 "Figure 9 â€£ Generating Safety-Critical Scenarios. â€£ 6.1 Qualitative Examples â€£ 6 Results â€£ GAIA-2: A Controllable Multi-View Generative World Model for Autonomous Driving").
- Inpainting allows selective modification of video content. A spatial-temporal mask is applied to the latent input, and the masked regions are regenerated via conditional denoising. Optional guidance from dynamic agent conditioning (e.g., agent locations) can steer the generation within the masked area. An example is shown inÂ  [Figure 12](https://arxiv.org/html/2503.20523v1#S6.F12 "In Inpainting. â€£ 6.1 Qualitative Examples â€£ 6 Results â€£ GAIA-2: A Controllable Multi-View Generative World Model for Autonomous Driving").
- Scene editing is achieved by partially noising latents extracted from real video, followed by denoising with altered conditioning. This enables targeted semantic or stylistic transformationsâ€”such as changing the weather, time of day, or road layoutâ€”without re-generating the full scene. [Figure 6](https://arxiv.org/html/2503.20523v1#S5.F6 "In Classifier-Free Guidance. â€£ 5 Inference â€£ GAIA-2: A Controllable Multi-View Generative World Model for Autonomous Driving") illustrates this capability.

ì €í¬ëŠ” ëª¨ë¸ì˜ ì£¼ìš” ê¸°ëŠ¥ ë„¤ ê°€ì§€ë¥¼ ê³ ë ¤í–ˆìŠµë‹ˆë‹¤. ê° ê¸°ëŠ¥ì€ ëª¨ë¸ì˜ ê³ ìœ í•œ ëŠ¥ë ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

- ë¬´ì‘ìœ„ ìƒì„±(Generation from scratch)ì€ ìˆœìˆ˜í•œ ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆë¥¼ ìƒ˜í”Œë§í•˜ê³ , [2.2.3](https://arxiv.org/html/2503.20523v1#S2.SS2.SSS3 "2.2.3 Conditioning â€£ 2.1 Video Tokenizer â€£ 2 Model â€£ GAIA-2: A Controllable Multi-View Generative World Model for Autonomous Driving") ì„¹ì…˜ì— ì •ì˜ëœ ì¡°ê±´ ë³€ìˆ˜ë¡œë¶€í„° ì•ˆë‚´ë¥¼ ë°›ì•„ ë…¸ì´ì¦ˆë¥¼ ì œê±°í•©ë‹ˆë‹¤. ê·¸ ê²°ê³¼ ìƒì„±ëœ ì ì¬ ë²¡í„°(latents)ëŠ” ë¹„ë””ì˜¤ í† í¬ë‚˜ì´ì € ë””ì½”ë”ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹„ë””ì˜¤ í”„ë ˆì„ìœ¼ë¡œ ë””ì½”ë”©ë©ë‹ˆë‹¤. ë¡¤ë§ ìœˆë„ìš° ë””ì½”ë”© ë©”ì»¤ë‹ˆì¦˜(rolling window decoding mechanism) [3](https://arxiv.org/html/2503.20523v1#S2.F3 "Figure 3 â€£ 2.1 Video Tokenizer â€£ 2 Model â€£ GAIA-2: A Controllable Multi-View Generative World Model for Autonomous Driving")ë¥¼ í†µí•´ ì‹œê°„ì ìœ¼ë¡œ ì¼ê´€ëœ ì¶œë ¥ì´ ìƒì„±ë©ë‹ˆë‹¤.

- ìê°€ íšŒê·€ì  ì˜ˆì¸¡(Autoregressive prediction)ì€ ê³¼ê±° ì»¨í…ìŠ¤íŠ¸ ì ì¬ ë²¡í„°(latent) ì‹œí€€ìŠ¤ë¡œë¶€í„° ë¯¸ë˜ ì ì¬ ë²¡í„°ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤. ì´ˆê¸° ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš°($k=3$)ì˜ ì‹œê°„ì  ì ì¬ ë²¡í„° 3ê°œë¡œ ì‹œì‘í•˜ì—¬ ëª¨ë¸ì€ ë‹¤ìŒ ì ì¬ ë²¡í„° ì„¸íŠ¸ë¥¼ ì˜ˆì¸¡í•˜ê³  ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€í•˜ë©° ìŠ¬ë¼ì´ë”© ìœˆë„ìš°(sliding window)ë¥¼ ì‚¬ìš©í•˜ì—¬ í”„ë¡œì„¸ìŠ¤ë¥¼ ë°˜ë³µí•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ì ‘ê·¼ ë°©ì‹ì€ ì—ê³  ëª¨ì…˜(ego motion)ê³¼ ê°™ì€ ì¡°ê±´ ë³€ìˆ˜ë¡œë¶€í„°ì˜ ì•ˆë‚´ë¥¼ í¬í•¨í•˜ë©´ì„œ ì¥ê¸° ë¡¤ì•„ì›ƒ(rollout)ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ì˜ˆì‹œëŠ” [9](https://arxiv.org/html/2503.20523v1#S6.F9 "Figure 9 â€£ Generating Safety-Critical Scenarios. â€£ 6.1 Qualitative Examples â€£ 6 Results â€£ GAIA-2: A Controllable Multi-View Generative World Model for Autonomous Driving")ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

- ì¸í˜ì¸íŒ…(Inpainting)ì€ ë¹„ë””ì˜¤ ì½˜í…ì¸ ì˜ ì„ íƒì  ìˆ˜ì •ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. ì ì¬ ë²¡í„° ì…ë ¥ì— ê³µê°„-ì‹œê°„ ë§ˆìŠ¤í¬(spatial-temporal mask)ë¥¼ ì ìš©í•˜ê³  ë§ˆìŠ¤í¬ëœ ì˜ì—­ì€ ì¡°ê±´ì  ë…¸ì´ì¦ˆ ì œê±°ë¥¼ í†µí•´ ì¬ìƒì„±ë©ë‹ˆë‹¤. ë™ì  ì—ì´ì „íŠ¸ ì¡°ê±´(dynamic agent conditioning, ì˜ˆë¥¼ ë“¤ì–´ ì—ì´ì „íŠ¸ ìœ„ì¹˜)ë¡œë¶€í„°ì˜ ì•ˆë‚´ëŠ” ë§ˆìŠ¤í¬ëœ ì˜ì—­ ë‚´ì—ì„œì˜ ìƒì„± ë°©í–¥ì„ ì•ˆë‚´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆì‹œëŠ” [12](https://arxiv.org/html/2503.20523v1#S6.F12 "In Inpainting. â€£ 6.1 Qualitative Examples â€£ 6 Results â€£ GAIA-2: A Controllable Multi-View Generative World Model for Autonomous Driving")ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

- ì¥ë©´ í¸ì§‘(Scene editing)ì€ ì‹¤ì œ ë¹„ë””ì˜¤ì—ì„œ ì¶”ì¶œí•œ ì ì¬ ë²¡í„° ë¶€ë¶„ì ìœ¼ë¡œ ë…¸ì´ì¦ˆë¥¼ ì ìš©í•œ ë‹¤ìŒ ìˆ˜ì •ëœ ì¡°ê±´ ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë…¸ì´ì¦ˆë¥¼ ì œê±°í•¨ìœ¼ë¡œì¨ ë‹¬ì„±ë©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ë‚ ì”¨, ì‹œê°„ëŒ€ ë˜ëŠ” ë„ë¡œ ë ˆì´ì•„ì›ƒì„ ë³€ê²½í•˜ëŠ” ë“± ì „ì²´ ì¥ë©´ì„ ì¬ìƒì„±í•˜ì§€ ì•Šê³ ë„ í‘œì  ì‹œë§¨í‹±(semantic) ë˜ëŠ” ìŠ¤íƒ€ì¼(stylistic) ë³€í™˜ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. [6](https://arxiv.org/html/2503.20523v1#S5.F6 "In Classifier-Free Guidance. â€£ 5 Inference â€£ GAIA-2: A Controllable Multi-View Generative World Model for Autonomous Driving")ì—ì„œ ì´ëŸ¬í•œ ê¸°ëŠ¥ì´ ì„¤ëª…ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

These modes demonstrate that GAIA-2 can serve as a general-purpose simulator for a wide variety of scene manipulation tasks, whether starting from noise, context, or existing video.

ì €í¬ GAIA-2ëŠ” ë‹¤ì–‘í•œ ì¥ë©´ ì¡°ì‘ ì‘ì—…ì— í™œìš©ë  ìˆ˜ ìˆìœ¼ë©°, ì†ŒìŒì´ë‚˜ ë§¥ë½, ë˜ëŠ” ê¸°ì¡´ ì˜ìƒìœ¼ë¡œë¶€í„° ì‹œì‘í•˜ë”ë¼ë„ í­ë„“ê²Œ í™œìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

##### Inference Noise Schedule.

For all inference tasks, we adopt the linear-quadratic noise schedule introduced by [^20]. This schedule begins with linearly spaced noise levels, which are effective for capturing coarse scene layouts and motion patterns. In later stages, the schedule transitions to quadratically spaced steps that allow more efficient refinement of high-frequency visual details. This hybrid approach improves both generation quality and computational efficiency. In our experiments, we use a fixed number of 50 denoising steps.

ëª¨ë“  ì¶”ë¡  ì‘ì—…ì—ì„œ ì €í¬ëŠ” [^20]ì—ì„œ ì œì‹œëœ ì„ í˜•-2ì°¨ ë…¸ì´ì¦ˆ ìŠ¤ì¼€ì¤„ì„ ì±„íƒí•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ ìŠ¤ì¼€ì¤„ì€ ëŒ€ëµì ì¸ ì¥ë©´ êµ¬ì„±ê³¼ ì›€ì§ì„ íŒ¨í„´ì„ í¬ì°©í•˜ëŠ” ë° íš¨ê³¼ì ì¸ ì„ í˜•ì ìœ¼ë¡œ ê°„ê²©ì´ ì¡°ì •ëœ ë…¸ì´ì¦ˆ ë ˆë²¨ë¡œ ì‹œì‘í•©ë‹ˆë‹¤. ì´í›„ ë‹¨ê³„ì—ì„œëŠ” ê³ ì£¼íŒŒ ì‹œê°ì  ë””í…Œì¼ì„ ë³´ë‹¤ íš¨ìœ¨ì ìœ¼ë¡œ ê°œì„ í•  ìˆ˜ ìˆë„ë¡ 2ì°¨ì ìœ¼ë¡œ ê°„ê²©ì´ ì¡°ì •ëœ ë‹¨ê³„ë¡œ ì „í™˜ë©ë‹ˆë‹¤. ì´ëŸ¬í•œ í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ ë°©ì‹ì€ ìƒì„± í’ˆì§ˆê³¼ ê³„ì‚° íš¨ìœ¨ì„±ì„ ëª¨ë‘ í–¥ìƒì‹œí‚µë‹ˆë‹¤. ì €í¬ ì‹¤í—˜ì—ì„œëŠ” ê³ ì •ëœ 50ë‹¨ê³„ì˜ ë…¸ì´ì¦ˆ ì œê±° ë‹¨ê³„ë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.

##### Classifier-Free Guidance.

Classifier-free guidance (CFG) is not used by default during inference. However, for challenging or out-of-distribution scenarios, such as those involving rare edge cases or unusual agent configurations (e.g., Figure [11](https://arxiv.org/html/2503.20523v1#S6.F11 "Figure 11 â€£ Generating Safety-Critical Scenarios. â€£ 6.1 Qualitative Examples â€£ 6 Results â€£ GAIA-2: A Controllable Multi-View Generative World Model for Autonomous Driving")), we activate CFG with a guidance scale ranging from 2 to 20, depending on the complexity of the scene.

ê¸°ë³¸ì ìœ¼ë¡œ ì¶”ë¡  ê³¼ì •ì—ì„œëŠ” í´ë˜ì‹-í”„ë¦¬ ê°€ì´ë“œ(Classifier-free guidance, CFG)ê°€ í™œì„±í™”ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ, ë“œë¬¼ê±°ë‚˜ ë¹„ì •ìƒì ì¸ ì‹œë‚˜ë¦¬ì˜¤, ì˜ˆë¥¼ ë“¤ì–´ í¬ê·€í•œ íŠ¹ìˆ˜í•œ ê²½ìš°ë‚˜ íŠ¹ì´í•œ ì—ì´ì „íŠ¸ êµ¬ì„±(ì˜ˆ: Figure [11](https://arxiv.org/html/2503.20523v1#S6.F11 "Figure 11 â€£ Generating Safety-Critical Scenarios. â€£ 6.1 Qualitative Examples â€£ 6 Results â€£ GAIA-2: A Controllable Multi-View Generative World Model for Autonomous Driving"))ì™€ ê°™ì´ ë³µì¡í•œ ì¥ë©´ì˜ ê²½ìš°, ì¥ë©´ì˜ ë³µì¡ì„±ì— ë”°ë¼ ê°€ì´ë“œ ìŠ¤ì¼€ì¼(guidance scale)ì„ 2ì—ì„œ 20 ì‚¬ì´ë¡œ ì¡°ì •í•˜ì—¬ CFGë¥¼ í™œì„±í™”í•©ë‹ˆë‹¤.

In scenarios involving dynamic agent conditioning, where the latent tokens associated with agent-specific regions are known a priori, we apply spatially selective CFG. In this case, guidance is applied only to the spatial locations influenced by the conditioning (e.g., 3D bounding boxes), which enhances generation quality in targeted regions without unnecessarily affecting the rest of the scene. This targeted approach enables more precise control over scene elements while preserving global coherence.

ë™ì  ì—ì´ì „íŠ¸ í›ˆë ¨ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ, ì—ì´ì „íŠ¸ë³„ ì˜ì—­ê³¼ ê´€ë ¨ëœ ì ì¬ í† í°ì´ ì‚¬ì „ì— ì•Œë ¤ì ¸ ìˆì„ ê²½ìš°, ì €í¬ëŠ” ê³µê°„ ì„ íƒì  CFGë¥¼ ì ìš©í•©ë‹ˆë‹¤. êµ¬ì²´ì ìœ¼ë¡œ, í›ˆë ¨ëœ ì˜ì—­ì—ë§Œ ì•ˆë‚´ê°€ ì ìš©ë˜ì–´, ì˜ˆë¥¼ ë“¤ì–´ 3D ë°”ìš´ë”© ë°•ìŠ¤ì™€ ê°™ì´ íŠ¹ì • ì˜ì—­ì˜ ìƒì„± í’ˆì§ˆì„ í–¥ìƒì‹œí‚¤ê³ , ë¶ˆí•„ìš”í•˜ê²Œ ë‚˜ë¨¸ì§€ ì¥ë©´ ì „ì²´ì— ì˜í–¥ì„ ì£¼ì§€ ì•ŠìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ì •ë°€í•œ ì ‘ê·¼ ë°©ì‹ì„ í†µí•´ ì¥ë©´ì˜ êµ¬ì„± ìš”ì†Œë¥¼ ë³´ë‹¤ ì •í™•í•˜ê²Œ ì œì–´í•˜ë©´ì„œ ì „ë°˜ì ì¸ ì¼ê´€ì„±ì„ ìœ ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

![Refer to caption](./figure/5.png)

Figure 5: Multi-rig video generation. GAIA-2 supports various vehicle platforms and camera setups, maintaining spatial and temporal consistency. The two examples shown include camera arrangements from a sports car, an SUV, and a large van.

![Refer to caption](./figure/6.png)

Figure 6: Augmentation through partial noising. By partially noising and denoising video frames, GAIA-2 transforms real video into diverse versions under different environmental settings, such as weather and time of day, while preserving semantic content and ego-actions.

![Refer to caption](./figure/7.png)
Figure 7:Scenario embedding conditioning. Scenario embeddings from a proprietary driving model enable GAIA-2 to generate diverse yet semantically consistent variations from real-world scenarios. For each scenario type the top row shows the real data the embedding is derived from, and the bottom three rows show synthetic variants of it, guided by the embedding as conditioning. We also optionally apply additional conditioning such as country, weather, time of day, or camera parameters to control diversity.

## 6 Results

We evaluate GAIA-2 through qualitative examples and quantitative metrics that highlight its generative fidelity, controllability, and suitability for synthetic data generation in autonomous driving. A comprehensive set of generated video examples is available at [https://wayve.ai/thinking/gaia-2](https://wayve.ai/thinking/gaia-2).

ì €í¬ëŠ” GAIA-2ì˜ ìƒì„±ì  ì¶©ì‹¤ë„, ì œì–´ ê°€ëŠ¥ì„±, ê·¸ë¦¬ê³  ììœ¨ ì£¼í–‰ ë¶„ì•¼ì˜ í•©ì„± ë°ì´í„° ìƒì„±ì— ì í•©ì„±ì„ ê°•ì¡°í•˜ëŠ” ì§ˆì  ì˜ˆì‹œì™€ ì •ëŸ‰ì  ì§€í‘œë¥¼ í†µí•´ ì„±ëŠ¥ì„ í‰ê°€í–ˆìŠµë‹ˆë‹¤. ìƒì„±ëœ ë¹„ë””ì˜¤ ì˜ˆì‹œì˜ í¬ê´„ì ì¸ ì„¸íŠ¸ê°€ [https://wayve.ai/thinking/gaia-2](https://wayve.ai/thinking/gaia-2) ì—ì„œ ì´ìš©í•´ ë³´ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

##### Augmenting Real-World Data.

GAIA-2 enables dataset augmentation through a range of techniques that allow for visual and contextual diversification of real-world sequences.

GAIA-2ëŠ” ë‹¤ì–‘í•œ ê¸°ìˆ ì„ í†µí•´ ì‹¤ì œ ë°ì´í„° ì„¸íŠ¸ì˜ ì‹œê°ì  ë° ë§¥ë½ì  ë‹¤ì–‘ì„±ì„ ë†’ì—¬ ë°ì´í„° ì¦ê°•ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.

### 6.1 Qualitative Examples

##### Diverse Scenario Generation.

GAIA-2 supports the generation of highly diverse driving scenarios, spanning multiple countries, weather conditions, times of day, and road layouts. In addition to structured conditioning via metadata, GAIA-2 can be guided by CLIP embeddings, allowing control over scene semantics not explicitly captured in labels. This includes geographic and environmental context such as urban, mountainous, or coastal scenes ([Figure 4](https://arxiv.org/html/2503.20523v1#S5.F4 "In 5 Inference â€£ GAIA-2: A Controllable Multi-View Generative World Model for Autonomous Driving")).

GAIA-2ëŠ” ë‹¤ìˆ˜ì˜ êµ­ê°€, ê¸°ìƒ ì¡°ê±´, ì‹œê°„ëŒ€, ë„ë¡œ ë ˆì´ì•„ì›ƒì„ í¬ê´„í•˜ëŠ” ë§¤ìš° ë‹¤ì–‘í•œ ì£¼í–‰ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ìƒì„±í•˜ëŠ” ë° í™œìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë©”íƒ€ë°ì´í„°ë¥¼ í†µí•œ êµ¬ì¡°í™”ëœ ì¡°ê±´ ì„¤ì • ì™¸ì—ë„, CLIP ì„ë² ë”©ì„ í†µí•´ ì‹œë‚˜ë¦¬ì˜¤ì˜ ì˜ë¯¸ë¥¼ ì§ì ‘ ë ˆì´ë¸”ë¡œ ê¸°ë¡ë˜ì§€ ì•Šì€ ë‚´ìš©ê¹Œì§€ ì œì–´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ë„ì‹œ, ì‚°ì•…, í•´ì•ˆ ë“± ë‹¤ì–‘í•œ ì§€ë¦¬ì  ë° í™˜ê²½ì  ë§¥ë½ì„ í¬í•¨í•œ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì œì–´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤ ([Figure 4](https://arxiv.org/html/2503.20523v1#S5.F4 "In 5 Inference â€£ GAIA-2: A Controllable Multi-View Generative World Model for Autonomous Driving")).

Furthermore, due to its exposure to varied camera configurations during training, GAIA-2 can simulate driving videos across different vehicle embodiments. By conditioning on camera parameters, it maintains spatial and temporal consistency across viewpoints for rigs mounted on sports cars, SUVs, and large vans ([Figure 5](https://arxiv.org/html/2503.20523v1#S5.F5 "In Classifier-Free Guidance. â€£ 5 Inference â€£ GAIA-2: A Controllable Multi-View Generative World Model for Autonomous Driving")).

- Partial noise and denoise: Latents derived from real videos are partially noised and then denoised under altered conditioning, such as different weather or lighting. This approach preserves semantics and ego motion while yielding diverse visual outputs (Figure [6](https://arxiv.org/html/2503.20523v1#S5.F6 "Figure 6 â€£ Classifier-Free Guidance. â€£ 5 Inference â€£ GAIA-2: A Controllable Multi-View Generative World Model for Autonomous Driving")). We demonstrate the ability to modify environmental aspects of the scene while leaving core semantic and functional components as per the original. Essentially turning a single real-world example into multiple new scenarios: the same ego trajectory but visually diversified to take place during sunshine, rain, at sunset, at night, and in the snow.
- Scenario embeddings: Scenario embeddings derived from our proprietary driving models give GAIA-2 the ability to generate multiple plausible variations from a single real-world example, providing many new examples of diverse yet contextually coherent synthetic data ([Figure 7](https://arxiv.org/html/2503.20523v1#S5.F7 "In Classifier-Free Guidance. â€£ 5 Inference â€£ GAIA-2: A Controllable Multi-View Generative World Model for Autonomous Driving")). Because the embedding space is meaningfully organized by the driving model, we are able to reliably generate scenarios that are semantically interpretable given their location in the embedding space, such as accelerating, decelerating, or overtaking other agents. By additionally conditioning on environmental factors, we can synthetically expand our dataset by increasing coverage across various conditions. By conditioning on different camera parameters, we can effectively replicate our corpus across any given vehicle platform.
- Action-based generation: GAIA-2 can synthesize new scenes purely from ego-vehicle action trajectories. Given the speed and curvature profiles, it generates plausible visual contexts aligned with those dynamics, such as traffic light changes, braking behavior, or turning maneuvers ([Figure 8](https://arxiv.org/html/2503.20523v1#S6.F8 "In Diverse Scenario Generation. â€£ 6.1 Qualitative Examples â€£ 6 Results â€£ GAIA-2: A Controllable Multi-View Generative World Model for Autonomous Driving")). We show three examples: (1) conditioning on a â€˜ start from stopped â€™ speed profile, GAIA-2 generates plausible visual observations to fit those actions, in this case, a UK traffic light turning from red and amber to green; (2) conditioning on a â€˜ slow to a stop â€™ speed trajectory, GAIA-2 generates a scenario where the ego agent is slowing down behind a London taxi; and (3) conditioned on a strong leftward curvature and slow ramping speed, GAIA-2 generates a U-turn at a US intersection.


ì €í¬ GAIA-2ëŠ” ë‹¤ì–‘í•œ ì¹´ë©”ë¼ ì„¤ì •ì— ë…¸ì¶œëœ ì˜ìƒì„ í†µí•´ ì—¬ëŸ¬ ì¢…ë¥˜ì˜ ì°¨ëŸ‰ ì‹œë®¬ë ˆì´ì…˜ ì˜ìƒì„ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¹´ë©”ë¼ íŒŒë¼ë¯¸í„°ì— ë”°ë¼ í•™ìŠµì‹œí‚¤ë©´, ìŠ¤í¬ì¸ ì¹´, SUV, ëŒ€í˜• íŠ¸ëŸ­ ë“± ë‹¤ì–‘í•œ ì°¨ëŸ‰ í”Œë«í¼ì—ì„œ ê´€ì  ê°„ì˜ ê³µê°„ì , ì‹œê°„ì  ì¼ê´€ì„±ì„ ìœ ì§€í•©ë‹ˆë‹¤ ([Figure 5](https://arxiv.org/html/2503.20523v1#S5.F5 "In Classifier-Free Guidance. â€£ 5 Inference â€£ GAIA-2: A Controllable Multi-View Generative World Model for Autonomous Driving")).

- ë¶€ë¶„ì ì¸ ì†ŒìŒ ë° ë…¸ì´ì¦ˆ ì œê±°: ì‹¤ì œ ë¹„ë””ì˜¤ì—ì„œ ìœ ë„ëœ **ë ˆì´í„´íŠ¸(Latent)**ëŠ” ë‹¤ì–‘í•œ ì¡°ê±´, ì˜ˆë¥¼ ë“¤ì–´ ë‚ ì”¨ë‚˜ ì¡°ëª… ë³€í™”ì— ë”°ë¼ ë¶€ë¶„ì ìœ¼ë¡œ ì†ŒìŒì´ ì¶”ê°€ëœ í›„ ë…¸ì´ì¦ˆ ì œê±° ê³¼ì •ì„ ê±°ì¹©ë‹ˆë‹¤. ì´ ì ‘ê·¼ ë°©ì‹ì€ í•µì‹¬ì ì¸ ì˜ë¯¸ ì •ë³´ì™€ ììœ¨ ì£¼í–‰ì„ ìœ„í•œ **ì—ê³  ëª¨ì…˜(Ego Motion)**ì„ ë³´ì¡´í•˜ë©´ì„œ ë‹¤ì–‘í•œ ì‹œê°ì  ê²°ê³¼ë¬¼ì„ ìƒì„±í•©ë‹ˆë‹¤ (**Figure 6** ì°¸ì¡°: [https://arxiv.org/html/2503.20523v1#S5.F6](https://arxiv.org/html/2503.20523v1#S5.F6)). ì‹¤ì œ ì„¸ê³„ì˜ ë‹¨ì¼í•œ ì˜ˆì‹œë¥¼ í–‡ë¹›, ë¹„, ì„ì–‘, ë°¤, ëˆˆ ë“± ë‹¤ì–‘í•œ í™˜ê²½ ì¡°ê±´ì—ì„œ ì‹œê°ì ìœ¼ë¡œ ë³€í™”ëœ ì—¬ëŸ¬ ìƒˆë¡œìš´ ì‹œë‚˜ë¦¬ì˜¤ë¡œ ë³€í™˜í•˜ëŠ” ê²ƒì„ ë³´ì—¬ë“œë¦½ë‹ˆë‹¤. í•µì‹¬ì ì¸ ì˜ë¯¸ì™€ ê¸°ëŠ¥ì ì¸ ìš”ì†ŒëŠ” ì›ë³¸ê³¼ ë™ì¼í•˜ê²Œ ìœ ì§€í•˜ë©´ì„œ ë§ì…ë‹ˆë‹¤.
- ì €í¬ì˜ ë…ìì ì¸ ì£¼í–‰ ëª¨ë¸ë¡œë¶€í„° íŒŒìƒëœ ì‹œë‚˜ë¦¬ì˜¤ ì„ë² ë”©ì€ GAIA-2ê°€ ë‹¨ì¼ ì‹¤ì œ ì‚¬ë¡€ë¡œë¶€í„° ì—¬ëŸ¬ ê°œì˜ íƒ€ë‹¹í•œ ë³€í˜•ì„ ìƒì„±í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ë‹¤ì–‘í•œ ë§¥ë½ ì†ì—ì„œë„ ì¼ê´€ì„± ìˆëŠ” í•©ì„± ë°ì´í„°ë¥¼ ë‹¤ìˆ˜ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤ ([Figure 7](https://arxiv.org/html/2503.20523v1#S5.F7 "In Classifier-Free Guidance. â€£ 5 Inference â€£ GAIA-2: A Controllable Multi-View Generative World Model for Autonomous Driving")). íŠ¹íˆ, ì£¼í–‰ ëª¨ë¸ì— ì˜í•´ ì˜ë¯¸ ìˆê²Œ êµ¬ì„±ëœ ì„ë² ë”© ê³µê°„ì„ í™œìš©í•˜ì—¬, í•´ë‹¹ ìœ„ì¹˜ì— ë”°ë¼ ê°€ì†, ê°ì†, ë‹¤ë¥¸ ì°¨ëŸ‰ì„ ì¶”ì›”í•˜ëŠ” ë“± ì˜ë¯¸ì ìœ¼ë¡œ í•´ì„ ê°€ëŠ¥í•œ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì‹ ë¢°ì„± ìˆê²Œ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ, í™˜ê²½ ìš”ì¸ì„ ì¶”ê°€ì ìœ¼ë¡œ ê³ ë ¤í•¨ìœ¼ë¡œì¨, ë‹¤ì–‘í•œ ì¡°ê±´ì— ëŒ€í•œ ë°ì´í„° ì»¤ë²„ë¦¬ì§€ë¥¼ í™•ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ, ì¹´ë©”ë¼ íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ê±´í™”í•˜ì—¬, íŠ¹ì • ì°¨ëŸ‰ í”Œë«í¼ì— ëŒ€í•œ ì½”í¼ìŠ¤ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- GAIA-2ëŠ” ìš´ì „ìì˜ í–‰ë™ ê²½ë¡œë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ì™„ì „íˆ ìƒˆë¡œìš´ ì¥ë©´ì„ í•©ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì†ë„ì™€ ê³¡ì„  í”„ë¡œí•„ì„ ê³ ë ¤í•˜ì—¬, í•´ë‹¹ ë™ì—­í•™ì— ë¶€í•©í•˜ëŠ” í˜„ì‹¤ì ì´ê³  ìì—°ìŠ¤ëŸ¬ìš´ ì‹œê°ì  ë§¥ë½ì„ ìƒì„±í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì‹ í˜¸ë“± ë³€í™”, ë¸Œë ˆì´í¬ ì‘ë™, íšŒì „ ì¡°ì‘ ë“±ì˜ ìƒí™©ì„ ë§Œë“¤ì–´ëƒ…ë‹ˆë‹¤ ([Figure 8](https://arxiv.org/html/2503.20523v1#S6.F8 "In Diverse Scenario Generation. â€£ 6.1 Qualitative Examples â€£ 6 Results â€£ GAIA-2: A Controllable Multi-View Generative World Model for Autonomous Driving")). ë‹¤ìŒ ì„¸ ê°€ì§€ ì˜ˆì‹œë¥¼ ë³´ì—¬ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ì²«ì§¸, â€˜ì •ì°¨ í›„ ì¶œë°œâ€™ ì†ë„ í”„ë¡œí•„ì„ ì¡°ê±´ìœ¼ë¡œ ì„¤ì •í•˜ë©´, GAIA-2ëŠ” í•´ë‹¹ í–‰ë™ì— ë§ëŠ” í˜„ì‹¤ì ì¸ ì‹œê°ì  ê´€ì°°ì„ ìƒì„±í•©ë‹ˆë‹¤. ì´ ê²½ìš°, ì˜êµ­ ì‹ í˜¸ë“±ì´ ë¹¨ì£¼ì˜¤ìƒ‰ìœ¼ë¡œ ë°”ë€ŒëŠ” ì¥ë©´ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ë‘˜ì§¸, â€˜ëŠë¦¬ê²Œ ì •ì°¨â€™ ì†ë„ ê²½ë¡œë¥¼ ì¡°ê±´ìœ¼ë¡œ ì„¤ì •í•˜ë©´, GAIA-2ëŠ” ììœ¨ì£¼í–‰ì°¨ê°€ ëŸ°ë˜ íƒì‹œë¥¼ ë”°ë¼ ëŠë¦¬ê²Œ ê°ì†í•˜ëŠ” ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ì…‹ì§¸, ê°•í•œ ì™¼ìª½ìœ¼ë¡œ í–¥í•˜ëŠ” ê³¡ë¥ ê³¼ ëŠë¦° ê°€ì† ì†ë„ë¥¼ ì¡°ê±´ìœ¼ë¡œ ì„¤ì •í•˜ë©´, GAIA-2ëŠ” ë¯¸êµ­ êµì°¨ë¡œì—ì„œ Uí„´ì„ ìˆ˜í–‰í•˜ëŠ” ì¥ë©´ì„ ìƒì„±í•©ë‹ˆë‹¤.

![Refer to caption](./figure/8.png)

Figure 8: Action-based generation. GAIA-2 synthesizes diverse scenes from specified speed and curvature profiles. Each scenario is contextually plausible, despite the original video inputs being dropped out.

##### Generating Safety-Critical Scenarios.

GAIA-2 is able to generate rare but safety-critical scenarios that would otherwise occur infrequently in the real world. We consider two classes of safety-critical situations: those caused by the ego agent, and those caused by other agents.

- Ego-vehicle induced scenarios: By conditioning on extreme or unsafe ego-vehicle actions (e.g., abrupt steering into oncoming traffic), GAIA-2 generates realistic scenarios critical to testing autonomous system resilience under hazardous conditions ([Figure 9](https://arxiv.org/html/2503.20523v1#S6.F9 "In Generating Safety-Critical Scenarios. â€£ 6.1 Qualitative Examples â€£ 6 Results â€£ GAIA-2: A Controllable Multi-View Generative World Model for Autonomous Driving") and [Figure 10](https://arxiv.org/html/2503.20523v1#S6.F10 "In Generating Safety-Critical Scenarios. â€£ 6.1 Qualitative Examples â€£ 6 Results â€£ GAIA-2: A Controllable Multi-View Generative World Model for Autonomous Driving")).
- Other-agent induced scenarios: Using 3D bounding boxes conditioning, GAIA-2 precisely controls the placement and motion of other agents, creating scenarios involving aggressive driving behaviors, emergency braking, or hazardous crossings, crucial for testing reactive capabilities of autonomous systems ([Figure 11](https://arxiv.org/html/2503.20523v1#S6.F11 "In Generating Safety-Critical Scenarios. â€£ 6.1 Qualitative Examples â€£ 6 Results â€£ GAIA-2: A Controllable Multi-View Generative World Model for Autonomous Driving")).

GAIA-2ëŠ” ì‹¤ì œ ì„¸ê³„ì—ì„œ ë“œë¬¼ê²Œ ë°œìƒí•˜ëŠ”, í•˜ì§€ë§Œ ë§¤ìš° ì¤‘ìš”í•œ ì•ˆì „ìƒ ìœ„ê¸° ìƒí™©ì„ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì €í¬ëŠ” í¬ê²Œ ë‘ ê°€ì§€ ìœ í˜•ì˜ ì•ˆì „ìƒ ìœ„ê¸° ìƒí™©ì„ ê³ ë ¤í•©ë‹ˆë‹¤. ì²«ì§¸, ììœ¨ì£¼í–‰ ì°¨ëŸ‰ì˜ ê·¹ë‹¨ì ì´ê±°ë‚˜ ìœ„í—˜í•œ í–‰ë™(ì˜ˆ: ê°‘ì‘ìŠ¤ëŸ¬ìš´ ì°¨ì„  ë³€ê²½)ì„ ê¸°ë°˜ìœ¼ë¡œ ìƒì„±ë˜ëŠ” ìƒí™©ì´ë©°, ë‘˜ì§¸, ë‹¤ë¥¸ ì°¨ëŸ‰ê³¼ì˜ ìƒí˜¸ì‘ìš©ì„ í†µí•´ ë°œìƒí•˜ëŠ” ìƒí™©ì…ë‹ˆë‹¤.

- ììœ¨ì£¼í–‰ ì°¨ëŸ‰ ìœ ë„ ì‹œë‚˜ë¦¬ì˜¤: GAIA-2ëŠ” ê·¹ë‹¨ì ì´ê±°ë‚˜ ìœ„í—˜í•œ ììœ¨ì£¼í–‰ ì°¨ëŸ‰ì˜ í–‰ë™(ì˜ˆ: ë§ˆì£¼ ì˜¤ëŠ” ì°¨ëŸ‰ìœ¼ë¡œ ê°‘ì‘ìŠ¤ëŸ¬ìš´ ì°¨ì„  ë³€ê²½)ì„ ì¡°ê±´ìœ¼ë¡œ ì„¤ì •í•˜ì—¬, ììœ¨ì£¼í–‰ ì‹œìŠ¤í…œì˜ ìœ„í—˜í•œ ì¡°ê±´ í•˜ì—ì„œì˜ íšŒë³µíƒ„ë ¥ì„±ì„ í…ŒìŠ¤íŠ¸í•˜ëŠ” ë° í•„ìˆ˜ì ì¸ í˜„ì‹¤ì ì¸ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤ ([Figure 9](https://arxiv.org/html/2503.20523v1#S6.F9 "In Generating Safety-Critical Scenarios. â€£ 6.1 Qualitative Examples â€£ 6 Results â€£ GAIA-2: A Controllable Multi-View Generative World Model for Autonomous Driving")).
- ë‹¤ë¥¸ ì°¨ëŸ‰ ìœ ë„ ì‹œë‚˜ë¦¬ì˜¤: 3D ë°”ìš´ë”© ë°•ìŠ¤ ê¸°ë°˜ ì¡°ê±´ ì„¤ì • ë°©ì‹ì„ í†µí•´ GAIA-2ëŠ” ë‹¤ë¥¸ ì°¨ëŸ‰ì˜ ìœ„ì¹˜ì™€ ì›€ì§ì„ì„ ì •ë°€í•˜ê²Œ ì œì–´í•˜ì—¬, ê³µê²©ì ì¸ ìš´ì „ í–‰ë™, ê¸´ê¸‰ ì œë™, ìœ„í—˜í•œ êµì°¨ ë“± ë°˜ì‘ì„± í…ŒìŠ¤íŠ¸ì— ì¤‘ìš”í•œ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤ ([Figure 11](https://arxiv.org/html/2503.20523v1#S6.F11 "In Generating Safety-Critical Scenarios. â€£ 6.1 Qualitative Examples â€£ 6 Results â€£ GAIA-2: A Controllable Multi-View Generative World Model for Autonomous Driving")).

![Refer to caption](./figure/9.png)

Figure 9: Ego-induced safety-critical scenarios. By applying unsafe action conditioning, GAIA-2 can generate hazardous situations, such as steering into oncoming traffic. We provide real video frames as context and GAIA-2 rolls out from these starting frames given some new action conditioning. In this example, we control the ego vehicle steering by conditioning on rightward curvature. We drop out speed so that GAIA-2 can generate multiple diverse outcomes. Note how the ego vehicle slows down while veering sharply into oncoming traffic. The oncoming vehicle swerves to avoid the ego vehicle.

![Refer to caption](./figure/10.png)
Figure 10:Extreme generalization behavior. When conditioned on high speed and strong curvature, GAIA-2 extrapolates off-road trajectories, capturing rare behaviors like driving into fields or forests.

![Refer to caption](./figure/11.png)

Figure 11: Other-agent induced safety-critical scenarios. 3D bounding box conditioning enables fine control over dynamic agents, allowing simulation of risky behaviors such as sudden braking, overtaking, or speeding through intersections.

##### Inpainting.

GAIA-2 supports spatial-temporal inpainting, enabling selective content editing. When provided with masked regions and corresponding agent-level conditioning, GAIA-2 inserts agents seamlessly into existing contexts without disrupting unrelated areas ([Figure 12](https://arxiv.org/html/2503.20523v1#S6.F12 "In Inpainting. â€£ 6.1 Qualitative Examples â€£ 6 Results â€£ GAIA-2: A Controllable Multi-View Generative World Model for Autonomous Driving")).

GAIA-2ëŠ” ê³µê°„-ì‹œê°„ ë³´ê°„(spatial-temporal inpainting)ì„ ì§€ì›í•˜ì—¬ ì„ íƒì ì¸ ì½˜í…ì¸  í¸ì§‘ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. ë§ˆìŠ¤í¬ ì²˜ë¦¬ëœ ì˜ì—­ê³¼ í•´ë‹¹ ì—ì´ì „íŠ¸ ë ˆë²¨ì˜ ì¡°ê±´ë¶€ ì •ë³´ë¥¼ ì œê³µë°›ìœ¼ë©´, GAIA-2ëŠ” ê¸°ì¡´ ë§¥ë½ì— ì—ì´ì „íŠ¸ë¥¼ ì›í™œí•˜ê²Œ ì‚½ì…í•˜ì—¬ ê´€ë ¨ ì—†ëŠ” ì˜ì—­ì„ ë°©í•´í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤ ([Figure 12](https://arxiv.org/html/2503.20523v1#S6.F12 "In Inpainting. â€£ 6.1 Qualitative Examples â€£ 6 Results â€£ GAIA-2: A Controllable Multi-View Generative World Model for Autonomous Driving")).

![Refer to caption](./figure/12.png)

Figure 12: Agent inpainting. GAIA-2 can insert dynamic agents into masked regions of a video, guided by 3D bounding box conditioning. Background consistency and semantic realism are preserved.

These qualitative results highlight GAIA-2â€™s ability to perform fine-grained and controllable video generation, supporting its use in scalable simulation and diverse scenario creation for autonomous systems.

ì €í¬ GAIA-2ëŠ” ë¯¸ì„¸í•œ ìˆ˜ì¤€ì—ì„œ ì •êµí•˜ê³  ì œì–´ ê°€ëŠ¥í•œ ë¹„ë””ì˜¤ ìƒì„± ëŠ¥ë ¥ì„ ë³´ì—¬ì£¼ì–´, í™•ì¥ ê°€ëŠ¥í•œ ì‹œë®¬ë ˆì´ì…˜ê³¼ ë‹¤ì–‘í•œ ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±ì— í™œìš©ë  ìˆ˜ ìˆìŒì„ ì‹œì‚¬í•©ë‹ˆë‹¤.

### 6.2 Metrics

To quantitatively evaluate GAIA-2, we use a suite of metrics that assess visual fidelity, temporal consistency, and conditioning accuracy.

GAIA-2ë¥¼ ì •ëŸ‰ì ìœ¼ë¡œ í‰ê°€í•˜ê¸° ìœ„í•´, ì‹œê°ì  ì¶©ì‹¤ë„, ì‹œê°„ì  ì¼ê´€ì„±, ê·¸ë¦¬ê³  ì¡°ê±´ë¶€ ì •í™•ì„±ì„ í‰ê°€í•˜ëŠ” ë‹¤ì–‘í•œ ì§€í‘œë“¤ì„ í™œìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤.

##### Visual Fidelity.

Similarly to [^27], we use the Frechet DINO Distance (FDD) and Frechet Inception Distance (FID) [^28] to measure the distance between the distributions of generated and real videos. The feature encoder is DINOv2 [^14] ViT-L/14, and the input images are bilinearly rescaled to a higher resolution $448\times 952$ compared to $299\times 299$ for InceptionV3 [^29] in FID. We observe that FDD is less noisy than FID and saturates much later in training.

[^27]ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ, ìƒì„±ëœ ì˜ìƒê³¼ ì‹¤ì œ ì˜ìƒì˜ ë¶„í¬ ê°„ì˜ ê±°ë¦¬ë¥¼ ì¸¡ì •í•˜ê¸° ìœ„í•´ í”„ë ˆì¼€ DINO ê±°ë¦¬(FDD)ì™€ í”„ë ˆì¼€ ì¸ception ê±°ë¦¬(FID)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. íŠ¹ì§• ì¸ì½”ë”ëŠ” DINOv2[^14] ViT-L/14ì´ë©°, ì…ë ¥ ì´ë¯¸ì§€ëŠ” FIDì—ì„œ InceptionV3[^29]ì˜ 299x299ë³´ë‹¤ ë” ë†’ì€ í•´ìƒë„ì¸ 448x952ë¡œ ì´ì§„ ë³´ê°„ë²•ì„ í†µí•´ resizedë©ë‹ˆë‹¤. FDDëŠ” FIDë³´ë‹¤ ë…¸ì´ì¦ˆê°€ ëœí•˜ê³  í›ˆë ¨ì´ í›¨ì”¬ ëŠ¦ê²Œ í¬í™”ë˜ëŠ” í˜„ìƒì„ ê´€ì°°í–ˆìŠµë‹ˆë‹¤.

##### Temporal Consistency.

To evaluate the temporal consistency of the world model, we use the Frechet Video Motion Distance (FVMD) [^30]. This metric compares distributions of explicit key-point motion features for generated and ground-truth videos. We find this metric to be more aligned with human preferences for temporal consistency compared to FVD [^31].

ì„¸ê³„ ëª¨ë¸ì˜ ì‹œê°„ì  ì¼ê´€ì„±ì„ í‰ê°€í•˜ê¸° ìœ„í•´ Frechet Video Motion Distance (FVMD) [^30]ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ ì§€í‘œëŠ” ìƒì„±ëœ ì˜ìƒê³¼ ì‹¤ì œ ì˜ìƒì˜ ëª…ì‹œì ì¸ í‚¤í¬ì¸íŠ¸ ì›€ì§ì„ íŠ¹ì§• ë¶„í¬ë¥¼ ë¹„êµí•©ë‹ˆë‹¤. FVD [^31]ì— ë¹„í•´ ì¸ê°„ì˜ ì‹œê°„ì  ì¼ê´€ì„± ì„ í˜¸ë„ì™€ ë” ì¼ì¹˜í•˜ëŠ” ê²ƒì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤.

![Refer to caption](./figure/13.png)

Figure 13: Validation loss and other metrics (evaluated on n = 1024 ğ‘› n=1024 italic\_n = 1024 samples). We found that the validation loss correlates well with human perceptual preferences.

##### Dynamic Agent Conditioning.

We evaluate dynamic agent fidelity by comparing the projection of the 3D bounding boxes (our conditional signal) with segmentation masks obtained via OneFormer [^32] on the generated video. We then compute class-based Intersection-over-Union (IoU) to measure how well the model adheres to the spatial and categorical specifications of dynamic agents during generation.

ì €í¬ëŠ” ìƒì„±ëœ ë¹„ë””ì˜¤ì—ì„œ OneFormer[^32]ë¥¼ í†µí•´ ì–»ì€ ì„¸ê·¸ë©˜í…Œì´ì…˜ ë§ˆìŠ¤í¬ì™€ 3D ë°”ìš´ë”© ë°•ìŠ¤ì˜ íˆ¬ì˜ì„ ë¹„êµí•˜ì—¬ ë™ì  ì—ì´ì „íŠ¸ì˜ ì¶©ì‹¤ë„ë¥¼ í‰ê°€í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ëª¨ë¸ì´ ë™ì  ì—ì´ì „íŠ¸ì˜ ê³µê°„ì , ë²”ì£¼ì  ì œì•½ ì¡°ê±´ì— ì–¼ë§ˆë‚˜ ì˜ ë¶€í•©í•˜ëŠ”ì§€ í´ë˜ìŠ¤ ê¸°ë°˜ Intersection-over-Union (IoU)ì„ ê³„ì‚°í•˜ì—¬ ì¸¡ì •í•©ë‹ˆë‹¤.

We report the metrics and the validation loss in [Figure 13](https://arxiv.org/html/2503.20523v1#S6.F13 "In Temporal Consistency. â€£ 6.2 Metrics â€£ 6 Results â€£ GAIA-2: A Controllable Multi-View Generative World Model for Autonomous Driving"). The validation loss and other metrics are evaluated on the generation from scratch task with $n=1024$ samples. Despite some noise from small sample size, all metrics exhibit a positive trend throughout training. Notably, the validation loss shows the strongest correlation with human preference, making it a valuable proxy for perceptual quality.

ì €í¬ëŠ” [Figure 13](https://arxiv.org/html/2503.20523v1#S6.F13 "In Temporal Consistency. â€£ 6.2 Metrics â€£ 6 Results â€£ GAIA-2: A Controllable Multi-View Generative World Model for Autonomous Driving")ì— ì œì‹œëœ ì§€í‘œì™€ ê²€ì¦ ì†ì‹¤ì„ ë³´ê³ ë“œë¦½ë‹ˆë‹¤. ê²€ì¦ ì†ì‹¤ ë° ê¸°íƒ€ ì§€í‘œëŠ” $n=1024$ ìƒ˜í”Œì„ ì‚¬ìš©í•˜ì—¬ ì²˜ìŒë¶€í„° ìƒì„±í•˜ëŠ” ì‘ì—…ì—ì„œ í‰ê°€ë˜ì—ˆìŠµë‹ˆë‹¤. ì†Œê·œëª¨ ìƒ˜í”Œë¡œ ì¸í•´ ì•½ê°„ì˜ ë…¸ì´ì¦ˆê°€ ì¡´ì¬í•˜ì§€ë§Œ, ëª¨ë“  ì§€í‘œëŠ” í›ˆë ¨ ê³¼ì • ì „ë°˜ì— ê±¸ì³ ê¸ì •ì ì¸ ì¶”ì„¸ë¥¼ ë³´ì…ë‹ˆë‹¤. íŠ¹íˆ, ê²€ì¦ ì†ì‹¤ì€ ì¸ê°„ ì„ í˜¸ë„ì™€ ê°€ì¥ ê°•í•œ ìƒê´€ê´€ê³„ë¥¼ ë³´ì—¬ì£¼ì–´ ì¸ì§€ í’ˆì§ˆì— ëŒ€í•œ ìœ ìš©í•œ ì§€í‘œë¡œ í™œìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## 7 Related Work

##### Video Generative Models.

Recent advancements in video generative models have mainly been driven by models that leverage latent representations derived through dimensionality reduction. These representations can be either continuous [^1] or quantized [^3], and the success of video generation heavily depends on how efficiently these representations capture spatiotemporal information. One of the key challenges lies in ensuring that the mappings to and from latent space preserve both visual fidelity and semantic coherence. While scaling to large datasets improves generalization [^2], challenges remain in preserving both visual realism and temporal consistency.

ìµœê·¼ ë¹„ë””ì˜¤ ìƒì„± ëª¨ë¸ ë¶„ì•¼ì˜ ë°œì „ì€ ì°¨ì› ì¶•ì†Œë¥¼ í†µí•´ ì–»ì€ ì ì¬ í‘œí˜„(latent representation)ì„ í™œìš©í•˜ëŠ” ëª¨ë¸ë“¤ì— ì˜í•´ ì£¼ë„ë˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ í‘œí˜„ì€ ì—°ì†ì ì¸ í˜•íƒœ[^1] ë˜ëŠ” ì–‘ìí™”ëœ í˜•íƒœ[^3]ì¼ ìˆ˜ ìˆìœ¼ë©°, ë¹„ë””ì˜¤ ìƒì„±ì˜ ì„±ê³µì€ ì´ëŸ¬í•œ í‘œí˜„ì´ ê³µê°„ì -ì‹œê°„ì  ì •ë³´ë¥¼ ì–¼ë§ˆë‚˜ íš¨ìœ¨ì ìœ¼ë¡œ í¬ì°©í•˜ëŠ”ì§€ì— í¬ê²Œ ì˜ì¡´í•©ë‹ˆë‹¤. í•µì‹¬ì ì¸ ê³¼ì œëŠ” ì ì¬ ê³µê°„(latent space)ìœ¼ë¡œì˜ ë§¤í•‘ì´ ì‹œê°ì  ì •í™•ì„±ê³¼ ì˜ë¯¸ì  ì¼ê´€ì„±ì„ ëª¨ë‘ ë³´ì¡´í•˜ëŠ”ì§€ í™•ì¸í•˜ëŠ” ë° ìˆìŠµë‹ˆë‹¤. ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ìœ¼ë¡œ í™•ì¥í•˜ì—¬ ì¼ë°˜í™” ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ê²ƒì€ ìœ ìš©í•˜ì§€ë§Œ, ì‹œê°ì  í˜„ì‹¤ê°ê³¼ ì‹œê°„ì  ì¼ê´€ì„±ì„ ëª¨ë‘ ìœ ì§€í•˜ëŠ” ë°ëŠ” ì—¬ì „íˆ ì–´ë ¤ì›€ì´ ë‚¨ì•„ìˆìŠµë‹ˆë‹¤.

Autoregressive and masked generative models using quantized tokens have demonstrated strong modeling of temporal dependencies and motion dynamics [^34]. However, their sequential generation processes lead to slow inference and error accumulation, which can limit their scalability for long sequences or complex multi-agent scenes. In contrast, latent diffusion models offer a more parallelizable alternative, using iterative denoising to generate full video sequences. These models, including Stable Video Diffusion [^43], DIAMOND [^44], Sora [^45], MovieGen [^20], Gen-3 [^46], and Cosmos [^2], have achieved impressive quality on text-to-video and image-to-video tasks, especially in open-domain settings.

ì–‘ìí™”ëœ í† í°ì„ ì‚¬ìš©í•˜ëŠ” ìê°€ íšŒê·€ ëª¨ë¸ ë° ë§ˆìŠ¤í‚¹ ìƒì„± ëª¨ë¸ì€ ì‹œê°„ì  ì˜ì¡´ì„±ê³¼ ìš´ë™ ì—­í•™ì„ íš¨ê³¼ì ìœ¼ë¡œ ëª¨ë¸ë§í•˜ëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤ [^34]. ê·¸ëŸ¬ë‚˜ ì´ëŸ¬í•œ ìˆœì°¨ì  ìƒì„± í”„ë¡œì„¸ìŠ¤ëŠ” ëŠë¦° ì¶”ë¡  ì†ë„ì™€ ì˜¤ë¥˜ ëˆ„ì ì„ ì´ˆë˜í•˜ì—¬ ê¸´ ì‹œí€€ìŠ¤ ë˜ëŠ” ë³µì¡í•œ ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ì¥ë©´ì˜ í™•ì¥ì„±ì— ì œí•œì„ ë‘¡ë‹ˆë‹¤. ì´ì— ë°˜í•´ ì ì¬ í™•ì‚° ëª¨ë¸ì€ ë°˜ë³µì ì¸ ë…¸ì´ì¦ˆ ì œê±°ë¥¼ í†µí•´ ì „ì²´ ë¹„ë””ì˜¤ ì‹œí€€ìŠ¤ë¥¼ ìƒì„±í•˜ëŠ” ë³´ë‹¤ ë³‘ë ¬í™” ê°€ëŠ¥í•œ ëŒ€ì•ˆì„ ì œê³µí•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ëª¨ë¸, ì¦‰ Stable Video Diffusion [^43], DIAMOND [^44], Sora [^45], MovieGen [^20], Gen-3 [^46], ê·¸ë¦¬ê³  Cosmos [^2]ëŠ” íŠ¹íˆ ê°œë°©í˜• í™˜ê²½ì—ì„œ í…ìŠ¤íŠ¸-íˆ¬-ë¹„ë””ì˜¤ ë° ì´ë¯¸ì§€-íˆ¬-ë¹„ë””ì˜¤ ì‘ì—…ì—ì„œ ë›°ì–´ë‚œ í’ˆì§ˆì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.

However, most of these works prioritize visual aesthetics and domain-agnostic generation, offering limited support for structured control over scene elements or agent behavior. This makes them less suitable for applications (such as autonomous driving) that require precise, semantic, and multi-modal scene control across space and time. In this context, GAIA-2 aims to bridge the gap between high-fidelity generation and domain-specific controllability required for safety-critical systems.

í•˜ì§€ë§Œ ì´ëŸ¬í•œ ëŒ€ë¶€ë¶„ì˜ ì‘ì—…ë“¤ì€ ì‹œê°ì  ë¯¸ì  ê°ê°ê³¼ ì˜ì—­ ë¬´ê´€í•œ ìƒì„±ì— ìš°ì„ ìˆœìœ„ë¥¼ ë‘ê³  ìˆì–´, ì¥ë©´ ìš”ì†Œë‚˜ ì—ì´ì „íŠ¸ í–‰ë™ì— ëŒ€í•œ êµ¬ì¡°í™”ëœ ì œì–´ì— ëŒ€í•œ ì œí•œì ì¸ ì§€ì›ì„ ì œê³µí•©ë‹ˆë‹¤. ë”°ë¼ì„œ ììœ¨ ì£¼í–‰ê³¼ ê°™ì´ ê³µê°„ê³¼ ì‹œê°„ì— ê±¸ì³ ì •í™•í•˜ê³ , ì˜ë¯¸ ìˆê³ , ë‹¤ì¤‘ ëª¨ë‹¬ ë°©ì‹ìœ¼ë¡œ ì¥ë©´ì„ ì •ë°€í•˜ê²Œ ì œì–´í•´ì•¼ í•˜ëŠ” ì• í”Œë¦¬ì¼€ì´ì…˜ì—ëŠ” ì í•©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë§¥ë½ì—ì„œ GAIA-2ëŠ” ì•ˆì „ ê´€ë ¨ ì‹œìŠ¤í…œì— í•„ìš”í•œ ê³ í’ˆì§ˆ ìƒì„± ëŠ¥ë ¥ê³¼ ì˜ì—­ë³„ ì œì–´ ê°€ëŠ¥ì„± ì‚¬ì´ì˜ ê²©ì°¨ë¥¼ ì¢íˆëŠ” ë° ëª©í‘œë¥¼ ë‘ê³  ìˆìŠµë‹ˆë‹¤.

##### Generative World Models in Autonomous Driving.

Unlike general-purpose video generation, generative world models for autonomous driving must satisfy stricter requirements: multi-agent interaction, ego-motion control, environmental diversity, and multi-camera coherence. Early work in this space includes GAIA-1 [^5], which introduced text and ego action conditioning in a discrete world model with a video diffusion decoder to improve temporal consistency. CommaVQ [^47] similarly leveraged a causal transformer on discrete tokens for ego-motion control. Although these methods took important steps toward controllability, they often targeted single-camera settings and partial controllability via action or text conditioning, limiting their applicability to comprehensive driving simulations.

ì¼ë°˜ì ì¸ ì˜ìƒ ìƒì„± ëª¨ë¸ê³¼ëŠ” ë‹¬ë¦¬, ììœ¨ ì£¼í–‰ì„ ìœ„í•œ ìƒì„±ì  ì„¸ê³„ ëª¨ë¸ì€ ë”ìš± ì—„ê²©í•œ ìš”êµ¬ ì‚¬í•­ì„ ì¶©ì¡±í•´ì•¼ í•©ë‹ˆë‹¤. ì¦‰, ë‹¤ì¤‘ ì—ì´ì „íŠ¸ ìƒí˜¸ ì‘ìš©, ììœ¨ ì£¼í–‰ ë°©í–¥ ì œì–´, í™˜ê²½ ë‹¤ì–‘ì„±, ê·¸ë¦¬ê³  ë‹¤ì¤‘ ì¹´ë©”ë¼ ì¼ê´€ì„±ì„ ë§Œì¡±í•´ì•¼ í•©ë‹ˆë‹¤. ì´ ë¶„ì•¼ì˜ ì´ˆê¸° ì—°êµ¬ë¡œëŠ” GAIA-1[^5]ê°€ ìˆìœ¼ë©°, ì´ ì—°êµ¬ì—ì„œëŠ” ë¹„ë””ì˜¤ í™•ì‚° ë””ì½”ë”ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ì‚°ì ì¸ ì„¸ê³„ ëª¨ë¸ì—ì„œ í…ìŠ¤íŠ¸ì™€ ììœ¨ ì£¼í–‰ ë™ì‘ ì œì–´ ì¡°ê±´ì„ ë„ì…í•˜ì—¬ ì‹œê°„ì  ì¼ê´€ì„±ì„ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤. CommaVQ[^47] ì—­ì‹œ ì´ì‚° í† í°ì— ëŒ€í•œ ì¸ê³¼ì  íŠ¸ëœìŠ¤í¬ë¨¸ë¥¼ í™œìš©í•˜ì—¬ ììœ¨ ì£¼í–‰ ë°©í–¥ ì œì–´ë¥¼ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ë°©ë²•ë“¤ì€ ì œì–´ ê°€ëŠ¥ì„±ì„ í–¥í•œ ì¤‘ìš”í•œ ë°œê±¸ìŒì„ ë”›ê³ ì„°ì§€ë§Œ, ì¢…ì¢… ë‹¨ì¼ ì¹´ë©”ë¼ í™˜ê²½ì´ë‚˜ ë™ì‘ ë˜ëŠ” í…ìŠ¤íŠ¸ ì¡°ê±´í™”ë¥¼ í†µí•´ ë¶€ë¶„ì ì¸ ì œì–´ì—ë§Œ ì´ˆì ì„ ë§ì¶”ì–´, í¬ê´„ì ì¸ ì£¼í–‰ ì‹œë®¬ë ˆì´ì…˜ì— ì ìš©í•˜ëŠ” ë° ì œí•œì´ ìˆì—ˆìŠµë‹ˆë‹¤.

Subsequent approaches adopted latent diffusion for higher fidelity generation and introduced more flexible control. DriveDreamer [^6] adopts a latent diffusion model conditioned on 3D bounding boxes, HD maps, and ego actions, and further incorporates an action decoder to predict future ego actions. Drive-WM [^7] extends this approach to a multi-camera setting (up to six cameras surrounding the ego vehicle) and introduces controllability over ego actions, dynamic agents, and environmental conditions, such as weather and lighting. UniMLVG [^48] similarly enables multi-view generation conditioned on text, camera parameters, 3D bounding boxes, and HD maps, while MaskGWM [^49] extends UniMLVG to support longer video generations. Vista [^8] employs latent diffusion for generating high-resolution, long-duration videos, and Delphi [^50] uses a data-driven approach to guide video generation toward failure cases in driving models, thereby improving the synthetic dataâ€™s training utility. More recently, GEM [^51] further generalizes scene control to include ego motion, object dynamics, and even human poses, suggesting broader applicability across domains such as human motion synthesis and drone navigation. A recent line of work has explored 4D geometry-aware generation. DriveDreamer4D [^9] uses video generative models to synthesize videos along novel trajectories and combines real and generated footage to optimize a 4D Gaussian Splatting model for closed-loop simulation. Similarly, DreamDrive [^52] leverages generative priors to construct high-quality 4D scenes from in-the-wild driving data.

ì´í›„ ê°œë°œëœ ì ‘ê·¼ ë°©ì‹ë“¤ì€ ë”ìš± ë†’ì€ ìˆ˜ì¤€ì˜ í’ˆì§ˆì„ ìœ„í•œ ì ì¬ ê³µê°„ í™•ì‚° ëª¨ë¸ì„ ì±„íƒí•˜ê³ , ë³´ë‹¤ ìœ ì—°í•œ ì œì–´ ê¸°ëŠ¥ì„ ë„ì…í–ˆìŠµë‹ˆë‹¤. DriveDreamer[^6]ëŠ” 3D ë°”ìš´ë”© ë°•ìŠ¤, HD ë§µ, ê·¸ë¦¬ê³  ììœ¨ ì£¼í–‰ ì°¨ëŸ‰ì˜ í–‰ë™ì„ ì¡°ê±´ìœ¼ë¡œ ì ì¬ ê³µê°„ í™•ì‚° ëª¨ë¸ì„ í™œìš©í•˜ë©°, ë¯¸ë˜ ììœ¨ ì£¼í–‰ ì°¨ëŸ‰ì˜ í–‰ë™ì„ ì˜ˆì¸¡í•˜ê¸° ìœ„í•œ í–‰ë™ ë””ì½”ë”ë¥¼ ì¶”ê°€ì ìœ¼ë¡œ í†µí•©í•©ë‹ˆë‹¤. Drive-WM[^7]ëŠ” ì´ ì ‘ê·¼ ë°©ì‹ì„ ë‹¤ì¤‘ ì¹´ë©”ë¼ í™˜ê²½(ììœ¨ ì£¼í–‰ ì°¨ëŸ‰ ì£¼ìœ„ ìµœëŒ€ 6ëŒ€ì˜ ì¹´ë©”ë¼)ë¡œ í™•ì¥í•˜ê³ , ììœ¨ ì£¼í–‰ ì°¨ëŸ‰ì˜ í–‰ë™, ë™ì  ì—ì´ì „íŠ¸, ê·¸ë¦¬ê³  ë‚ ì”¨ ë° ì¡°ëª…ê³¼ ê°™ì€ í™˜ê²½ ì¡°ê±´ì— ëŒ€í•œ ì œì–´ ê°€ëŠ¥ì„±ì„ ë„ì…í–ˆìŠµë‹ˆë‹¤. UniMLVG[^48]ëŠ” í…ìŠ¤íŠ¸, ì¹´ë©”ë¼ íŒŒë¼ë¯¸í„°, 3D ë°”ìš´ë”© ë°•ìŠ¤, HD ë§µì„ ì¡°ê±´ìœ¼ë¡œ ë‹¤ì¤‘ ë·° ìƒì„± ê¸°ëŠ¥ì„ ì§€ì›í•˜ëŠ” ë™ì‹œì—, MaskGWM[^49]ì€ UniMLVGë¥¼ í™•ì¥í•˜ì—¬ ë” ê¸´ ì˜ìƒ ìƒì„± ê¸°ëŠ¥ì„ ì§€ì›í•©ë‹ˆë‹¤. Vista[^8]ëŠ” ê³ í•´ìƒë„, ì¥ì‹œê°„ ì˜ìƒ ìƒì„±ì„ ìœ„í•´ ì ì¬ ê³µê°„ í™•ì‚° ëª¨ë¸ì„ í™œìš©í•˜ë©°, Delphi[^50]ì€ ìš´ì „ ëª¨ë¸ì˜ ì‹¤íŒ¨ ì‚¬ë¡€ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì˜ìƒ ìƒì„± ê³¼ì •ì„ ì•ˆë‚´í•˜ê¸° ìœ„í•œ ë°ì´í„° ê¸°ë°˜ ì ‘ê·¼ ë°©ì‹ì„ ì‚¬ìš©í•˜ì—¬ í•©ì„± ë°ì´í„°ì˜ í•™ìŠµ ìœ ìš©ì„±ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤. ìµœê·¼ì—ëŠ” GEM[^51]ì´ ììœ¨ ì£¼í–‰ ì°¨ëŸ‰ì˜ ì›€ì§ì„, ê°ì²´ì˜ ë™ì—­í•™, ì‹¬ì§€ì–´ ì¸ê°„ì˜ ìì„¸ê¹Œì§€ í¬í•¨í•˜ì—¬ ì¥ë©´ ì œì–´ ê¸°ëŠ¥ì„ ë”ìš± ì¼ë°˜í™”í•˜ì—¬ ì¸ê°„ ì›€ì§ì„ í•©ì„± ë° ë“œë¡  ë‚´ë¹„ê²Œì´ì…˜ê³¼ ê°™ì€ ë‹¤ì–‘í•œ ë¶„ì•¼ì— ì ìš©ë  ìˆ˜ ìˆëŠ” ê°€ëŠ¥ì„±ì„ ì œì‹œí–ˆìŠµë‹ˆë‹¤. ìµœê·¼ ì—°êµ¬ì—ì„œëŠ” 4D ê¸°í•˜í•™ ì •ë³´ë¥¼ í™œìš©í•œ ìƒì„± ë°©ì‹ì— ëŒ€í•œ ì—°êµ¬ê°€ ì§„í–‰ë˜ê³  ìˆìŠµë‹ˆë‹¤. DriveDreamer4D[^9]ëŠ” ìƒˆë¡œìš´ ê²½ë¡œë¥¼ ë”°ë¼ ì˜ìƒì„ í•©ì„±í•˜ê¸° ìœ„í•´ ì˜ìƒ ìƒì„± ëª¨ë¸ì„ ì‚¬ìš©í•˜ë©°, ì‹¤ì œ ì˜ìƒê³¼ ìƒì„±ëœ ì˜ìƒì„ ê²°í•©í•˜ì—¬ íì‡„ ë£¨í”„ ì‹œë®¬ë ˆì´ì…˜ì„ ìœ„í•œ 4D ê°€ìš°ì‹œì•ˆ ìŠ¤í”Œë˜íŠ¸ë‹ ëª¨ë¸ì„ ìµœì í™”í•©ë‹ˆë‹¤. ìœ ì‚¬í•˜ê²Œ, DreamDrive[^52]ëŠ” ì‹¤ì œ ì£¼í–‰ ë°ì´í„°ë¡œë¶€í„° ê³ í’ˆì§ˆì˜ 4D ì¥ë©´ì„ êµ¬ì„±í•˜ê¸° ìœ„í•´ ìƒì„±ì  ìš°ì„  ìˆœìœ„ë¥¼ í™œìš©í•©ë‹ˆë‹¤.

Despite these advancements, most of these solutions only address subsets of the requirements for realistic driving simulation. Some are limited to single-camera settings, others lack structured agent-level control, and few integrate multi-view generation and agent-level semantics into a single framework. Additionally, many do not support fine-grained editing tasks such as inpainting or targeted scene modification, essential for data augmentation and scenario variation in simulation loops.

ìµœê·¼ ì´ëŸ¬í•œ ê¸°ìˆ  ë°œì „ì—ë„ ë¶ˆêµ¬í•˜ê³ , ëŒ€ë¶€ë¶„ì˜ ì†”ë£¨ì…˜ì€ í˜„ì‹¤ì ì¸ ìš´ì „ ì‹œë®¬ë ˆì´ì…˜ì— í•„ìš”í•œ ìš”êµ¬ ì‚¬í•­ì˜ ì¼ë¶€ë§Œì„ í•´ê²°í•©ë‹ˆë‹¤. ì–´ë–¤ ì†”ë£¨ì…˜ì€ ë‹¨ì¼ ì¹´ë©”ë¼ ì„¤ì •ì—ë§Œ ì œí•œë˜ëŠ” ê²½ìš°ê°€ ìˆìœ¼ë©°, ë‹¤ë¥¸ ì†”ë£¨ì…˜ì€ ì—ì´ì „íŠ¸ ìˆ˜ì¤€ì˜ êµ¬ì¡°í™”ëœ ì œì–´ ê¸°ëŠ¥ì„ ì œê³µí•˜ì§€ ì•Šê±°ë‚˜, ë‹¤ì¤‘ ë·° ìƒì„± ë° ì—ì´ì „íŠ¸ ìˆ˜ì¤€ì˜ ì˜ë¯¸ë¡ ì„ ë‹¨ì¼ í”„ë ˆì„ì›Œí¬ì— í†µí•©í•˜ì§€ ëª»í•˜ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤. ê²Œë‹¤ê°€, ë§ì€ ì†”ë£¨ì…˜ì€ ì¸í˜ì¸íŒ…ì´ë‚˜ íŠ¹ì • ì¥ë©´ ìˆ˜ì •ê³¼ ê°™ì€ ì„¸ë°€í•œ í¸ì§‘ ì‘ì—…ì„ ì§€ì›í•˜ì§€ ì•Šì•„ë„ ë˜ëŠ”ë°, ì´ëŠ” ì‹œë®¬ë ˆì´ì…˜ ë£¨í”„ ë‚´ì—ì„œ ë°ì´í„° ì¦ê°• ë° ì‹œë‚˜ë¦¬ì˜¤ ë‹¤ì–‘ì„±ì„ ìœ„í•œ í•µì‹¬ì ì¸ ìš”ì†Œì…ë‹ˆë‹¤.

Motivated by these limitations, GAIA-2 is designed as a unified latent diffusion framework that brings together:

- multi-camera generation (up to five views);
- structured and semantic conditioning over ego action, dynamic agents, and scene-level metadata;
- flexible inference modes, including generation from scratch, inpainting, scene editing, and long-horizon rollouts; and
- support for external latent spaces like CLIP and driving scenario embeddings.

ì´ëŸ¬í•œ í•œê³„ì ì„ ê³ ë ¤í•˜ì—¬ GAIA-2ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ê¸°ëŠ¥ì„ í†µí•©í•œ ë‹¨ì¼ ì ì¬ í™•ì‚° í”„ë ˆì„ì›Œí¬ë¡œ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.

- ìµœëŒ€ ë‹¤ì„¯ ê°œì˜ ë·°ë¥¼ ìƒì„±í•˜ëŠ” ë‹¤ì¤‘ ì¹´ë©”ë¼ ê¸°ëŠ¥
- ìì•„ í–‰ë™, ë™ì  ì—ì´ì „íŠ¸, ì¥ë©´ ìˆ˜ì¤€ ë©”íƒ€ë°ì´í„°ì— ëŒ€í•œ êµ¬ì¡°í™” ë° ì˜ë¯¸ ê¸°ë°˜ ì¡°ê±´í™”
- ìƒì„±ë¶€í„° ì‹œì‘, ì¸í˜ì¸íŒ…, ì¥ë©´ í¸ì§‘, ì¥ê¸° ë¡¤ì•„ì›ƒ ë“± ìœ ì—°í•œ ì¶”ë¡  ëª¨ë“œ ì§€ì›
- CLIP ë° ë“œë¼ì´ë¹™ ì‹œë‚˜ë¦¬ì˜¤ ì„ë² ë”©ê³¼ ê°™ì€ ì™¸ë¶€ ì ì¬ ê³µê°„ ì§€ì›

By combining continuous latent representations with large-scale training on a geographically and environmentally diverse dataset, GAIA-2 delivers both visual fidelity and scenario controllability. It fills a critical gap between general-purpose diffusion models and the domain-specific needs of autonomous driving, namely, the ability to simulate realistic, controllable, multi-view driving scenes for robust, scalable training and evaluation.

ì§€ë¦¬ì , í™˜ê²½ì  ë‹¤ì–‘ì„±ì´ í’ë¶€í•œ ë°ì´í„°ì…‹ì„ í™œìš©í•˜ì—¬ ì§€ì†ì ì¸ ì ì¬ í‘œí˜„ê³¼ ê²°í•©ëœ GAIA-2ëŠ” ë›°ì–´ë‚œ ì‹œê°ì  í’ˆì§ˆê³¼ ì‹œë‚˜ë¦¬ì˜¤ ì œì–´ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤. ì´ëŠ” ì¼ë°˜ì ì¸ í™•ì‚° ëª¨ë¸ê³¼ ììœ¨ ì£¼í–‰ì˜ íŠ¹ì • ìš”êµ¬ ì‚¬ì´ì˜ ì¤‘ìš”í•œ ê²©ì°¨ë¥¼ ë©”ìš°ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤. ì¦‰, ê²¬ê³ í•˜ê³  í™•ì¥ ê°€ëŠ¥í•œ í•™ìŠµ ë° í‰ê°€ë¥¼ ìœ„í•´ í˜„ì‹¤ì ì´ê³  ì œì–´ ê°€ëŠ¥í•œ ë‹¤ì¤‘ ë·° ì£¼í–‰ ì¥ë©´ì„ ì‹œë®¬ë ˆì´ì…˜í•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ì„ ì œê³µí•©ë‹ˆë‹¤.

## 8 Conclusions and Future Work

We presented GAIA-2, a domain-specialized latent diffusion model for video generation, designed to meet the diverse and exacting requirements of simulation in autonomous driving. GAIA-2 sets a new benchmark in generative world modeling by unifying multi-camera coherence, structured semantic conditioning, and fine-grained control within a scalable architecture. The model supports controllable generation conditioned on ego-vehicle dynamics, environmental features, road layout, and dynamic agents, while maintaining spatial and temporal consistency across up to five camera views.

ì €í¬ëŠ” ììœ¨ ì£¼í–‰ ì‹œë®¬ë ˆì´ì…˜ì˜ ë‹¤ì–‘í•œ ìš”êµ¬ì‚¬í•­ê³¼ ê¹Œë‹¤ë¡œìš´ ê¸°ì¤€ì„ ì¶©ì¡±í•˜ë„ë¡ ì„¤ê³„ëœ, ë¹„ë””ì˜¤ ìƒì„±ì— íŠ¹í™”ëœ ì ì¬ í™•ì‚° ëª¨ë¸ì¸ GAIA-2ë¥¼ ì†Œê°œí•˜ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤. GAIA-2ëŠ” ë‹¤ì¤‘ ì¹´ë©”ë¼ ì¼ê´€ì„±, êµ¬ì¡°í™”ëœ ì˜ë¯¸ ì¡°ê±´ë¶€, í™•ì¥ ê°€ëŠ¥í•œ ì•„í‚¤í…ì²˜ ë‚´ ë¯¸ì„¸í•œ ì œì–´ ê¸°ëŠ¥ì„ í†µí•©í•˜ì—¬ ìƒì„± ì„¸ê³„ ëª¨ë¸ë§ì˜ ìƒˆë¡œìš´ ê¸°ì¤€ì„ ì œì‹œí•©ë‹ˆë‹¤. ì´ ëª¨ë¸ì€ ììœ¨ ì£¼í–‰ ì°¨ëŸ‰ì˜ ë™ì—­í•™, í™˜ê²½ íŠ¹ì§•, ë„ë¡œ ë ˆì´ì•„ì›ƒ, ê·¸ë¦¬ê³  ë™ì  ì—ì´ì „íŠ¸ ë“±ì„ ì¡°ê±´ìœ¼ë¡œ í•˜ì—¬, ìµœëŒ€ 5ê°œì˜ ì¹´ë©”ë¼ ë·°ë¥¼ í†µí•´ ê³µê°„ì , ì‹œê°„ì  ì¼ê´€ì„±ì„ ìœ ì§€í•˜ë©´ì„œ ì œì–´ ê°€ëŠ¥í•œ ìƒì„±ì„ ì§€ì›í•©ë‹ˆë‹¤. 

The architecture combines a space-time factorized latent diffusion world model with a compact and semantically meaningful video tokenizer, enabling high-fidelity generation across diverse driving environments. GAIA-2 supports multiple inference modesâ€”including generation from scratch, autoregressive rollouts, spatial inpainting, and real-scene editingâ€”each empowering systematic exploration of the driving scene space. The model excels at producing typical as well as rare safety-critical scenarios, thus significantly enhancing data diversity, scenario coverage, and validation rigor in autonomous driving systems.

ì €í¬ GAIA-2ëŠ” ê³µê°„-ì‹œê°„ ìš”ì¸ ë¶„í•´ëœ ì ì¬ í™•ì‚° ì„¸ê³„ ëª¨ë¸ê³¼, ê°„ê²°í•˜ë©´ì„œë„ ì˜ë¯¸ ìˆëŠ” ë¹„ë””ì˜¤ í† í¬ë‚˜ì´ì €ë¥¼ ê²°í•©í•˜ì—¬, ë‹¤ì–‘í•œ ì£¼í–‰ í™˜ê²½ì—ì„œ ê³ í’ˆì§ˆì˜ ê²°ê³¼ë¬¼ì„ ìƒì„±í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤. GAIA-2ëŠ” ìƒì„±ë¶€í„° ì‹œì‘, ìê°€ íšŒê·€ ë¡¤ì•„ì›ƒ, ê³µê°„ ì¸í˜ì¸íŒ…, ì‹¤ì‹œê°„ ì¥ë©´ í¸ì§‘ ë“± ë‹¤ì–‘í•œ ì¶”ë¡  ëª¨ë“œë¥¼ ì§€ì›í•˜ë©°, ê° ëª¨ë“œëŠ” ì£¼í–‰ ì¥ë©´ ê³µê°„ì— ëŒ€í•œ ì²´ê³„ì ì¸ íƒìƒ‰ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤. íŠ¹íˆ, ì¼ë°˜ì ì¸ ìƒí™©ë¿ë§Œ ì•„ë‹ˆë¼ ë“œë¬¼ì§€ë§Œ ì•ˆì „ì— ì¤‘ìš”í•œ ìƒí™©ê¹Œì§€ í›Œë¥­í•˜ê²Œ ìƒì„±í•  ìˆ˜ ìˆì–´, ììœ¨ ì£¼í–‰ ì‹œìŠ¤í…œì˜ ë°ì´í„° ë‹¤ì–‘ì„±, ì‹œë‚˜ë¦¬ì˜¤ ì»¤ë²„ë¦¬ì§€, ê·¸ë¦¬ê³  ê²€ì¦ì˜ ì—„ë°€ì„±ì„ í¬ê²Œ í–¥ìƒì‹œí‚µë‹ˆë‹¤. 

By scaling controllability and scene diversity in simulation, GAIA-2 bridges the gap between real-world data limitations and the increasing demands of training and evaluating robust AI-driving models. In doing so, it serves as a powerful tool for accelerating iteration cycles, improving generalization to out-of-distribution conditions, and stress-testing AI agents across complex, long-tail scenarios.

ì‹œë®¬ë ˆì´ì…˜ì—ì„œ ì œì–´ ê°€ëŠ¥ì„±ê³¼ ì¥ë©´ ë‹¤ì–‘ì„±ì„ í™•ì¥í•¨ìœ¼ë¡œì¨ GAIA-2ëŠ” ì‹¤ì œ ë°ì´í„°ì˜ í•œê³„ì™€ ê°•ë ¥í•œ AI ììœ¨ì£¼í–‰ ëª¨ë¸ì„ í›ˆë ¨í•˜ê³  í‰ê°€í•˜ëŠ” ì¦ê°€í•˜ëŠ” ìš”êµ¬ ì‚¬ì´ì˜ ê²©ì°¨ë¥¼ ì¢í˜€ ë“œë¦½ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ë³µì¡í•˜ê³  ê¸´ ê¼¬ë¦¬ ë¶„í¬ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ AI ì—ì´ì „íŠ¸ì˜ ì„±ëŠ¥ì„ ê°€ì†í™”í•˜ê³ , ì¼ë°˜í™” ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¤ë©°, ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ë¥¼ ìˆ˜í–‰í•˜ëŠ” ë° ê°•ë ¥í•œ ë„êµ¬ë¡œ í™œìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

##### Future Work.

While GAIA-2 represents a significant step forward, several research directions remain open and will guide future iterations of this work: (i) Like all generative models, GAIA-2 occasionally produces temporal or semantic inconsistencies, particularly in long-horizon or complex scenarios. Improving the reliability and consistency of video generation through better failure detection, refinement models, or constraint-aware sampling is a key challenge. (ii) Although GAIA-2 enables parallelized generation, real-time or near-real-time video synthesis remains computationally intensive. Future work will explore model distillation, efficient transformer variants, and inference-time acceleration techniques to improve deployment feasibility in real-world pipelines. (iii) Despite supporting a rich set of conditionings, new efforts will target richer agent behavior modeling, more nuanced environmental conditions, and open-ended natural language control. Further enrichment of the training dataset with rare and safety-critical eventsâ€”especially those involving human agents, complex infrastructure, or social interactionsâ€”will push the boundaries of realistic simulation.

GAIA-2ëŠ” ê´„ëª©í•  ë§Œí•œ ì§„ì „ì„ ë³´ì—¬ì£¼ì§€ë§Œ, ì—¬ì „íˆ í•´ê²°í•´ì•¼ í•  ì—°êµ¬ ë°©í–¥ë“¤ì´ ë‚¨ì•„ ìˆìœ¼ë©°, í–¥í›„ ì´ ì—°êµ¬ì˜ ë°œì „ ë°©í–¥ì„ ì œì‹œí•  ê²ƒì…ë‹ˆë‹¤. êµ¬ì²´ì ìœ¼ë¡œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

(i) ëª¨ë“  ìƒì„± ëª¨ë¸ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ, GAIA-2ëŠ” íŠ¹íˆ ì¥ê¸°ì  ë˜ëŠ” ë³µì¡í•œ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ì‹œê°„ì  ë˜ëŠ” ì˜ë¯¸ì  ì¼ê´€ì„± ë¶€ì¡± í˜„ìƒì„ ê°„í˜¹ ë³´ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë”ìš± ì‹ ë¢°í•  ìˆ˜ ìˆê³  ì¼ê´€ì„± ìˆëŠ” ë¹„ë””ì˜¤ ìƒì„± ê¸°ìˆ ì„ í™•ë³´í•˜ê¸° ìœ„í•´, ì˜¤ë¥˜ íƒì§€ ê°œì„ , ì •ì œ ëª¨ë¸ ê°œë°œ, ë˜ëŠ” ì œì•½ ì¡°ê±´ì— ìœ ì˜í•˜ëŠ” ìƒ˜í”Œë§ ë°©ë²•ì„ ì—°êµ¬í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•œ ê³¼ì œì…ë‹ˆë‹¤.

(ii) GAIA-2ëŠ” ë³‘ë ¬ ìƒì„± ê¸°ëŠ¥ì„ ì§€ì›í•˜ì§€ë§Œ, ì‹¤ì‹œê°„ ë˜ëŠ” ê±°ì˜ ì‹¤ì‹œê°„ ë¹„ë””ì˜¤ í•©ì„±ì—ëŠ” ì—¬ì „íˆ ìƒë‹¹í•œ ì»´í“¨íŒ… ìì›ì´ í•„ìš”í•©ë‹ˆë‹¤. í–¥í›„ ì—°êµ¬ì—ì„œëŠ” ëª¨ë¸ ì••ì¶•, íš¨ìœ¨ì ì¸ íŠ¸ëœìŠ¤í¬ë¨¸ ë³€í˜•, ê·¸ë¦¬ê³  ì¶”ë¡  ì‹œê°„ ê°€ì†í™” ê¸°ìˆ  ë“±ì„ íƒêµ¬í•˜ì—¬ ì‹¤ì œ íŒŒì´í”„ë¼ì¸ì—ì„œì˜ ë°°í¬ ê°€ëŠ¥ì„±ì„ ë†’ì´ëŠ” ë° ì£¼ë ¥í•  ê²ƒì…ë‹ˆë‹¤.

(iii) ë‹¤ì–‘í•œ ì¡°ê±´ë¶€ ì§€ì›ì„ ì œê³µí•˜ëŠ” ê²ƒ ì™¸ì—ë„, ìƒˆë¡œìš´ ë…¸ë ¥ì€ ë”ìš± í’ë¶€í•œ ì—ì´ì „íŠ¸ í–‰ë™ ëª¨ë¸ë§, ë¯¸ë¬˜í•œ í™˜ê²½ ì¡°ê±´, ê·¸ë¦¬ê³  ê°œë°©í˜• ìì—°ì–´ ì œì–´ì— ì´ˆì ì„ ë§ì¶”ì–´ ì§„í–‰ë  ê²ƒì…ë‹ˆë‹¤. íŠ¹íˆ, ì¸ê°„ ì—ì´ì „íŠ¸, ë³µì¡í•œ ì¸í”„ë¼, ë˜ëŠ” ì‚¬íšŒì  ìƒí˜¸ ì‘ìš©ì´ í¬í•¨ëœ í¬ê·€í•˜ê³  ì•ˆì „ì— ì¤‘ìš”í•œ ì´ë²¤íŠ¸ë“¤ì„ í•™ìŠµ ë°ì´í„°ì— ì¶”ê°€í•¨ìœ¼ë¡œì¨, ë”ìš± í˜„ì‹¤ì ì¸ ì‹œë®¬ë ˆì´ì…˜ì˜ í•œê³„ë¥¼ í™•ì¥í•  ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤.

Together, these directions will ensure that GAIA-2 and its successors continue to advance the role of generative models as core infrastructure in the development of safe, robust, and generalizable autonomous systems.

ì €í¬ëŠ” ì´ ì§€ì¹¨ë“¤ì„ í†µí•´ GAIA-2ì™€ ê·¸ í›„ì† ëª¨ë¸ë“¤ì´ ì•ˆì „í•˜ê³ , ê²¬ê³ í•˜ë©°, ì¼ë°˜í™”ëœ ììœ¨ ì‹œìŠ¤í…œ ê°œë°œì˜ í•µì‹¬ ì¸í”„ë¼ë¡œì„œì˜ ì—­í• ì„ ì§€ì†ì ìœ¼ë¡œ ë°œì „ì‹œí‚¬ ìˆ˜ ìˆë„ë¡ ì§€ì›í•  ê²ƒì…ë‹ˆë‹¤.

## Acknowledgments and Disclosure of Funding

The authors would like to thank and acknowledge the many individuals whose invaluable contributions made this work possible: Alex Persin, Ana-Maria Marcu, Aniruddha Kembhavi, Benoit Hanotte, Evgenii Kashin, Francesca Iovu, Giacomo Gallino, Giulio Dâ€™Ippolito, Jeremy Plassmann, Lorenzo von Ritter, Mohamed Nabil, Nikhil Mohan, Oleh Chernov, Pragya Kale, Remi Tachet, Rudi Rankin, Sahana Vankatesh, Sarah Belghiti, SofÃ­a Dudas, Tilly Pielichaty, Vassia Simaiaki, Victor DelÃ©pine, Vincent Micheli, Zak Murez.

## References

[^1]: D. P. Kingma and M. Welling.Auto-encoding variational bayes.*Proceedings of the International Conference on Learning Representations (ICLR)*, 2014.

[^2]: N. Agarwal, A. Ali, M. Bala, Y. Balaji, E. Barker, T. Cai, P. Chattopadhyay, Y. Chen, Y. Cui, Y. Ding, et al.Cosmos world foundation model platform for physical ai.*arXiv preprint arXiv:2501.03575*, 2025.

[^3]: A. van den Oord, O. Vinyals, and K. Kavukcuoglu.Neural discrete representation learning.In *Advances in Neural Information Processing Systems (NeurIPS)*, 2017.

[^4]: P. Esser, R. Rombach, and B. Ommer.Taming transformers for high-resolution image synthesis.In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, 2021.

[^5]: A. Hu, L. Russell, H. Yeo, Z. Murez, G. Fedoseev, A. Kendall, J. Shotton, and G. Corrado.Gaia-1: A generative world model for autonomous driving.*Technical Report arXiv:2309.17080*, 2023.

[^6]: X. Wang, Z. Zhu, G. Huang, X. Chen, J. Zhu, and J. Lu.Drivedreamer: Towards real-world-driven world models for autonomous driving.*Proceedings of the European Conference on Computer Vision (ECCV)*, 2024a.

[^7]: Y. Wang, J. He, L. Fan, H. Li, Y. Chen, and Z. Zhang.Driving into the future: Multiview visual forecasting and planning with world model for autonomous driving.In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 14749â€“14759, 2024b.

[^8]: S. Gao, J. Yang, L. Chen, K. Chitta, Y. Qiu, A. Geiger, J. Zhang, and H. Li.Vista: A generalizable driving world model with high fidelity and versatile controllability.*Advances in Neural Information Processing Systems (NeurIPS)*, 2024.

[^9]: G. Zhao, C. Ni, X. Wang, Z. Zhu, X. Zhang, Y. Wang, G. Huang, X. Chen, B. Wang, Y. Zhang, et al.Drivedreamer4d: World models are effective data machines for 4d driving scene representation.*arXiv preprint arXiv:2410.13571*, 2024.

[^10]: J. Chen, H. Cai, J. Chen, E. Xie, S. Yang, H. Tang, M. Li, Y. Lu, and S. Han.Deep compression autoencoder for efficient high-resolution diffusion models.*Proceedings of the International Conference on Learning Representations (ICLR)*, 2025.

[^11]: Y. HaCohen, N. Chiprut, B. Brazowski, D. Shalem, D. Moshe, E. Richardson, E. Levin, G. Shiran, N. Zabari, O. Gordon, P. Panet, S. Weissbuch, V. Kulikov, Y. Bitterman, Z. Melumian, and O. Bibi.Ltx-video: Realtime video latent diffusion.*arXiv preprint*, 2024.

[^12]: W. Shi, J. Caballero, F. Huszar, J. Totz, A. P. Aitken, R. Bishop, D. Rueckert, and Z. Wang.Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network.In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, 2016.

[^13]: J. Johnson, A. Alahi, and L. Fei-Fei.Perceptual losses for real-time style transfer and super-resolution.In *Proceedings of the European Conference on Computer Vision (ECCV)*, 2016.

[^14]: M. Oquab, T. Darcet, T. Moutakanni, H. V. Vo, M. Szafraniec, V. Khalidov, P. Fernandez, D. Haziza, F. Massa, A. El-Nouby, R. Howes, P.-Y. Huang, H. Xu, V. Sharma, S.-W. Li, W. Galuba, M. Rabbat, M. Assran, N. Ballas, G. Synnaeve, I. Misra, H. Jegou, J. Mairal, P. Labatut, A. Joulin, and P. Bojanowski.DINOv2: Learning robust visual features without supervision.*Transactions on Machine Learning Research (TMLR)*, 2024.

[^15]: R. Zhang.Making convolutional networks shift-invariant again.In *Proceedings of the International Conference on Machine Learning (ICML)*, 2019.

[^16]: T. Miyato, T. Kataoka, M. Koyama, and Y. Yoshida.Spectral normalization for generative adversarial networks.In *Proceedings of the International Conference on Learning Representations (ICLR)*, 2018.

[^17]: Y. Lipman, R. T. Q. Chen, H. Ben-Hamu, M. Nickel, and M. Le.Flow matching for generative modeling.*Proceedings of the International Conference on Learning Representations (ICLR)*, 2023.

[^18]: J. Ho, A. Jain, and P. Abbeel.Denoising diffusion probabilistic models.In *Advances in Neural Information Processing Systems (NeurIPS)*, 2020.

[^19]: W. Peebles and S. Xie.Scalable diffusion models with transformers.In *Proceedings of the IEEE International Conference on Computer Vision (ICCV)*, 2023.

[^20]: A. Polyak, A. Zohar, A. Brown, A. Tjandra, A. Sinha, A. Lee, A. Vyas, B. Shi, C.-Y. Ma, C.-Y. Chuang, et al.Movie gen: A cast of media foundation models.*arXiv preprint*, 2024.

[^21]: M. Dehghani, J. Djolonga, B. Mustafa, P. Padlewski, J. Heek, J. Gilmer, A. Steiner, M. Caron, R. Geirhos, I. Alabdulmohsin, R. Jenatton, L. Beyer, M. Tschannen, A. Arnab, X. Wang, C. Riquelme, M. Minderer, J. Puigcerver, U. Evci, M. Kumar, S. van Steenkiste, G. F. Elsayed, A. Mahendran, F. Yu, A. Oliver, F. Huot, J. Bastings, M. P. Collier, A. Gritsenko, V. Birodkar, C. Vasconcelos, Y. Tay, T. Mensink, A. Kolesnikov, F. PavetiÄ‡, D. Tran, T. Kipf, M. LuÄiÄ‡, X. Zhai, D. Keysers, J. Harmsen, and N. Houlsby.Scaling vision transformers to 22 billion parameters.In *Proceedings of the International Conference on Machine Learning (ICML)*, 2023.

[^22]: J. B. W. Webber.A bi-symmetric log transformation for wide-range data.*Measurement Science and Technology*, 2012.

[^23]: S. Wang, Y. Liu, T. Wang, Y. Li, and X. Zhang.Exploring object-centric temporal modeling for efficient multi-view 3d object detection.In *Proceedings of the IEEE/CVF International Conference on Computer Vision*, pages 3621â€“3631, 2023.

[^24]: A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever.Learning transferable visual models from natural language supervision.*Proceedings of the International Conference on Machine Learning (ICML)*, 2021.

[^25]: R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer.High-resolution image synthesis with latent diffusion models.In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, 2022.

[^26]: R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang.The unreasonable effectiveness of deep features as a perceptual metric.In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, 2018.

[^27]: G. Stein, J. C. Cresswell, R. Hosseinzadeh, Y. Sui, B. L. Ross, V. Villecroze, Z. Liu, A. L. Caterini, J. E. T. Taylor, and G. Loaiza-Ganem.Exposing flaws of generative model evaluation metrics and their unfair treatment of diffusion models.In *Advances in Neural Information Processing Systems (NeurIPS)*, 2023.

[^28]: M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter.Gans trained by a two time-scale update rule converge to a local nash equilibrium.In *Advances in Neural Information Processing Systems (NeurIPS)*, 2017.

[^29]: C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna.Rethinking the inception architecture for computer vision.In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, 2016.

[^30]: J. Liu, Y. Qu, Q. Yan, X. Zeng, L. Wang, and R. Liao.FrÃ©chet video motion distance: A metric for evaluating motion consistency in videos.In *Proceedings of the International Conference on Machine Learning, workshop (ICMLw)*, 2024.

[^31]: T. Unterthiner, S. Steenkiste, K. Kurach, R. Marinier, M. Michalski, and S. Gelly.Towards accurate generative models of video: A new metric & challenges.*arXiv preprint*, 2018.

[^32]: J. Jain, J. Li, M. Chiu, A. Hassani, N. Orlov, and H. Shi.Oneformer: One transformer to rule universal image segmentation.In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, 2023.

[^33]: NVIDIA.Cosmos tokenizer: A suite of image and video neural tokenizers.[https://research.nvidia.com/labs/dir/cosmos-tokenizer](https://research.nvidia.com/labs/dir/cosmos-tokenizer), 2024.

[^34]: W. Yan, Y. Zhang, P. Abbeel, and A. Srinivas.VideoGPT: Video generation using vq-vae and transformers.In *arXiv preprint*, 2021.

[^35]: G. L. Moing, J. Ponce, and C. Schmid.CCVS: Context-aware controllable video synthesis.In *Advances in Neural Information Processing Systems (NeurIPS)*, 2021.

[^36]: S. Ge, T. Hayes, H. Yang, X. Yin, G. Pang, D. Jacobs, J.-B. Huang, and D. Parikh.Long video generation with time-agnostic vqgan and time-sensitive transformer.*Proceedings of the European Conference on Computer Vision (ECCV)*, 2022.

[^37]: Y. Seo, K. Lee, F. Liu, S. James, and P. Abbeel.HARP: Autoregressive latent video prediction with high-fidelity image generator.In *Proceedings of the IEEE International Conference on Image Processing (ICIP)*, 2022.

[^38]: C. Hawthorne, A. Jaegle, C. Cangea, S. Borgeaud, C. Nash, M. Malinowski, S. Dieleman, O. Vinyals, M. Botvinick, I. Simon, H. Sheahan, N. Zeghidour, J.-B. Alayrac, J. Carreira, and J. Engel.General-purpose, long-context autoregressive modeling with Perceiver AR.In *Proceedings of the International Conference on Machine Learning (ICML)*, 2022.

[^39]: V. Micheli, E. Alonso, and F. Fleuret.Transformers are sample-efficient world models.*Proceedings of the International Conference on Learning Representations (ICLR)*, 2023.

[^40]: W. Yan, D. Hafner, S. James, and P. Abbeel.Temporally consistent transformers for video generation.In *Proceedings of the International Conference on Machine Learning (ICML)*, 2023.

[^41]: R. Villegas, M. Babaeizadeh, P.-J. Kindermans, H. Moraldo, H. Zhang, M. T. Saffar, S. Castro, J. Kunze, and D. Erhan.Phenaki: Variable length video generation from open domain textual description.In *Proceedings of the International Conference on Learning Representations (ICLR)*, 2023.

[^42]: L. Yu, Y. Cheng, K. Sohn, J. Lezama, H. Zhang, H. Chang, A. G. Hauptmann, M.-H. Yang, Y. Hao, I. Essa, and L. Jiang.MAGVIT: Masked Generative Video Transformer.In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, 2023.

[^43]: A. Blattmann, T. Dockhorn, S. Kulal, D. Mendelevitch, M. Kilian, D. Lorenz, Y. Levi, Z. English, V. Voleti, A. Letts, et al.Stable video diffusion: Scaling latent video diffusion models to large datasets.*arXiv preprint*, 2023.

[^44]: E. Alonso, A. Jelley, V. Micheli, A. Kanervisto, A. Storkey, T. Pearce, and F. Fleuret.Diffusion for world modeling: Visual details matter in atari.In *Advances in Neural Information Processing Systems (NeurIPS)*, 2024.

[^45]: OpenAI.Video generation models as world simulators.[https://openai.com/index/video-generation-models-as-world-simulators](https://openai.com/index/video-generation-models-as-world-simulators), 2024.

[^46]: Runway.Introducing gen-3 alpha: A new frontier for video generation.[https://runwayml.com/research/introducing-gen-3-alpha](https://runwayml.com/research/introducing-gen-3-alpha), 2024.

[^47]: comma.ai.commavq.[https://github.com/commaai/commavq](https://github.com/commaai/commavq), 2023.

[^48]: R. Chen, Z. Wu, Y. Liu, Y. Guo, J. Ni, H. Xia, and S. Xia.Unimlvg: Unified framework for multi-view long video generation with comprehensive control capabilities for autonomous driving.*arXiv preprint arXiv:2412.04842*, 2024.

[^49]: J. Ni, Y. Guo, Y. Liu, R. Chen, L. Lu, and Z. Wu.Maskgwm: A generalizable driving world model with video mask reconstruction.*arXiv preprint arXiv:2502.11663*, 2025.

[^50]: E. Ma, L. Zhou, T. Tang, Z. Zhang, D. Han, J. Jiang, K. Zhan, P. Jia, X. Lang, H. Sun, et al.Unleashing generalization of end-to-end autonomous driving with controllable long video generation.*arXiv preprint arXiv:2406.01349*, 2024.

[^51]: M. Hassan, S. Stapf, A. Rahimi, P. Rezende, Y. Haghighi, D. BrÃ¼ggemann, I. Katircioglu, L. Zhang, X. Chen, S. Saha, et al.Gem: A generalizable ego-vision multimodal world model for fine-grained ego-motion, object dynamics, and scene composition control.*arXiv preprint arXiv:2412.11198*, 2024.

[^52]: J. Mao, B. Li, B. Ivanovic, Y. Chen, Y. Wang, Y. You, C. Xiao, D. Xu, M. Pavone, and Y. Wang.Dreamdrive: Generative 4d scene modeling from street view images.*arXiv preprint arXiv:2501.00601*, 2024.
