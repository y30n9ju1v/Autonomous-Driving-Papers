14.1 Problem Definition One of the basic requirements of autonomous driving is to fully understand the surrounding area, such as a complex traffic scene. The complex task of outdoor scene understanding involves several sub-tasks such as depth estima- tion, scene categorization, object detection and tracking, event categorization, and more. Each of these tasks describes a particular aspect of a scene. It can be beneficial to model some of these aspects jointly in order to exploit the complementary nature of the different cues in the scene and to obtain a more holistic understanding. The goal of most scene understanding models is to obtain a rich but compact representation of the scene including its elements, e.g., layout, traffic participants, and their relation with each other.

In contrast to modeling these problems in 2D, 3D reasoning allows geo- metric scene understanding and results in a more informative representation of the scene in the form of 3D object models, layout elements, and occlusion relationships. In this section, we will focus on a subset of 3D scene understand- ing techniques that are particularly relevant to the autonomous driving task, excluding works on scene graph estimation or image tagging. One specific challenge in this context is the interpretation of urban and sub-urban traffic scenarios. Compared to highways and rural roads, urban scenarios comprise dynamic objects, a large degree of variability in the geometric layout of roads and crossroads, and an increased level of difficulty due to ambiguous visual features, occlusions, and challenging illumination conditions.

14.2 Methods While early work in computer vision [566, 273, 80, 501] already tackled the scene understanding problem from various perspectives, e.g., using a block world assumption [566] or via bottom-up top-down inference [501], most ap- proaches relied on heuristics rather than learning and were not able to general- ize to complex real-world scenes. In contrast, modern approaches try to learn complex relationships directly from data. In their pioneering work, Hoiem et al. [300] infer the overall 3D structure of an outdoor scene from a single image. The surface layout is represented as a set of coarse geometric classes with certain orientations such as support, vertical, and sky. These elements are inferred by learning an appearance-based model for each class. Oliveira et al. [503] propose a time-varying 3D representation using a set of planar polygons as primitives. Given 3D LiDAR point clouds, they find the support plane using RANSAC followed by a clustering of inliers to separate instances.

14.2.1 Road Topology and Traffic Participants For autonomous driving, understanding the road topology and other traffic participants in the scene is of utmost importance. Ess et al. [196] use semantic segmentation as an intermediate representation to extract the road topology and to detect crosswalks and other traffic participants. In addition, their intermediate representation simultaneously encodes the spatial layout of the scene. Wojek and Schiele [720] detect vehicles and track them with a temporal filter based on a linear motion model. They also estimate the camera motion and propagate it to the next frame using a dynamic Conditional Random Field model for joint labeling of object and scene classes. However, [196, 720] only infer a topological model of the scene and not a geometric model.

Wojek et al. [718] extend [720] to a probabilistic 3D scene model that en- compasses multi-class object detection, object tracking, scene labeling, and reasoning about geometric relations. Geiger et al. [241] jointly reason about the 3D scene layout of intersections as well as the location and orientation of vehicles in the scene. They present a probabilistic generative model captur- ing the scene topology, geometry, and traffic activities by leveraging vehicle tracks, semantic labels, scene flow and occupancy grids.

Apart from 3D primitive-based representations, there exist other ways of representing a street scene. A more fine-grained model of the road is proposed by Topfer et al. [668]. The complex road scene is hierarchically decomposed into roads, lanes, and finally road-edges and lane-markings. This allows them to infer a more expressive model of the road compared to [241]. Seff and Xiao [606] define a list of road layout attributes such as the number of lanes, drivable directions, distance to intersections, etc. They first automatically collect a large-scale dataset for these attributes by leveraging existing street view image databases and online navigation maps (e.g., OpenStreetMap). Based on this dataset, they train a deep convolutional network to predict each attribute from a single street view image.

14.2.2 Physical and Temporal Relationships While the detection of traffic participants is addressed in our review on ob- ject detection (Chapter 5) and object tracking (Chapter 6) approaches, scene understanding systems aim at integrating object detection and tracking with physical constraints and model the temporal behavior and relationship be- tween traffic participants and the scene. Pellegrini et al. [513] model inter- actions between pedestrians (social behavior) and the scene (collisions) in a multi-target tracking formulation. Kuettel et al. [372] model spatio-temporal dependencies of moving agents in complex dynamic scenes by learning co- occurring activities and temporal rules between them. However, both ap- proaches assume a static observer and a long observation period, i.e., the scene must be observed for a significant period of time before making a deci- sion, therefore it is not applicable to autonomous systems.

In contrast, [722, 723, 778] consider a moving vehicle as observer and construct expressive 3D scene models by reasoning about occlusions and traffic patterns. Wojek et al. [722, 723] integrate multiple object part detectors [718] into the 3D scene model for explicit object-object occlusion reasoning (Figure 14.2). In addition, they enforce physically plausible trajectories by pruning geometrically infeasible detections. Zhang et al. [778] propose a more expressive generative model of 3D urban scenes similar to [241]. While the independent tracklets in [241] can lead to implausible inference results, they reason about high-level semantics in the form of traffic patterns to avoid this problem (Figure 14.1) and force the solution to conform to traffic rules. This allows them to significantly improve scene estimation and vehicle-to-lane association results. Wang et al. [704] propose a top-view representation for complex road scenes that can be inferred from a single camera using a deep neural network.

14.3 Discussion While early work on scene understanding struggled to infer expressive models of the real world, learning-based approaches led to models with increasing expressivity, ranging from simple 2D models to represent road topologies and objects [196, 720], to more complex 3D models [503, 241] which also incorpo- rate physical [723, 704] and temporal [722, 723, 778] constraints. As motivated in [606], more expressive models can reduce the dependency on high definition maps. However, the level of expressiveness needed in autonomous driving re-mains an open question and the accuracy achieved by state-of-the-art scene understanding models is still limited. In addition, a unified evaluation of scene understanding approaches is difficult due to the varying complexity of models and the different challenges they tackle.