13.1 Problem Definition Navigating a vehicle requires a precise understanding of the position and orientation of the car. Localization is a well-studied problem in both robotics and vision, covering a broad range of techniques from indoor localization using noisy sensory measurements to locating where a picture was taken. From an autonomous driving perspective, the main task is to localize the vehicle on a map in order to exploit static features provided by the map. The task of generating a map of the world is defined as the mapping problem. In this chapter, we discuss approaches for generating both metric as well as semantic maps. While metric maps allow for accurate localization, semantic maps provide problem-specific information such as the location of parking areas. Maps for localization can be generated offline, exploiting accurate, but computationally expensive optimization techniques.

In contrast to localization, the ego-motion estimation problem considers the change in position and orientation of the vehicle. While this problem can be addressed more efficiently than the localization problem (the previous posi- tion is assumed to be known), small inaccuracies quickly accumulate to larger drifts. Approaches for Simultaneous Localization and Mapping (SLAM) ad- dress this problem by detecting loop closures to correct for drift.

13.2 Mapping Street, aerial, and satellite imagery enable the generation of precise metric and semantic maps. Depending on the required level of detail, various com- puter vision techniques, i.e., multi-view reconstruction, scene understanding, or semantic segmentation, are typically employed for generation maps.

13.2.1 Metric Maps For autonomous driving, 2D metric maps (i.e., representing information in bird’s-eye view) are usually sufficient for localization. Methods for scene un- derstanding, such as [196, 720, 718, 241, 668, 606, 778] discussed in Chapter 14 can also be used to extract road features. 3D information can be obtained using multi-view reconstruction techniques operating on street-level [2, 208, 209] (Section 10.2) or aerial [217, 58, 182] images.

The Google Street View project [14] is a prominent example of a large collection of panoramic images that are registered with respect to each other to form a world map, see Figure 13.1. For registering the dataset, Anguelov et al. [14] estimate the pose of the vehicle using a Kalman filter, fusing data from GPS, wheel encoder, and inertial navigation [357]. The pose estimates are refined with a probabilistic graphical model, and the 3D scene geometry is recovered by robustly fitting coarse meshes to the 3D measurements.

Levinson et al. [409] propose to construct a map based on aggregated reflectance measurements from a LiDAR scanner. They exploit these maps for centimeter-accurate LiDAR-based localization during the DARPA Urban Challenge. In contrast, Geiger [239] presents an approach for road mosaicing in dynamic environments with the goal of creating obstacle-free bird’s-eye views. The road surface is extracted using optical flow on Harris corners and approximated by a plane. Afterwards, multiple road reconstructions are combined using multi-band blending.

13.2.2 Semantic Maps Metric maps ignore semantic information, which is important for some tasks such as automated parking. Semantic maps are necessary to address this problem. Several approaches address the creation of semantic maps [711, 712, 479, 685, 454, 455, 710, 455] . Scene understanding approaches like [196, 241] also estimate semantic classes to extract road topologies but do not create a semantic map.

Sengupta et al. [611] present an approach to generate a semantic overhead map of an urban scene from street-level images. They formulate the problem using two CRFs. The first is used for semantic image segmentation of the street view images treating each image independently. Each street view image is then projected into an overhead view. These views are then aggregated over many images to form the input for a second CRF producing a semantic labeling of the ground plane.

In contrast, Grimmett et al. [257] fuse semantic and metric maps for vision- only automated parking. They update the map with static and dynamic labels and use active learning for lane, parking space, and pedestrian crossings detection.

13.3 Localization Localization can be performed using either a sensor like GPS or visual in- formation based on images. Using GPS alone typically provides an accuracy of around 5 meters. Although centimeter-level precision is possible in unob-structed environments using correction signals and a combination of several sensors as in the KITTI car [243], it is often rendered infeasible in traffic scenes with several disturbing effects such as occlusions by vegetation and buildings or multi-path effects due to reflections. Therefore, image-based localization independent of satellite systems remains highly relevant.

Visual localization techniques are commonly classified into metric and topological methods. Metric localization [160, 500] is achieved by computing the 3D pose with respect to a map. Topological localization approaches [419, 793, 281] provide a coarse estimate from a finite set of possible locations that are represented as nodes in a graph and connected by edges that link them according to some distance or appearance criteria. Metric localization can be very accurate, but is usually not suitable for very long sequences, while topological localization may be more reliable, but only provides rough estimates.

Metric Localization: The problem of metric map localization has been traditionally addressed using Monte Carlo methods which recover the proba- bility distribution over the agent’s pose by drawing a set of samples. Dellaert et al. [160] define indoor localization in two steps, global position estimation and tracking of the local position over time. Instead of modeling the probabil- ity density function itself, they represent uncertainty by a set of samples and update the representation over time using Monte Carlo methods. This allows them to model arbitrary multi-modal distributions in a memory-efficient way.

Outdoor localization is, in general, more challenging compared to the in- door localization task due to its scale and often unreliable sensor information, e.g., GPS failures. Oh et al. [500] use semantic information available in maps to compensate for the failure cases of GPS sensors. By exploiting knowledge about the environment, they assign low probabilities to implausible map lo- cations, e.g., inside buildings. They incorporate these map-based priors into their particle filter formulation to bias the motion model towards areas of higher probability.

Topological Localization: Early image-based techniques [419, 793] ap- proach the problem of localizing in topological maps as classification into one of a predefined set of places which are often referred to as “landmarks”. Oth- ers [281, 141, 512, 669, 15] create a database of images with known locations and formulate localization as an image retrieval problem. These methods require a similarity measure to compare images based on local or global ap- pearance cues. The larger the database, the more difficult the localization task becomes. Challenges include appearance changes, similar-looking places, and changes due to viewpoint or position. In Figure 13.2, we show an example for the appearance change of a scene over different seasons from the Workshop organized by Balntas et al. [30].

Lowry et al. [436] provide a comprehensive review of visual place recogni-tion techniques. Given a map of the environment, the goal of place recognition is to decide whether the current observation is a place already included in the map, and if so, which one.

Topometric Localization: In contrast to purely topological methods, the graph of a topometric localization model is more fine-grained: each node cor- responds to a metric location without semantic meaning. Towards this goal, Badino et al. [21] propose to construct a graph using the vehicle’s position from GPS at fixed distance intervals while associating visual or 3D features to the corresponding graph node. At runtime, real-time localization is performed using a Bayes filter to estimate the probability distribution of the vehicle po- sition along the route by matching features extracted from the sensor data to the map’s feature database. Brubaker et al. [82] leverage a graph-based representation. In contrast to traditional localization approaches, however, they do not require a visual feature database of the environment, but instead, directly build this graph from road networks extracted from OpenStreetMap. They further propose a probabilistic model that allows inferring a distribu- tion over the vehicle location along the edges of this road graph using visual odometry measurements. For tractability in very large environments, they leverage several analytic approximations for efficient inference yielding higher stability compared to particle-based filtering techniques.

Scale and Accuracy: The scale of the target area is a distinctive prop- erty to compare different approaches and is related to the accuracy achieved. Both scale and accuracy depend on the methodology used, such as map-based approaches [82] which cover a large area but might suffer from the errors on the map compared to descriptor-based approaches [21, 603] on a smaller area. While the descriptor-based method of Badino et al. [21] achieves an average localization accuracy of 1 m over an 8 km route, the localization approach of Brubaker et al. [82] which requires only road networks as input attains an accuracy of 4 m on a 18 km2 map containing 2,150 km of drivable roads.

Schreiber et al. [603] point out that the required precision for autonomous driving and future driver assistance systems is in the range of a few centime- ters and present a feature- and map-matching-based localization algorithm which can achieve centimeter-level accuracy on approximately 50 km of rural roads. They approach the problem from the perspective of lane recognition. First, they create a highly accurate map that contains road markings and curbs. Then while driving, they detect and match them to the map in order to determine the position of the vehicle relative to the markings.

13.3.1 Structure-based Localization While the output of the aforementioned localization approaches is either a rough camera position or a distribution over positions, another line of work which is known as “structure-based localization”, aims to estimate all camera matrix parameters, including position, orientation, and sometimes also cam- era intrinsics. Estimating the intrinsics usually enables more accurate results. Localization is realized as a 2D-to-3D matching problem where the 2D points on the images are matched to a large, geo-registered 3D point cloud, and the pose is estimated with respect to correspondences as shown in Figure 13.3.

Direct matching by approximate nearest neighbor search using SIFT fea- tures usually results in many incorrect matches. Therefore, many approaches rely on the SIFT ratio test [435] to detect and reject ambiguous matches. This works well on small to medium scale scenes. However, with growing model size, the discriminative power of the descriptors decreases, and many matches will be rejected by the ratio test. On the other hand, relaxing the ratio test leads to many ambiguous and wrong matches.

Several approaches [319, 579, 420, 648, 777] address this problem by re- stricting the search space. Irschara et al. [319] and Sarlin et al. [579] use image retrieval techniques to identify parts of the scene which likely include the query image. Afterwards, 2D-3D matching is performed to 3D points visible in the retrieved images. In contrast, Li et al. [420] find statistical co-occurrences of 3D model points in images and then use them as a sampling prior for RANSAC to exploit co-visibility relations. In addition, they employ a bidirectional matching scheme, forward from features in the image to points in the database and inverse from points to image features. They show that the bidirectional approach performs better than forward or inverse matching alone. Sv¨arm et al. [648] and Zeisl et al. [777] propose to use geometric cues to obtain matches that are likely to be inliers. They also exploit the gravity direction obtained from gravitational sensors and an approximation of the camera height to reduce the search space.

Besides ambiguities, the efficiency of the matching stage and memory re- quirements to store the large number of descriptors contained in the model are also problems related to large scale. Therefore, several approaches use only a subset of the 3D points [441] or present compression schemes for the descriptors [441, 581, 580, 426, 95] for more efficient matching or memory reduction. Sattler et al. [581] and Sattler et al. [580] use quantization into a fine vocabulary to accelerate the matching stage where each descriptor is rep- resented by its word ID. Sattler et al. [581] separate the difficult problem of finding a unique 2D-3D matching into two simpler ones. They first establish locally unique 2D-3D matches using a fine visual vocabulary and a visibility graph which encodes the visibility relation between 3D points and cameras. Then, they disambiguate these matches by using a simple voting scheme to enforce the co-visibility of the selected 3D points. Their experiments show that matching based on a visual vocabulary leads to state-of-the-art results. Sattler et al. [580] propose a prioritized matching scheme based on quantiza- tion, focusing on efficiency. They significantly accelerate 2D-to-3D matching by considering more likely features first and terminating the correspondence search as soon as enough matches are found. A hybrid approach combining the idea of working on a subset of 3D points and the compression of the descriptors is presented by Camposeco et al. [95]. For a small subset of 3D points, they keep the full appearance information, while for a larger set of points, they store a compressed descriptor. This enables them to obtain a more complete representation of the scene with a memory consumption sim- ilar to the previous approaches.

Deep Learning: The motivation for using CNNs for structure-based localization is to learn high-level information which might help to handle problems like textureless areas, motion blur, and illumination changes. In contrast to classical lo- calization approaches whose runtime depends on several factors such as the number of features found in a query image or the number of 3D points in the model, the runtime of CNN-based approaches only depends on the size of the network.

Kendall et al. [343] and Walch et al. [694] use a convolutional neural network to regress the camera pose from a single RGB image in an end-to-end manner. Kendall et al. [343] modify GoogLeNet [651] by replacing the softmax classifiers with affine regressors and inserting another fully connected layer before the final regressor, which can be used as a localization feature vector for further analysis. The final architecture, dubbed PoseNet, is initialized by using the weights of classification networks trained on giant datasets such as ImageNet [162] and Places [794]. The network is further fine-tuned on a new pose dataset which was automatically created using SfM to generate camera poses from a video of the scene. Walch et al. [694] use a similar approach, but in addition, they spatially correlate each element of the output of the CNN using Long Short-Term Memory (LTSM) units. This way, the network is able to capture more contextual information and outperform PoseNet in different localization tasks, including large-scale outdoor, small-scale indoor, and a newly proposed large-scale indoor localization benchmark.

Recently, Brahmbhatt et al. [66] proposed MapNet for representing maps as deep neural networks. They exploit visual odometry and GPS in addition to images for image-based localization and formulate geometric constraints as additional loss terms. Thus, the model can be updated in a self-supervised fashion using unlabeled data. This allows them to significantly improve in comparison to PoseNet-based approaches. The model is illustrated in Fig- ure 13.4.

While previous methods [343, 694, 66] regress the absolute pose in a given scene, another line of work [577, 31] proposes to learn the relative pose with respect to an image retrieved from a database. Eventually, the absolute pose is obtained from the known pose of the retrieved image and the relative pose.

Sattler et al. [583] notice that PoseNet-based approaches [343, 694] are not able to outperform simple image retrieval approaches [669] and learning- based approaches are in general still inferior to structure-based approaches such as [647].

13.3.2 Cross-view Localization It is a very difficult endeavor to keep an up-to-date repository of ground-level imagery around the world. In contrast, establishing live maps from aerial and satellite images is comparably easier. This motivated the development of geo-localization approaches that try to register ground-level images to aerial imagery. The underlying idea is to learn a mapping between ground-level and aerial image viewpoints to localize a ground-level query in an aerial reference image database.

Lin et al. [422] match ground-level queries to other ground-level reference photos as in traditional geo-localization, but then use the overhead appear- ance and land cover attributes of those ground-level matches to build sliding- window classifiers in the aerial and land cover domain. In contrast to previous methods, they are able to localize a query even if it has no corresponding ground-level image in the database by learning the co-occurrence of features in different views. Inspired by the success of face verification algorithms using deep learning, Lin et al. [423] train a Siamese network to match cross-view pairs of the same location. Towards this goal, they collect a cross-view patch dataset using range data and camera parameters from Google Street View. Finally, they warp the dominant building surface plane to appear approxi- mately as a 45% aerial view. In contrast, Workman et al. [727] use CNNs for extracting ground-level image features and predict these features from aerial images of the same location. This way, the CNN is able to extract seman- tically meaningful features from aerial images without manually specifying semantic labels. They conclude that the cross-view localization approach can obtain a precise estimate of the geographic locations which are distinctive from above. Otherwise, it can be used as a pre-processing step to a more expensive matching process.

Buildings Facades: Several methods have been developed which specialize in building facades from cross-view matching. The repeating patterns yield valuable matching cues, as illustrated in Figure 13.5. By combining satellite and oblique bird’s-eye views, Bansal et al. [33] first extract building outlines as well as facades and then match the ground image to oblique aerial images based on a statistical description of the facade pattern. Wolff et al. [725] define a matching cost function to compare street-view motifs to aerial view motifs based on the similarity of color, texture, and edge-based context features.

Geo-Referenced Reconstruction: Another line of work addresses the problem of geo-referencing a reconstruction by automatic alignment with a satellite image, floor plan, map, or other overhead views. Kaminsky et al. [335] compute the optimal alignment between SfM reconstructions and over- head images using an objective function that matches 3D points to image edges and imposes free space constraints based on the visibility of points in each camera. Matching ground and aerial images directly is a difficult endeavor due to the large differences in camera viewpoints, occlusions, and imaging conditions. Instead of seeking invariant feature detections, Shan et al. [617] propose a viewpoint-dependent matching technique by exploiting approximate alignment information and the underlying 3D geometry.

13.3.3 Semantic Alignment from LiDAR Several companies acquire LiDAR data from scanners mounted on cars driv- ing through cities to acquire 3D models of real-world urban environments. However, the accuracy of the 3D point positions acquired by the 3D scan- ners depends on the scanner poses predicted by GPS, inertial sensors, and structure-from-motion, which often fail in urban environments. These mis- alignments cause problems for point cloud registration methods. Yu et al. [769] propose to align semantic features that can be matched robustly at dif-ferent scales. By following a coarse-to-fine approach, they first successively align roads, facades, and poles which can be matched robustly. Afterwards, they match cars and other small objects which require better initial align- ments to find correct correspondences. The use of semantic features provides a globally consistent alignment of LiDAR scans, and their evaluation shows improvement over the initial alignments.

13.4 Ego-Motion Estimation One of the simplest ways of estimating the ego-motion of a vehicle is to use the wheel angle in combination with the output of wheel encoders which measure the rotation of the wheel. These methods suffer from wheel slip in uneven terrain or adverse conditions and can not recover from errors in the measurements. Visual odometry and LiDAR-based odometry techniques that estimate ego-motion from visual observations (images or laser range mea- surements) are more robust in many situations and can correct for drift by loop closure detection, i.e., by recognizing re-visited places (Section 13.4.2). In this section, we provide a summary of the most relevant visual odome- try techniques for autonomous driving. For a more detailed survey on visual odometry techniques, we refer the reader to Scaramuzza and Fraundorfer [588] and Fraundorfer and Scaramuzza [213].

In visual odometry, the goal is to recover a trajectory (i.e., a sequence of poses) of one camera or a camera system comprising multiple cameras from images. Most approaches incrementally estimate the relative transformation between two frames and integrate this information over time to recover the full trajectory. The incremental approach is illustrated in Figure 13.6. Methods on visual odometry can be roughly divided into two main categories: feature- based methods [432, 499, 589, 397, 354, 489] that extract features from key points to optimize a geometric error, and direct formulations [497, 346, 184, 186, 198, 200, 800, 757, 185] which directly operate on raw measurements by optimizing the photometric error.

Feature-based methods typically detect corners in the image and match the corresponding feature descriptors across different images. While these ap- proaches are very efficient, they discard valuable information, e.g., straight or curved edges, that are very common in man-made environments. In contrast, direct methods leverage structural information in the entire image. There- fore, these methods usually achieve higher accuracy and robustness in envi- ronments with fewer key points. In addition, they allow to simultaneously estimate semi-dense [184, 186] and even dense depth maps [639, 497], as il- lustrated in Figure 13.7. However, direct methods suffer more from local minima in the optimization problem compared to feature-based methods, in particular when the pose initialization is far from the true solution. Initially, the field was dominated by feature-based methods since they are typically more efficient, but direct formulations have recently grown in popularity due to their increased accuracy [497, 639, 346, 184, 186, 198, 200, 800, 757, 185].

Feature-based Methods
2D-to-2D Matching: Depending on how corresponding points between two time steps are represented (2D or 3D), different methods must be used to obtain the camera transformation. The essential matrix (or fundamental ma- trix), which represents the epipolar geometry between the two cameras and contains relative pose information, can be recovered from 2D feature matches (2D-to-2D). One of the most popular algorithms for estimating the essential or fundamental matrix is the eight-point algorithm [277]. The five-point algo-rithm [499] is a minimal solution that only applies to the scenario of calibrated cameras. Scaramuzza et al. [589] estimate the essential matrix from monoc- ular images with only one 2D feature correspondence using non-holonomic constraints of wheeled vehicles imposing a restrictive motion model.

In general, visual odometry with monocular images cannot recover the metric scale due to the inherent scale ambiguity. Lee et al. [397] extend [589] to a novel two-point minimal solution that is able to obtain the metric scale using a multi-camera system. In contrast to the non-holonomic constraints, Lee et al. [399] assume the vertical directions to be known (from an Inertial Measurement Unit) and propose a minimal four-point and linear eight-point algorithm for a multi-camera system. Kitt et al. [354] estimate the ego-motion using trifocal geometry, which relates features between three images. Most algorithms employ RANSAC for robust estimation. The number of iterations necessary to guarantee that a correct solution is found with RANSAC depends on the number of points from which the model can be instantiated. Minimal solvers allow to the reduction of the number correspondences leading to a reduced number of iterations and runtime of the approach.

Omnidirectional cameras discussed in Section 3.1.1 enable feature-based approaches that extract and match interest points from all around the car. The increased field of view makes the visual odometry problem more con- strained and consequently allows for more accurate visual odometry. Scara- muzza and Siegwart [587] exploit this observation and estimate the ego-motion of the vehicle relative to the road from a single, central omnidirectional camera using a homography-based tracker for the ground plane and an appearance- based tracker for the rotation of the vehicle.

3D-to-2D Matching: If stereo or RGB-D information is available, a sim- ple solution to the visual odometry problem is to project 3D features from one image into the other view and optimize for the pose by minimizing re- projection errors. Following this idea, Geiger et al. [246] present a real-time visual odometry and sparse 3D reconstruction method. They detect sparse features in stereo images using blob and corner detectors and estimate the vehicle’s ego-motion by minimizing the reprojection error of the projected 3D features. In addition, they propose a real-time stereo reconstruction algo- rithm [245] and fuse disparity maps over time into a coherent city-scale 3D reconstruction.

3D-to-3D Matching: When dealing with 3D correspondences (3D-to-3D), the relative transformation between two time steps can be obtained by align- ing the two sets of 3D features, for instance, using the iterative closest point (ICP) algorithm [49]. In visual odometry, the features extracted from im- ages are projected into 3D using depth, whereas LiDAR-based approaches such as Zhang and Singh [780, 781] directly obtain the 3D points from the sensor. However, the triangulated 3D points from stereo will exhibit a large anisotropic uncertainty due to the small baseline and the quadratic increase of errors with respect to distance. Thus it is more natural to minimize re- projection errors in the images where error statistics can be approximated more easily. Laser-based approaches do not suffer from this problem and thus typically optimize in 3D space.

Direct Methods In contrast to feature-based methods that optimize reprojection errors, di- rect approaches optimize the photometric error for estimating motion. Engel et al. [184] estimate a semi-dense inverse depth map for whole-image align- ment of monocular images. Depth is estimated using multi-view stereo for pixels with non-negligible gradients and is represented by a Gaussian prob- ability distribution. They propagate depth information from frame to frame and obtain camera poses by minimizing the photometric error. With this semi-dense formulation, they achieve comparable performance to fully dense methods [497] while not requiring a depth sensor [346]. Engel et al. [185] present a direct sparse approach for monocular visual odometry. They use a probabilistic model and jointly optimize all model parameters (camera poses, camera intrinsics, and inverse depth) in real-time.

13.4.1 Drift The incremental approach to ego-motion estimation greatly suffers from drift caused by the accumulation of estimation errors of the individual transforma- tions. One way of alleviating the drift problem is to use an iterative refine- ment over several images that are observed most recently. In feature-based approaches, this is done by reprojecting image points into 3D by triangula- tion and minimizing the sum of squared reprojection errors (sliding window bundle adjustment or windowed bundle adjustment). However, simpler tech- niques such as a proper selection of the extracted features can also reduce drift. Kitt et al. [354] use bucketing to obtain well distributed corner-like feature matches, whereas Deigmoeller and Eggert [157] use various heuristics on flow and depth estimation to reject non-stable features.

The drift problem can also be addressed with simultaneous localization and mapping (SLAM) discussed in Section 13.4.3, which jointly estimates the location and a map of the environment to recognize places that have been visited before. The detection of already mapped places is also known as “loop closure detection”. If a loop has been detected, additional constraints can be added to the bundle adjustment problem, which leads to globally consistent maps and vehicle poses. However, poses are only corrected in hindsight, and thus, the drift problem persists during longer periods in which no loop closure can be detected. Furthermore, as loop closure detection is computationally expensive and computation increases with the length of the trajectory, such techniques are often only executed sporadically and not with every new incoming frame.

13.4.2 Loop Closure Detection The relocalization in already mapped areas is an important subproblem of SLAM, known as loop closure detection. Relocalization is used to correct drift in the trajectory and inaccuracies in the map caused by drift.

Cummins and Newman [141] present a probabilistic approach for the recognition of places based on their appearance. They learn a generative model of appearances using a bag-of-words model as distinctive combinations of visual words will often arise from common objects. The generative model is robust and works even in visually repetitive environments. The performance of the approach is demonstrated on a self-recorded dataset and visualized in Figure 13.8. Paul and Newman [512] extend this idea by incorporating pair- wise distances between words coupled to the observation of visual words using a random graph. The random graph models the pairwise distance between words besides their distribution of occurrences. In contrast, Lee et al. [398] consider a pose graph with vertices representing camera poses and edges rep- resenting constraints between the poses. They show that the relative pose with metric scale between two loop-closing vertices can be obtained from the epipolar geometry of a multi-camera system with overlapping views.

Image-based loop closure detection can become unreliable in case of strong illumination or viewpoint changes. In contrast, LiDAR-based localization is not affected by changes in illumination and does not suffer as much from changes in viewpoint due to the captured 3D geometry and the large field of view. Dub´e et al. [183] propose a loop closure detection algorithm based on matching 3D segments. Segments from the point cloud are extracted and described using a combination of descriptors. Matching of segments is performed by obtaining candidates with k-d tree search in feature space and estimating matching scores using a random forest.

13.4.3 Simultaneous Localization and Mapping (SLAM) A detailed map of the environment simplifies planning and navigation in au- tonomous vehicles. However, in places for which no map is provided or the map is outdated or incomplete, the autonomous car must locate itself while generating the map. Further, the map needs to be updated continuously to reflect environmental changes over time. In this context, SLAM refers to the task of simultaneous estimation of the location of an agent while continu- ously constructing a map of the environment. While SLAM addresses a sim- ilar problem as structure-from-motion techniques discussed in Section 10.2, SLAM approaches focus particularly on large-scale environments, loop-closure detection, and real-time performance.

Traditionally, a map is represented by a set of landmarks that may cor- respond to semantically meaningful parts or detected image features. Early approaches to SLAM have addressed the problem with Bayesian formulations using extended Kalman filters [629] or particle filters [478]. Given the last state and new observations, the current state, represented by pose, velocity, and the locations of the landmarks is recursively updated. However, this for- mulation is not applicable to large environments since the belief state and time complexity of the filter update grow quadratically with the number of landmarks in the map (n).

One solution for reducing complexity is to leverage filtering techniques that maintain a tractable approximation of the belief state as proposed by Paskin [511]. However, filtering may lead to inconsistent maps when applied to non-linear SLAM problems [331]. In contrast, full SLAM approaches, such as graph-based or least-squares formulations, provide more accurate solutions as they consider all poses at once. Kaess et al. [334] propose an incremental smoothing and mapping approach based on fast incremental matrix factor- ization. They extend their earlier work [161] on factorizing the matrix of a non-linear least-squares problem to an incremental approach that only recal- culates entries which change in the matrix. Kaess et al. [333] introduce the Bayes tree, a novel data structure, which allows for a better understanding of the connection between inference in graphical models and sparse matrix fac- torization. Factored probability densities are encoded in the Bayes tree which naturally maps to a sparse matrix. Recently, Lenac et al. [404] proposed a filtering-based SLAM method that is able to compete with graph-based opti- mization techniques.

Stereo SLAM: Stereo cameras are a popular choice for tackling the SLAM problem since they allow to estimate the depth while simultaneously pro- viding detailed information of an objects’ appearance (in contrast to LiDAR sensors). Lategahn et al. [391] propose a dense stereo visual SLAM method that estimates a dense 3D map. Using a sparse visual SLAM system, they ob-tain the pose and a sparse map. For the dense 3D map, they compute a dense representation from stereo in a local coordinate system and continuously up- date the map by tracking the local coordinate systems with the sparse SLAM system. Engel et al. [187] propose LSD-SLAM, a real-time large-scale direct SLAM algorithm that couples static stereo from a camera setup with tempo- ral multi-view stereo (Figure 13.9). This allows them to estimate the depth of pixels that are under-constrained in static stereo while avoiding scale-drift that occurs using multi-view stereo. The images are directly aligned based on the photoconsistency of high contrast pixels. Mur-Artal et al. [489] use the ORB features proposed by Rublee et al. [572] for tracking, mapping, relocal-ization, and loop closure. They combine methods from loop detection [232], loop closing [637, 636], and pose graph optimization [375] into a single system which they call ORB-SLAM and which became one of the most widely used SLAM systems today.

A fusion approach is proposed by Leutenegger et al. [406] in order to take advantage of the complementary nature of visual and inertial cues. They use a non-linear optimization approach and integrate IMU measurements with reprojection errors into a joint cost function. Similarly, Usenko et al. [679] also propose a joint visual-inertial SLAM method. However, they present a fully direct method based on [187] that estimates geometry from semi-dense depth maps in contrast to sparse key points.

Environmental Changes: Changes in the environment that might not be represented in the map are a major challenge in SLAM. Levinson et al. [409] alleviate this problem by creating a map comprising of features that are very likely to be static over time. Using 3D LiDAR, they retain only flat surfaces and obtain an infrared reflectivity map of overhead views of the road surface. The map is then used to locate a vehicle with a particle filter in real-time. Levinson and Thrun [410] extend this work considering maps as probability distributions over environment properties instead of a fixed representation. Specifically, every cell of the probabilistic map is represented as its own Gaussian distribution. This allows them to represent the world more accurately and localize with fewer errors. In addition, they use offline SLAM to align multiple passes of the same environment at different times to establish an increasingly robust understanding of the world.

13.5 Datasets Several datasets have been considered in the localization and ego-motion es- timation literature. The popular dataset 7 Scenes from Shotton et al. [624] focuses only on indoor scenes. Large-scale reconstruction datasets such as Vienna [319], Dubrovnik [416], Rome [140] are very popular in particular for structure-based localization methods. With the introduction of deep learning to structure-based localization, [343] presented a new outdoor localization dataset (Cambridge Landmarks dataset), which became popular for CNN- based approaches. Most of the aforementioned datasets are limited in their variety in terms of weather conditions and seasons, which are important fac- tors for evaluating the robustness of localization systems. To address this issue, Carlevaris-Bianco et al. [96] proposed a new long-term vision and Li- DAR dataset created on the campus of the University of Michigan comprising 27 sessions. Recently, Sattler et al. [582] presented three datasets for the same problem: Aachen Day-Night, RobotCar Seasons, and CMU Seasons. [21] also recorded a dataset in different weather conditions, seasons, and during the night as well as day.

Only few datasets exist which particularly address the visual odometry problem. Most of these datasets are either small [628, 507, 56], provide only low-quality images [260], or are not yet established [445, 312]. A notable ex- ception is the KITTI benchmark [243] discussed in Chapter 4, which provides a large dataset of challenging sequences and evaluation metrics as well as an online evaluation server. We list the current leading monocular, stereo, and LiDAR methods on the KITTI benchmark in Table 13.1, Table 13.2, and Table 13.3, respectively.

13.6 Metrics For image-retrieval approaches, a popular metric is the percentage of recog- nized queries (Recall at N). A place is considered recognized if at least one of the top N retrievals are within 25 meters from the query. For autonomous driving, this precision is not satisfactory since a higher accuracy is necessary to navigate through the environment. Consequently, localization approaches for loop closure detection [141, 512, 398] strive for higher accuracy and typi- cally consider the precision-recall metric.

Structure-based localization approaches consider the position error (Eu- clidean distance between the estimated pose and the ground truth pose) as well as the orientation error. Sattler et al. [582] report the percentage of local- ized query images that differ from the ground truth pose using high (0.25m, 2deg), medium (0.5, 5deg), and low (5m, 10deg) accuracy thresholds.

The performance of methods for visual odometry is often measured using the Absolute Trajectory Error (ATE) or Relative Pose Error (RPE). The APE estimates the absolute distance between the estimated and ground truth trajectory. The RPE considers a fixed time interval and measures the local accuracy of the translational and rotational component. The KITTI dataset reports the average translational and rotational error measured for all possible subsequences of length (100, 200, . . . , 800) meters.

13.7 State of the Art on KITTI Localization: A unified and established benchmark for localization methods is still missing which makes the comparison of different approaches difficult. However, several newly introduced datasets [96, 582], reveal open challenges to the community. Sattler et al. [582] compare two structure-based meth- ods [580, 647] and three image retrieval approaches [669, 15, 141] on their dataset. While the structure-based methods significantly outperform the im- age retrieval approaches and show better robustness, all methods fail in more challenging conditions, particularly at night, when foliage changes as well as in suburban and park regions.

Monocular Visual Odometry: Monocular visual odometry methods are able to recover motion only up to a scale factor. The absolute scale can be determined by computing the size of objects in the scene, from motion constraints, or by integrating other sensors.

Fanani et al. [198] follow a direct approach and propagate 3D key points into the next frame using relative pose predictions. Combined with the scale estimation method proposed in [199] which uses dense and sparse ground plane estimates for scale correction, they achieve competitive results in Ta- ble 13.1. However, their approach is not applicable in real-time. In contrast, Mirabdollah and Mertsching [473] follow a robust feature-based monocular visual odometry approach capable of real-time estimation using the iterative five-point method. They obtain the location of landmarks using a probabilis- tic triangulation method and estimate the scale of the motion from sparse low-quality features on the ground plane. Fanani et al. [200] improve the scale correction of [199] by utilizing street pixels detected with a convolu- tional neural network for ground plane pose estimation. Furthermore, they extend the keypoint propagation method presented in [198] which allows them to improve on previous work.

In contrast to other approaches, Pereira et al. [515] consider backward mo- tion with a backward-facing camera or by processing the images of a forward facing-camera in reverse order. They argue that initial depth estimation of sparse feature matching approaches is not very accurate since usually, new features are initialized the first time they have been observed in the far dis- tance. By considering the reverse order for a forward-facing camera, new features will be detected in the nearest frame, which allows more accurate depth estimates in case of forward motion.

Recently, Yang et al. [757] propose to use deep monocular depth predic- tions for monocular visual odometry by incorporating depth predictions into a windowed direct bundle adjustment. With this direct approach, they out- perform all monocular visual odometry methods in Table 13.1. However, we remark that the KITTI dataset requires metric output, thus scale drift and scale estimation have a strong impact on the performance of the approaches.

Stereo Visual Odometry: Stereo visual odometry methods exploit the known baseline between the cameras of the stereo camera rig for estimating scale. Therefore, stereo methods are typically able to outperform monocular methods on the KITTI dataset (see Table 13.1 and Table 13.2).

Cvisic and Petrovic [144] decouple estimation of rotation and translation as translation is dependent on the scene depth while rotation is not. They estimate rotation using the five-point algorithm [499] and translation using the three-point method. Buczko and Willert [84] exploit the same idea and propose to use an initial rotation estimation to decouple rotational and trans- lational optical flow. In contrast, Wang et al. [699] tackle the visual odome- try problem with a direct method by combining static stereo with multi-view stereo as in [186, 187]. In contrast to [186, 187], they extend the energy func- tion instead of relying on filtering approaches to update the geometry and provide an efficient bundle adjustment procedure for real-time optimization. One weakness of direct methods is that they often get stuck in local optima, especially in case of large motions. Zhu [800] addresses this problem with a dual Jacobian scheme for multi-scale pyramid optimization. This allows them to avoid local optima and obtain more accurate camera pose estimations that are closer to the optimal solution. In addition, they introduce a gradient- based feature representation, which improves robustness against illumination changes.

Lenac et al. [404] propose a filtering-based SLAM approach that leverages a novel filtering solution on Lie groups. Combined with the visual odometry method proposed in [144], they are ranked second in stereo visual odome- try. Cviˇsic et al. [143] improve the feature selection approach suggested in [144] with an age-based weighting factor suggested in [246] that gives higher weight to features that are horizontally closer to the image center. This al- lows them to better handle calibration errors and outperform all stereo-based methods (Table 13.2) while obtaining results competitive with LiDAR-based techniques.

Kreˇso and ˇSegvi´c [366] observed that camera calibration is critical for visual odometry and that the remaining calibration errors in pre-calibrated systems like KITTI have adversarial effects on the estimation results. They, therefore, propose to explicitly correct the calibration of the camera by ex- ploiting ground truth motion which they use to recover a deformation field by optimizing the reprojection error of point feature correspondences in neigh- boring stereo frames.

LiDAR-based Odometry: Motivated by the impact of small calibration errors on the depth estimation of stereo-based methods [366], Gr¨ater et al. [256] leverages depth information obtained from LiDAR for monocular visual odometry. Rejecting outliers based on a local plane assumption and fusing depth similar to [144, 85], they obtain competitive results (Table 13.3).

In contrast, Neuhaus et al. [494] directly address the SLAM problem by integrating LiDAR data with inertial measurements. The integration of IMU data allows them to cope with high-frequency motion, e.g., in off-road envi- ronments.

Inspired by RGB-D methods [496], Deschaud [164] uses an implicit surface representation [142] of the map for aligning new scans in a LiDAR SLAM approach. In combination with a specific sampling strategy based on LiDAR scans, they achieve results similar to [494].

The best performing methods on KITTI use 3D point clouds from Li- DAR for ego-motion estimation (Table 13.3). Zhang and Singh [780] split the SLAM problem into LiDAR-based odometry at high frequency with low accu- racy and LiDAR-mapping at low frequency with high accuracy, as illustrated in Figure 13.10. Their LiDAR-based odometry approach matches two consec- utive LiDAR scans, whereas their LiDAR-based mapping approach matches and registers the new scan to a map, resulting in low drift and low compu- tational complexity at the same time. Zhang and Singh [781] extend this work by combining visual odometry at high frequency with LiDAR-mapping at low frequency, which allows them to further improve upon their results (Table 13.3).

13.8 Discussion While localization approaches are still missing an established unified bench- mark for fair comparison and evaluation of methods, a new benchmark 1 based on multiple diverse datasets has recently been proposed by Sattler et al. [582]. Based on these results, it can be concluded that current techniques still fail to perform well in challenging real-world conditions, as identified in [96, 582]. One possible direction towards higher recall and more robustness is to incorporate deep CNN features encoding high-level information. For instance, Sch¨onberger et al. [601] and Radwan et al. [543] demonstrate that localization accuracy in challenging conditions can benefit from a semantic understanding of the environment.

In ego-motion estimation, monocular visual odometry methods can not yet compete with approaches using 3D information on the KITTI dataset. While LiDAR provides the richest source of information, stereo-based methods also achieve competitive results. In Figure 13.11, we visualize the average transla- tional and rotational errors of the best performing visual odometry methods on the KITTI benchmark. The second row shows the translational error, and the third row shows the rotational error while the last row shows the speed. The highest translational and rotational errors are usually observed in case of strong turns. Furthermore, the error is correlated with speed and the amount of independently moving objects in the scene, which causes a decrease in the number of matched features in the background. While large errors can be observed for crowded highway scenes (second from right), only moderate er- rors occur when the highway is empty (right and second from left). Larger errors can also be observed in very narrow environments (fourth from right) where feature displacements are large. Overall, the most accurate motion estimation is achieved using 3D information. However, it is remarkable that state-of-the-art stereo-based methods achieve competitive results using cheap passive stereo sensors in comparison to more expensive LiDAR scanners.

