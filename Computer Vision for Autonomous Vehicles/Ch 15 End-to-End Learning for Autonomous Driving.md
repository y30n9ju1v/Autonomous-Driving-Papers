15.1 Problem Definition Current state-of-the-art autonomous driving systems in industry are com- posed of numerous modules, e.g., detection (of traffic signs, lights, cars, pedes- trians), segmentation (of lanes, facades), motion estimation, tracking of traffic participants, reconstruction etc. The results from these components are then typically combined in a planning module that feeds the control. However, this requires robust solutions to many open challenges in scene understand- ing in order to solve the problem of manipulating the car direction and speed. Furthermore, auxiliary loss functions are required to train each module (e.g., object detection, semantic segmentation) independently, hence ignoring the actual goals of the driving task which include travel time, safety, and comfort.

As an alternative, several methods consider autonomous driving as an end-to-end learning problem. In these approaches, the tasks of perception, planning, and control are combined, and a single model is trained end-to-end using a deep neural network. Most end-to-end autonomous driving systems map from sensory inputs, such as front-facing camera images, directly to driving actions such as steering angle.

15.2 Methods End-to-end driving methods are typically trained from expert demonstrations to learn a driving policy that imitates the behavior of an expert or using reinforcement learning to explore the environment by trial and error (often in simulation). In the following sections, we first introduce the most relevant approaches proposed in the literature. We then discuss methods that combine ideas from behavior cloning and reinforcement learning. Finally, we discuss approaches that propose intermediate representations and demonstrate how driving models can be transferred from simulation to the real-world.

15.2.1 Behavior Cloning Behavior cloning approaches learn to map sensor observations, such as RGB images, to desired driving behavior by learning to clone the behavior of an expert. Thus, these approaches fall into the category of supervised learning techniques. Most commonly, a deep neural network is employed to represent the mapping from observations to expert actions. In the 1980s, Pomerleau [534] propose ALVINN, the first demonstration of imitation learning for self- driving vehicles using a small fully connected neural network. 30 years later, Bojarski et al. [60] propose a deeper end-to-end deep convolutional neural network for lane following, illustrated in Figure 15.1, that maps images from the front-facing camera of a car to steering angles, given expert data. Xu et al. [746] propose an alternative approach and exploit large scale online datasets from uncalibrated sources to learn a driving model. Specifically, they formu- late autonomous driving as a future ego-motion prediction problem. They claim that predicting ego-motion instead of vehicle control allows their ap- proach to generalize better to new platforms. Their deep learning architecture combines FCNs and LSTMs, and learns to predict the motion path given the current state of the agent.

Another problem with behavior cloning approach is that the training data is collected using an off-policy expert teacher, i.e., the training data is collected by rolling out the expert policy, which is different from the policy being learned. As collecting expert demonstrations for all possible situations is not practical, the training trajectories do not cover all possible states. At test time, the rollout of the behavior cloning policy thus causes it to move to a different distribution of states compared to the one it was trained on. Due to this covariate shift between the training and test time trajectories, the behavior cloning agent’s errors compound when drifting away from the expert demonstrations. In other words, the vehicle is likely to encounter new situations it has not been trained for and therefore acts wrongly.

In contrast, in on-policy rollout, training data is collected using the cur- rent policy being learned. Ross and Bagnell [570] propose DAgger to alleviate covariate shift by iteratively collecting corrective expert actions for the states visited by rolling out the currently learned driving policy. The driving pol- icy parameters are then trained using the data collected on-policy. However, doing on-policy rollouts with an imperfect policy has the disadvantage of drifting and potentially reaching dangerous states, thus requiring a simulator for safe training. Laskey et al. [390] claim to provide a safer way of gener- ating training data using expert policy with small amounts of noise injected to approximate the errors of on-policy rollout. They achieve this by iterat- ing between learning a noise model that minimizes the covariate shift and generating data for training the behavior cloning agent.

Besides the drifting problem during test time, behavior cloning-based driv- ing systems have other limitations. Sensor input alone is often not sufficient to uniquely infer control. Consider intersections, for example, where multiple possible actions are valid (left, right, straight). Without conditioning on the goal, all three options are acceptable. Thus, some of the behavior cloning agents, such as the one by Bojarski et al. [60], require human intervention for lane changes or turns. To alleviate this limitation, Codevilla et al. [130] propose a conditional imitation learning framework to learn a driving pol- icy for steering and throttle control from a high-level navigational input in addition to the observations from the camera (Figure 15.2). The high-level navigational input represents the driver’s intention, such as the direction to take at the next intersection, which cannot be recovered from sensory input alone.

Codevilla et al. [131] identify other limitations of behavior cloning ap- proaches related to generalization performance. They observe that in con- trast to typical supervised learning tasks, the generalization performance for behavior cloning does not scale with training data. Moreover, they identify significant variance in performance when varying the model initialization or the order in which training examples are sampled from the dataset.

Chen et al. [106] show that imitation learning can be simplified by decom- posing it into two stages, as illustrated in Figure 15.3. They first train an agent that has access to privileged information. This privileged agent cheats by observing the ground-truth layout of the environment and the positions of all traffic participants. In the second stage, the privileged agent acts as a teacher that trains a purely vision-based sensorimotor agent. The resulting sensorimotor agent does not have access to any privileged information and does not cheat. They demonstrate that this approach substantially outper- forms the state of the art on the CARLA benchmark and the recent NoCrash benchmark, attaining the best performance to date.

15.2.2 Reinforcement Learning Approaches based on reinforcement learning (RL) learn to drive by training an agent that tries to maximize a user defined reward which the agent receives while interacting with the environment. In the autonomous driving applica- tion, the reward is defined by specifying the driving agent’s preferences and goals. Dosovitskiy et al. [179] propose a reinforcement learning method that trains a deep network based on a reward function provided by the CARLA simulator which combines speed, distance traveled towards the goal, colli-sion damage, overlap with sidewalk and overlap with the opposite lane. For training the agent, they use the asynchronous advantage actor-critic (A3C) algorithm [361] which uses the value function learned by the critic to update the actor’s policy. Dosovitskiy et al. [179] observe that the RL agent per- forms significantly worse compared to a behavior cloning agent trained using conditional imitation learning [179] despite the fact that the RL agent was trained on a significantly larger set of visual observations. Recently, Kendall et al. [344] showed first promise in learning to drive in the real-world using a reinforcement learning agent (Figure 15.4). They use the deep deterministic policy gradients algorithm for training the RL agent and define the reward as the distance traveled by the vehicle without the safety driver taking control.

The aforementioned methods are trained using model-free reinforcement learning. The disadvantage of model-free methods is that they are often data inefficient and require a large number of interactions with the environment. In contrast, model-based reinforcement learning approaches learn a model of the environment dynamics from observational data and then exploit this model for training a driving policy. Model-based methods have been shown to significantly reduce the number of environment interactions required to learn an effective policy. However, model-based methods also typically require an interactive environment as a dynamics model trained on a fixed set of demonstrations may make incorrect predictions outside the training domain. The interactive training environment is however not practical in the real- world where such interactions are expensive and dangerous. To alleviate this problem, Henaff et al. [291] propose to train a model-based policy which is encouraged to produce actions which the forward dynamics model is confident about. They achieve this by training the policy network to minimize an uncertainty cost which represents the mismatch between the states it induces and the states in the trained data.

15.2.3 Combined Methods Behavior cloning methods are easy to train in a supervised fashion. However, they are poor at exploring the environment and therefore require extensive on- policy data augmentation using methods like DAgger [570]. RL approaches, in contrast, do not require per-frame supervision and are better at exploration. However, they are inefficient to train and require a simulator or non-practical trial and error runs in a real environment as well as careful design of the reward function. Therefore, several methods have been proposed to combine the strengths of both approaches.

Liang et al. [421] propose an approach to alleviate the low exploration efficiency of RL for large action space. They achieve this by constraining the policy search space by initializing the weights of the policy network of an RL algorithm by a network trained to clone the expert behavior. They observe significant improvements on the CARLA benchmark over agents trained us- ing RL from scratch. Li et al. [412] propose an approach that learns to clone only the best behaviors of several sub-optimal teachers. They estimate the best teacher by estimating the value function of each sub-optimal teacher. The sub-optimal teachers are defined using several simple controllers over the planner output. Therefore, they do not require expert teachers for labeling data and allow for better exploration compared to learning from a single ex- pert teacher. In addition, learning from multiple sub-optimal teachers leads to faster training compared to pure RL agents as exploration only happens from feasible states. The requirement to specify the reward function limits the practical use of Reinforcement Learning. An accurate specification of the reward requires tedious and computationally inefficient hyper-parameter tun- ing. Sharifzadeh et al. [618] propose to learn the unknown reward function of the driving behavior from expert demonstrations by applying Inverse Re- inforcement Learning (IRL). In contrast to behavior cloning approaches that directly learn the observation-control mapping in a supervised fashion, In- verse Reinforcement Learning approaches claim to offer better generalization by learning a reward function that explains the expert behavior.

15.2.4 Intermediate Representations Instead of directly learning a mapping from pixels to actions, Chen et al. [105] present an approach which first estimates a small number of human interpretable, pre-defined affordance measures such as the angle of the car relative to the road, the distance to the lane markings, and the distance to cars in the current and adjacent lane. These predicted affordances are then mapped to car actions using a rule-based controller to enable autonomous driving in the TORCS car racing simulation [736]. The advantage of mid-level representations is that the network predicting the mid-level representations can be trained and validated before deploying them. In addition, the mid- level representations are more interpretable compared to traditional behavior cloning approaches. Similarly, Sauer et al. [584] estimate several affordances from sensor inputs in order to drive a car, as illustrated in Figure 15.5. In contrast to Chen et al. [105], they consider the more challenging scenario of urban driving using the CARLA simulator [179]. In CARLA, the agent needs to obey traffic rules such as speed limits, red lights, avoid colliding with obstacles on the road and navigate at junctions with multiple possible driving directions. Sauer et al. [584] realize their driving agent by expanding the set of affordances to cover the most important aspects of urban environments. Similar to Chen et al. [105], they use a rule-based controller to map affordances to vehicle controls.

Recently, Zhou et al. [795] studied the significance of using intermediate representations pursued in computer vision research such as depth, segmen- tation, optical flow for improving several sensorimotor tasks such as urban driving. They observed that an agent that takes as input one or more of these intermediate representations along with the image learns significantly better sensorimotor control than an agent which uses just the raw image as input. They observed significant improvements even when the intermediate representations were noisy predictions by a simple deep network. Bansal et al. [32] propose a perception module that translates raw sensor observations to a mid-level representation. Their representation includes a top-down rendering of the environment where 2D boxes of vehicles are drawn along with a render- ing of the road information and traffic light states. They use this mid-level representation as input to a recurrent neural network (RNN) which outputs the control command. Similarly, Wang et al. [696] infer the depth and poses of the objects present in the scene from front-facing camera images and project the objects into an overhead view. They train a behavior cloning agent over the concatenation of front-facing and overhead images and observe improved performance over an agent trained only on front-facing images. In the same spirit, M¨uller et al. [486] train a driving policy in CARLA with mid-level rep- resentations as input. Specifically, they used binary segmentation estimated from a scene segmentation network as input to the driving policy network and observed improvements over an agent trained on raw camera images.

Similar to the aforementioned methods, Mehta et al. [459] also propose to use intermediate visual affordances such as “distance to intersection”, and action primitives such as “slow down” as input to the driving policy network. However, in contrast to the aforementioned works, they predict visual affor-dances and action primitives as an auxiliary task to the driving control task. They claim that predicting representations which are crucial for the driving decision allow the policy network to learn superior internal representations leading to more efficient training and better generalization. Kendall et al. [344] studied the importance of using an intermediate representation for state representation instead of raw pixels for learning a reinforcement learning- based driving policy. They observed significant improvements in data effi- ciency in training the driving policy using a compressed representation of the raw image, obtained using a Variational Autoencoder (VAE)

15.2.5 Transferring from Simulation to the Real World One major limitation of reinforcement learning is the necessity of a simulation environment for trial and error. Thus, during training only synthetic data is considered and the models usually do not generalize to real data. To address this problem, Pan et al. [506] propose to transfer a reinforcement learning agent trained in a virtual environment to the real-world. More specifically, they learn an image translation network to translate non-realistic simulated images to realistic images. Their translation network is composed of two con- ditional GANs, the first for segmenting virtual images from the simulator, and the second for translating the segmented images to their realistic coun- terparts. In the same spirit, Bewley et al. [51] propose to train an image-to- image translation network for transferring a driving policy from simulation to real-world without any real-world control labels (Figure 15.6). In contrast to Pan et al. [506], which uses an explicit semantic segmentation as intermediate representation, they use an implicit latent structure as intermediate represen- tation. They propose two autoencoder-like networks for translating between domains where a common latent space is learned through direct and cyclic losses. Their control network is trained using behavior cloning by passing the latent code as input to the control network.

15.3 Datasets As behavior cloning approaches can be trained on offline expert demonstra- tions, several public datasets have been introduced in the last few years to train and evaluate such methods. The comma.ai dataset [578] provides 7.25 hours of driving data with a camera in the windshield, capturing images of the road at 20Hz. The dataset also provides observations from several other sensors such as car speed, steering angle, GPS, gyroscope, and IMU. How- ever, the dataset only contains training examples for highway scenarios, and is therefore not suitable for learning a driving policy which operates in more challenging situations such as in cities. The Berkeley DeepDrive Video dataset [746] comprises 10,000 hours of driving in cities, on highways, in towns, and in rural areas. The dataset has been recorded using forward-facing dash cameras along with observations from sensors such as GPS, IMU, gyroscope, and mag- netometer. As discussed in the previous section, online rollouts of the driving policy is an important requirement for training and evaluation of most end- to-end learning methods. However, deploying a partially trained model in a real environment to collect training data is both dangerous and impractical. Therefore, realistic driving simulators are a key requirement for training and evaluating these models.

As one of the first open-source simulators, the TORCS racing car simulator [736] has been used for learning and evaluation of road lane following by Chen et al. [105]. However, the TORCS environments is simplistic, lacking complexities such as traffic participants, junctions, etc.

In contrast, CARLA [179] provides a more realistic, complex and flexi- ble open-source simulator for autonomous driving that enables training and validation in urban driving conditions. It provides high quality images along with ground-truth depth and semantic segmentation as pseudo-sensors, as il- lustrated in Figure 15.7. In order to replicate the complex nature of urban driving, the environments in CARLA exhibit realistic urban street layouts with traffic rules, intersections, buildings, pedestrians, street signs and other traffic participants. The simulator also provides different weather and light- ing conditions in order to evaluate the generalization ability of the driving agent. CARLA also provides a benchmark based on four increasingly difficult driving tasks and is actively expanded in terms of the environments, assets and agents it provides.

However, existing real-world datasets and synthetic simulators often fail to capture the long tail of the distribution which covers important but rare situations. These rare events can only be effectively captured with a large fleet of vehicles that log these situations in real-world driving. Tesla’s Autopilot system [659] is a dormant logging-only mode that can be queried for multiple instances of rare failure situations so that the model can be trained to avoid such failures. In addition, Shadow Mode allows Tesla to validate the Autopilot system running in the background in real situations. However, data from Tesla vehicles are proprietary and hence not released to other companies or public research institutions.

15.4 Metrics There are no standard metrics and benchmarks for autonomous driving and thus most methods usually evaluate on their own set of metrics and datasets. The most popular benchmark CARLA [179] evaluates on two metrics. First, the percentage of successfully completed episodes under the four different conditions provided by CARLA. And second, the average distance (in kilo- meters) driven between two infractions. Infractions include driving on the opposite lane, driving on the sidewalk, colliding with other vehicles, colliding with pedestrians, and hitting static objects. Codevilla et al. [129] use CARLA to analyze the correlation between offline and online metrics for evaluation of autonomous driving agents. They observe that offline metrics such as the squared or absolute error of the steering angle are poorly correlated with online metrics such as the success rate of reaching the goal. Their work highlights the tension between imitation learning and reinforcement learning. While reinforcement learning allows to train for the desired goal, training an imitation learning agent is significantly easier and does not require potentially unsafe exploration.

15.5 Discussion A common characteristic of most end-to-end driving methods is the need to collect online training data. While behavior cloning methods have shown promising results by learning purely from expert demonstrations, minimizing the covariate shift between the expert trajectories and the agent’s policy is still an open problem. Similarly, reinforcement learning approaches require millions of trial and error runs and thus can only be safely applied in simu- lation environments. Moreover, as shown by [129], offline evaluation metrics are poorly correlated with online driving performance. Therefore, safe train- ing and validation of end-to-end learning models require further development of realistic simulators such as CARLA [179] on which these methods can be trained before transferring the resulting policies to the real-world. Flexibility is another desirable characteristic when developing simulators: the resulting simulations should allow for highly diverse and complex scenarios, yet also model the long tail of the data distribution to capture rare events. While realistic simulation environments are important, there will likely remain a domain gap between simulated and real data. Therefore, another critical di- rection of future research is the design of end-to-end learning methods which can be robustly transferred from simulated environments to the real-world. Furthermore, the lack of interpretability of end-to-end driving networks pre- vents deeper insights into the modes of operation (in particular legal relevant failure cases) and thus requires further investigation.