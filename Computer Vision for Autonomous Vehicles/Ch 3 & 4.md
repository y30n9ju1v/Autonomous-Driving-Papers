# Chapter 3 
## Sensors

The navigation of autonomous systems is usually addressed with a sensor suite which comprises various different types of sensors, including cameras, wheel odometry, and range sensors (SONAR, RADAR, and LiDAR). As an example, Tesla uses several cameras, RADAR, and ultrasonics for their advanced driver- assistance system Autopilot. Fusing information from several sensors allows exploiting their complementary characteristics and addressing the limitations of individual sensors, e.g., the loss of structure information in cameras or missing color information in range data.

자율 시스템의 항법은 다양한 센서들을 활용하여 해결됩니다. 여기에는 카메라, 바퀴 속도 측정, 그리고 레이더(RADAR), LiDAR 센서 등 여러 종류의 센서가 포함됩니다. 예를 들어 테슬라는 고급 운전자 보조 시스템인 오토파일럿에 여러 대의 카메라, 레이더, 초음파 센서를 사용합니다. 여러 센서로부터 정보를 융합함으로써 각 센서의 보완적인 특징을 활용하고, 개별 센서의 한계를 보완할 수 있습니다. 예를 들어 카메라에서는 구조 정보 손실이나 레이더 데이터에서 색상 정보가 누락되는 문제 등을 해결할 수 있습니다.

Wheel odometry measures the rotation of a wheel and can be used to esti- mate the distance covered by the autonomous vehicle. However, wheel odom- etry does not provide the full vehicle pose (i.e., all six degrees of freedom) and is thus typically combined with visual odometry or SLAM techniques discussed in Chapter 13. Range sensors, i.e., SONAR, RADAR, LiDAR, pro- vide additional information about the geometry and structure of the scene. Ultrasonic sensors (SONAR) emit high-frequency sound waves and measure the time for sound waves to travel to nearby objects. The distance to objects is computed from the travel time since the speed of sound waves is known. RADAR and LiDAR work with the same principle but use electromagnetic waves and laser light pulses instead of sound waves. Because of the larger wavelength, RADAR sensors benefit from a larger working distance than Li- DAR and SONAR but at the price of lower accuracy.

바퀴 오도메트리는 바퀴의 회전을 측정하여 자율 주행 차량이 이동한 거리를 추정하는 데 활용될 수 있습니다. 그러나 바퀴 오도메트리는 차량의 전체적인 자세(즉, 6도 자유도)를 제공하지 않으므로 일반적으로 13장에서 논의된 비전 오도메트리나 SLAM 기술과 함께 사용됩니다. 레이더 센서(예: SONAR, RADAR, LiDAR)는 장면의 기하학적 정보와 구조에 대한 추가 정보를 제공합니다. 초음파 센서(SONAR)는 고주파 음파를 방출하여 주변 물체로의 음파 이동 시간을 측정합니다. 음파 속이 알려진 상태에서 이동 시간을 통해 물체까지의 거리가 계산됩니다. RADAR 및 LiDAR는 동일한 원리를 사용하지만 음파 대신 전자기파와 레이저 펄스를 사용합니다. 더 큰 파장에 따라 RADAR 센서는 LiDAR 및 SONAR보다 더 긴 작동 거리를 제공하지만 정확도는 낮을 수 있습니다.

As cameras are cheap, passive, and easy to deploy, they are an attractive sensor choice for self-driving cars, and several existing driver assistance sys- tems rely on cameras for lane keeping or pedestrian detection. We now briefly discuss the most dominant camera types and give a short overview of popular calibration pipelines for estimating intrinsic and extrinsic sensor parameters.

카메라가 저렴하고, 간편하게 설치할 수 있으며, 성능도 뛰어나 자율주행차를 위한 매력적인 센서 선택이 되고 있습니다. 또한, 기존의 운전자 보조 시스템들은 차선 유지나 보행자 감지에 카메라를 활용하고 있습니다. 현재는 가장 널리 사용되는 카메라 종류를 간략하게 살펴보고, 내부 및 외부 센서 파라미터를 추정하기 위한 인기 있는 캘리브레이션 파이프라인에 대한 개요를 제공하고자 합니다.

### 3.1 Camera Models

Most conventional cameras comprise an aperture and one or multiple lenses and can be well approximated by the pinhole camera model (Figure 3.1). Omnidirectional cameras allow to significantly increase the field of view by exploiting mirrors or special lenses. Event cameras enable the acquisition of intensity changes at very high temporal resolutions. In the following, we provide a brief overview of omnidirectional and event cameras. We refer the reader to [652, 278] for an in-depth discussion of the pinhole camera model and projective geometry.

일반적인 카메라들은 조리개와 하나 이상의 렌즈로 구성되며, 피놀라 카메라 모델(그림 3.1)로 잘 근사할 수 있습니다. 오므니디렉셔널 카메라들은 거울이나 특수 렌즈를 활용하여 시야각을 크게 확장할 수 있도록 합니다. 이벤트 카메라들은 매우 높은 시간 해상도로 강도 변화를 획득할 수 있습니다. 다음에서는 오므니디렉셔널 카메라와 이벤트 카메라에 대한 간략한 개요를 제공합니다. 피놀라 카메라 모델 및 투영 기하에 대한 심층적인 논의는 [652, 278]를 참고해 주시기 바랍니다.

#### 3.1.1 Omnidirectional Cameras

A panoramic field of view is desirable in autonomous driving to gain maximum information about the surrounding area for safe navigation. Omnidirectional cameras with a 360-degree field of view (see Figure 3.2) provides enhanced coverage by eliminating the need for more cameras or mechanically turnable cameras. There are different types of omnidirectional cameras. Catadioptric cameras combine a standard camera with a shaped mirror, such as a parabolic, hyperbolic, or elliptical mirror, while dioptric cameras use purely dioptric fisheye lenses. Polydioptric cameras use multiple cameras with overlapping field of view to provide a full spherical field of view.

자율 주행 시스템에서 안전한 항해를 위해 주변 환경에 대한 최대한 많은 정보를 얻는 것이 중요합니다. 360도 시야각을 제공하는 오므라미 카메라(Omnidirectional cameras, Figure 3.2 참조)는 추가적인 카메라나 회전 가능한 카메라의 필요성을 없애면서 더욱 향상된 커버리지를 제공합니다. 오므라미 카메라는 다양한 종류가 있으며, 카타디옵트릭 카메라는 표준 카메라와 파라볼릭, 하이퍼볼릭 또는 elliptical 렌즈와 같은 특수 모양의 거울을 결합하는 방식입니다. 또한, 디옵트릭 카메라는 순수한 디옵트릭 핀홀렌즈를 사용합니다. 폴디옵트릭 카메라는 겹치는 시야각을 가진 여러 카메라를 사용하여 완전한 구형 시야각을 제공합니다.

Geyer and Daniilidis [248] provide a unifying theory for all central cata- dioptric systems which is known as unified projection model in the literature and widely used by different calibration toolboxes [460, 293, 292]. Scara- muzza and Martinelli [590] propose to model the imaging function using the Taylor series expansion. Mei and Rives [460] improve upon the unified projec-tion model of [248] to account for real-world errors by modeling distortions. Sch¨onbein et al. [598] propose a fast approximation to computationally ex- pensive non-central camera models.

게이저와 다니일리디스[248]는 문헌에서 널리 사용되는 통합 투사 모델로 알려진 모든 중앙 렌즈 굴절 시스템에 대한 통일된 이론을 제시합니다. 이 모델은 다양한 캘리브레이션 툴박스[460, 293, 292]에서 활용되고 있습니다. 스카라무자와 마르텔리[590]는 타일러 급 전개(Taylor series expansion)를 사용하여 이미지 함수를 모델링하는 방안을 제안합니다. 메이와 리브스[460]는 실제 환경에서의 오차를 고려하기 위해 게이저와 다니일리디스[248]의 통합 투사 모델을 개선하여 왜곡을 모델링합니다. 쇤베인 연구팀[598]은 계산 비용이 많이 드는 비중심 카메라 모델에 대한 빠른 근사 방법을 제안합니다.

Omnidirectional cameras are gaining popularity in autonomous driving re- search. For feature-based applications such as navigation, motion estimation, and mapping, a large field of view enables the extraction and matching of interest points from all around the car. Thus, omnidirectional cameras have been successfully used to improve ego-motion estimation of vehicles [587] and 3D reconstruction of static scenes [597, 271].

자율 주행 연구 분야에서 올리전방 카메라의 인기가 높아지고 있습니다. 특히 내비게이션, 움직임 추정, 맵핑과 같은 기능 기반 애플리케이션의 경우, 차량 전체를 촬영할 수 있는 넓은 화각을 통해 주변의 관심 지점을 추출하고 일치시키는 데 활용될 수 있습니다. 따라서 올리전방 카메라는 차량의 자가 운동 추정 정확도를 향상시키고, 정적인 장면의 3D 재구성을 개선하는 데 성공적으로 사용되고 있습니다.

#### 3.1.2 Event Cameras

Contrary to conventional frame-based cameras, event cameras produce a stream of asynchronous events of brightness changes surpassing a pre-defined threshold at microsecond resolution, as illustrated in Figure 3.3. An event comprises the location, sign, and timestamp of the change. As events are sparse in both space and time, this representation has the potential to reduce transmission and processing demands. The high temporal resolution enables the development of highly reactive systems. 

존재론적 프레임 기반 카메라와는 달리, 이벤트 카메라는 마이크로초 단위의 밝기 변화를 미리 설정된 임계값 이상으로 생성하여 비동기적인 이벤트 스트림을 제공합니다. 이러한 이벤트는 위치, 변화 방향, 그리고 시간 정보를 포함합니다. 이벤트는 공간 및 시간 모두에서 드물게 발생하므로, 이 표현 방식은 전송 및 처리 부담을 줄일 수 있는 잠재력을 가지고 있습니다. 높은 시간 해상도는 매우 반응성이 뛰어난 시스템 개발을 가능하게 합니다.

Dynamic and Active-Pixel Vision Sensors (DAVIS) output both CMOS images at fixed frame rates as well as asynchronous events, hence combining the benefits of both sensors. Mueggler et al. [485] provide a collection of real and synthetic datasets captured with DAVIS to push research on event- based methods. Binas et al. [53] present the DAVIS Driving Dataset and demonstrate end-to-end learning of steering angles. Recent work exploits DAVIS for feature tracking [237] and SLAM [686], improving accuracy and robustness over using only a single modality. 

저희 DAVIS 센서는 고정 프레임 속도로 CMOS 이미지를 출력하는 동시에 비동기 이벤트를 제공함으로써, 두 센서의 장점을 결합합니다. Mueggler et al. [485]은 DAVIS를 사용하여 획득한 실제 및 합성 데이터 세트를 제공하여 이벤트 기반 방법 연구를 발전시키는 데 기여하고 있습니다. Binas et al. [53]은 DAVIS Driving Dataset을 제시하고, 조향각의 종단 간 학습을 보여줍니다. 최근 연구에서는 DAVIS를 활용하여 특징 추적 [237] 및 SLAM [686]을 수행함으로써, 단일 모달리티만을 사용하는 것보다 정확성과 견고성을 향상시키고 있습니다.

Several methods have been developed which exploit the high temporal resolution and the asynchronous nature of event sensor for various problems. The majority of these methods focus on the application in unmanned aerial vehicles (UAVs) since very efficient methods are necessary to navigate these systems. In this context, event-based cameras have been used for ego-motion estimation [484], simultaneous localization and mapping (SLAM) [547] as well as for finding feature correspondences [226]. More recently, the benefits of event-based sensors have been exploited for autonomous vehicles by learning steering angles end-to-end [449].

여러 가지 방법이 개발되어 왔습니다. 이러한 방법들은 시간 해상도가 높고 이벤트 센서의 비동기적인 특성을 활용하여 다양한 문제 해결에 적용됩니다. 특히, 자율적으로 움직이는 무인 항공기(UAV) 시스템에서 매우 효율적인 방법이 필요하기 때문에 대부분의 방법이 UAV 분야에 초점을 맞추고 있습니다. 이와 관련하여, 이벤트 기반 카메라들은 자기 운동 추정[484], 동시 위치 추정 및 매핑(SLAM)[547] 뿐만 아니라 특징점 일치 탐색[226]에도 활용되고 있습니다. 최근에는 이벤트 기반 센서의 장점을 활용하여 자율 주행 차량에서 엔드-투-엔드 방식으로 조향 각도를 학습하는 데에도 적용되고 있습니다.

### 3.2 Calibration

Geometric calibration is the problem of estimating intrinsic and extrinsic parameters of one or multiple sensors in order to accurately relate 3D world points to 2D measurements. Fiducial markers and checkerboards are often used to facilitate parameter estimation [784, 62, 340, 9, 244].

정밀 기하 보정은 하나 이상의 센서의 내재적 및 외재적 파라미터를 추정하여 3차원 세계의 점들을 정확하게 2차원 측정값과 연결하는 문제입니다. 일반적으로 파라미터 추정을 돕기 위해 파사이드 마커와 체커보드가 활용됩니다 [784, 62, 340, 9, 244].

Various methods for camera calibration can be found since the beginning of the 1970s. Heikkila and Silven [290] were the first to consider the entire cal- ibration pipeline, including control point extraction, model fitting, and image correction. They proposed a four-step procedure to obtain the parameters of a physical camera model and address the problem of compensating image distortions.

1970년대 초부터 다양한 카메라 보정 방법이 개발되어 왔습니다. Heikkila와 Silven[290]은 제어점 추출, 모델 적합, 이미지 보정 등 전체 보정 파이프라인을 처음으로 고려했으며, 물리적 카메라 모델의 파라미터를 얻고 이미지 왜곡을 보정하는 문제를 해결하기 위한 네 단계의 절차를 제안했습니다.

Modern vehicles are typically equipped with multiple different sensors with the goal of increasing robustness and coverage. Several calibration procedures have been proposed to address the needs of such big sensor suites. While early approaches [784, 62] rely on manual extraction of interest points in laser scans, Kassir and Peynot [340] and Andreasson and Lilienthal [9] propose the first complete automatic camera-to-range calibration systems. Geiger et al. [244] demonstrate how to automatically calibrate a setup involving two cameras and a single range sensor such as Kinect or Velodyne laser scanner. Heng et al. [293] tackle the problem of estimating the intrinsic and extrinsic parameters of a multi-camera rig without overlapping field of view. Heng et al. [292] extend this work by removing the requirement to modify the environment by using a map and natural features instead of fiducial markings.

최신 차량은 내구성과 커버리지를 향상시키기 위해 다양한 센서들을 여러 개 장착하는 경우가 많습니다. 이러한 복잡한 센서 시스템의 요구사항을 충족하기 위해 여러 가지 보정 절차가 제안되었습니다. 초기 접근 방식 [784, 62]은 레이저 스캔에서 관심 지점을 수동으로 추출하는 데 의존했지만, Kassir와 Peynot [340] 및 Andreasson과 Lilienthal [9]은 최초의 완전 자동 카메라-범위 보정 시스템을 제안했습니다. Geiger et al. [244]은 Kinect 또는 Velodyne 레이저 스캐너와 같은 두 대의 카메라와 단일 범위 센서 설정을 자동으로 보정하는 방법을 보여주었습니다. Heng et al. [293]은 겹치는 시야를 갖지 않고 다중 카메라  Rig의 내부 및 외부 파라미터를 추정하는 문제를 해결했으며, Heng et al. [292]은 fiducial marking 수정의 필요성을 제거하고 지도와 자연적 특징을 사용하여 이를 확장했습니다.
